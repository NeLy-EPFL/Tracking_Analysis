{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "import platform\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "from Utilities.Utils import *\n",
    "from Utilities.Processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFolder\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    DataPath = Path(\"/Volumes/Ramdya-Lab/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "# Linux Datapath\n",
    "if platform.system() == \"Linux\":\n",
    "    DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "\n",
    "print(DataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the folders I want to use\n",
    "# For instance, I want to use the folders that have the \"FeedingState\" in the name\n",
    "\n",
    "Folders = []\n",
    "for folder in DataPath.iterdir():\n",
    "    minfolder = str(folder).lower()\n",
    "    #if \"tnt\" in minfolder and \"tracked\" in minfolder and \"pm\" in minfolder:\n",
    "    if 'feedingstate' in minfolder and 'pm' in minfolder:\n",
    "        Folders.append(folder)\n",
    "\n",
    "Folders\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe that will store the ball y positions and the arena and corridor numbers as metadata\n",
    "\n",
    "Dataset = pd.DataFrame(columns=[\"Fly\", \"yball\", \"arena\", \"corridor\"])\n",
    "\n",
    "# Loop over all the .analysis.h5 files in the folder and store the ball y positions and the arena and corridor numbers as metadata\n",
    "\n",
    "Flynum = 0\n",
    "# Loop over all the foldes that don't have \"Dark\" in the name\n",
    "for folder in Folders:\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "        \n",
    "        print (metadata_dict)\n",
    "        \n",
    "    for file in folder.glob(\"**/*.analysis.h5\"):\n",
    "        #print(file)\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            locations = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "        locations.shape\n",
    "        \n",
    "        if \"Flipped\" in folder.name:\n",
    "            yball[:, 0, 0] = -yball[:, 0, 0]\n",
    "\n",
    "        else:\n",
    "            yball : np.ndarray = locations[:, :, 1, :]\n",
    "        \n",
    "        # Get the filename from the path\n",
    "        foldername = folder.name\n",
    "\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        corridor = file.parent.name\n",
    "        \n",
    "        # Get the metadata for this arena\n",
    "        arena_key = arena.capitalize()\n",
    "        arena_metadata = {var: metadata_dict[var][arena_key] for var in metadata_dict}\n",
    "        \n",
    "        Flynum += 1\n",
    "        \n",
    "        # Store the ball y positions and the arena and corridor numbers as metadata\n",
    "        data = {\"Fly\": \"Fly\" + str(Flynum),\n",
    "                \"yball\": yball[:, 0, 0], \n",
    "                \"experiment\": foldername,\n",
    "                \"arena\": arena, \n",
    "                \"corridor\": corridor}\n",
    "        data.update(arena_metadata)\n",
    "        Dataset = Dataset.append(data, ignore_index=True).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack yball positions\n",
    "\n",
    "Dataset = Dataset.explode(\"yball\")\n",
    "\n",
    "Dataset['Frame'] = Dataset.groupby('Fly').cumcount()\n",
    "\n",
    "Dataset['time'] = Dataset['Frame'] / 30\n",
    "#DataFrame['time'] = DataFrame.groupby(['experiment', 'arena', 'corridor']).cumcount() / 30\n",
    "\n",
    "#DataFrame['Fly'] = 'Fly' + (DataFrame.groupby(['experiment', 'arena', 'corridor']).ngroup() + 1).astype(str)\n",
    "\n",
    "#Dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences of \"Fed\" with \"fed\" in the 'FeedingState' column\n",
    "Dataset['FeedingState'] = Dataset['FeedingState'].replace('Fed', 'fed')\n",
    "\n",
    "# If there is 'Flipped' in the foldername, replace the correspondint 'Orientation' with 'flipped'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.loc[Dataset['experiment'].str.contains('Flipped'), 'Orientation'] = 'flipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "checksave(\n",
    "    path=savepath.joinpath(\"230821_TNTScreen_4exps.feather\"),\n",
    "    object=\"dataframe\",\n",
    "    file=Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum time value for each fly\n",
    "max_time = Dataset.groupby(\"Fly\")[\"time\"].max()\n",
    "\n",
    "# Compute the number of rows for each fly\n",
    "num_rows = Dataset.groupby(\"Fly\").size()\n",
    "\n",
    "# Display the results\n",
    "print(\"Maximum time value for each fly:\")\n",
    "print(max_time)\n",
    "print()\n",
    "print(\"Number of rows for each fly:\")\n",
    "print(num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each possible value of 'Fly'\n",
    "print(Dataset['Fly'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightDataset = Dataset[Dataset['Light'] == 'on']\n",
    "\n",
    "GroupedDF = Dataset.groupby(['Genotype','time',])['yball'].mean().reset_index()\n",
    "\n",
    "GroupedDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique values of the column FeedingState\n",
    "feeding_states = GroupedDF['FeedingState'].unique()\n",
    "\n",
    "print(feeding_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a FacetGrid object with the 'Period' column as the row variable\n",
    "g = sns.FacetGrid(data=GroupedDF, row='Period')\n",
    "\n",
    "# Map a line plot of the 'yball' column over time to each facet, with the hue set to 'FeedingState'\n",
    "g.map(sns.lineplot, 'time', 'yball', 'FeedingState')\n",
    "\n",
    "# Add a legend to the plot\n",
    "g.add_legend()\n",
    "\n",
    "# Invert the y-axis of each Axes object in the FacetGrid\n",
    "for ax in g.axes.flat:\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "print(sns.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, axes = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "# Plot the data for each period on a separate axis\n",
    "for i, period in enumerate(GroupedDF['Period'].unique()):\n",
    "    data = GroupedDF[GroupedDF['Period'] == period]\n",
    "    for feeding_state in data['FeedingState'].unique():\n",
    "        subset = data[data['FeedingState'] == feeding_state]\n",
    "        axes[i].plot(subset['time'], subset['yball'], label=feeding_state)\n",
    "    axes[i].set_title(period)\n",
    "    axes[i].invert_yaxis()\n",
    "\n",
    "# Add a legend to the first axis\n",
    "axes[0].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the data to only include rows where 'Light' == 'on'\n",
    "GroupedDF = GroupedDF[GroupedDF['Light'] == 'on']\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, axes = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "# Plot the data for each period on a separate axis\n",
    "for i, period in enumerate(GroupedDF['Period'].unique()):\n",
    "    data = GroupedDF[GroupedDF['Period'] == period]\n",
    "    for feeding_state in data['FeedingState'].unique():\n",
    "        subset = data[data['FeedingState'] == feeding_state]\n",
    "        axes[i].plot(subset['time'], subset['yball'], label=feeding_state)\n",
    "    axes[i].set_title(period)\n",
    "    axes[i].invert_yaxis()\n",
    "\n",
    "# Add a legend to the first axis\n",
    "axes[0].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Define colors for each feeding state and light combination\n",
    "colors = {('fed', 'on'): 'C0', ('fed', 'off'): 'lightblue',\n",
    "          ('starved', 'on'): 'C1', ('starved', 'off'): 'lightgreen',\n",
    "          ('starved_noWater', 'on'): 'C2', ('starved_noWater', 'off'): 'pink'}\n",
    "\n",
    "# Plot the data for the PM period\n",
    "data = GroupedDF[GroupedDF['Period'] == 'PM']\n",
    "for feeding_state in data['FeedingState'].unique():\n",
    "    subset = data[data['FeedingState'] == feeding_state]\n",
    "    for light in subset['Light'].unique():\n",
    "        subsubset = subset[subset['Light'] == light]\n",
    "        linestyle = '-' if light == 'on' else '-'\n",
    "        label = f'{feeding_state} - Light {light}'\n",
    "        color = colors[(feeding_state, light)]\n",
    "        ax.plot(subsubset['time'], subsubset['yball'], linestyle=linestyle, color=color, label=label)\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('PM')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add a legend to the axis\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to compute the confidence interval for a given array of values\n",
    "def confint(x, alpha=0.05):\n",
    "    # Check if the input array contains at least two values\n",
    "    if len(x) < 2:\n",
    "        # If not, return a tuple containing two nan values\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    # Compute the mean and standard error of the mean\n",
    "    mean = np.mean(x)\n",
    "    sem = stats.sem(x)\n",
    "    \n",
    "    # Compute the confidence interval\n",
    "    ci = stats.t.interval(1 - alpha, len(x) - 1, loc=mean, scale=sem)\n",
    "    \n",
    "    return ci\n",
    "\n",
    "# Apply the confint function to each group of rows in your original dataframe\n",
    "confint_df = DataFrame.groupby(['Period', 'time'])['yball'].apply(confint).reset_index()\n",
    "\n",
    "# Rename the columns of the resulting dataframe\n",
    "confint_df.columns = ['Period', 'time', 'yball_lower', 'yball_upper']\n",
    "\n",
    "# Merge the resulting dataframe with your grouped dataframe\n",
    "GroupedDF = pd.merge(GroupedDF, confint_df, on=['Period', 'time'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of the 'yball' column over time, grouped by the 'Period' column\n",
    "sns.lineplot(data=GroupedDF, x='time', y='yball', hue='Period')\n",
    "\n",
    "# Add the confidence intervals to the plot\n",
    "for period, group in GroupedDF.groupby('Period'):\n",
    "    plt.fill_between(group['time'], group['yball_lower'], group['yball_upper'], alpha=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutants\n",
    "\n",
    "GroupedDF_TNT = Dataset.groupby(['Genotype','time',])['yball'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample size for each Genotype group based on the number of individual flies\n",
    "sample_size = Dataset.groupby('Genotype')['Fly'].nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the sample size for each Genotype group based on the number of individual flies\n",
    "sample_size = Dataset.groupby('Genotype')['Fly'].nunique()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a line plot of the 'yball' column over time, colored by the 'Genotype' column\n",
    "sns.lineplot(data=GroupedDF, x='time', y='yball', hue='Genotype', linewidth=1)\n",
    "\n",
    "# Invert the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Modify the labels of the legend to include the sample size for each Genotype group\n",
    "legend = plt.legend()\n",
    "for text, genotype in zip(legend.texts, sample_size.index):\n",
    "    text.set_text(f'{genotype} (n = {sample_size[genotype]})')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confints_BS = Dataset.groupby(['Genotype','time'])['yball'].apply(lambda x: draw_bs_ci(x, n_reps=300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Confints_BS_Process = Confints_BS.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split values of Confints_process[\"cumulated_success\"] into two columns ci_lower and ci_upper\n",
    "Confints_BS_Process[[\"ci_lower\", \"ci_upper\"]] = pd.DataFrame(\n",
    "    Confints_BS_Process[\"yball\"].tolist(), index=Confints_BS_Process.index\n",
    ")\n",
    "\n",
    "GroupedDF[\"ci_lower\"] = Confints_BS_Process[\"ci_lower\"]\n",
    "GroupedDF[\"ci_upper\"] = Confints_BS_Process[\"ci_upper\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a line plot of the 'yball' column over time, colored by the 'Genotype' column\n",
    "sns.lineplot(data=GroupedDF, x='time', y='yball', hue='Genotype', linewidth=1)\n",
    "\n",
    "# Add the confidence intervals to the plot\n",
    "for genotype, data in GroupedDF.groupby('Genotype'):\n",
    "    plt.fill_between(data['time'], data['ci_lower'], data['ci_upper'], alpha=0.2)\n",
    "\n",
    "# Invert the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Modify the labels of the legend to include the sample size for each Genotype group\n",
    "legend = plt.legend()\n",
    "for text, genotype in zip(legend.texts, sample_size.index):\n",
    "    text.set_text(f'{genotype} (n = {sample_size[genotype]})')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubGroup = GroupedDF[GroupedDF['Genotype'].isin(['TNTxTH', 'TNTxE-PG', 'PR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a line plot of the 'yball' column over time, colored by the 'Genotype' column\n",
    "sns.lineplot(data=SubGroup, x='time', y='yball', hue='Genotype', linewidth=1)\n",
    "\n",
    "# Add the confidence intervals to the plot\n",
    "for genotype, data in SubGroup.groupby('Genotype'):\n",
    "    plt.fill_between(data['time'], data['ci_lower'], data['ci_upper'], alpha=0.2)\n",
    "\n",
    "# Invert the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Modify the labels of the legend to include the sample size for each Genotype group\n",
    "legend = plt.legend()\n",
    "for text, genotype in zip(legend.texts, sample_size.index):\n",
    "    text.set_text(f'{genotype} (n = {sample_size[genotype]})')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedDF['Genotype'] = GroupedDF['Genotype'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "checksave(\n",
    "    path=savepath.joinpath(\"230821_TNTScreen_4exps_GroupedDF.feather\"),\n",
    "    object=\"dataframe\",\n",
    "    file=Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Method for more efficient dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the folders I want to use\n",
    "# For instance, I want to use the folders that have the \"FeedingState\" in the name\n",
    "\n",
    "Folders = []\n",
    "for folder in DataPath.iterdir():\n",
    "    minfolder = str(folder).lower()\n",
    "    #if \"tnt\" in minfolder and \"tracked\" in minfolder and \"pm\" in minfolder:\n",
    "    if 'feedingstate' in minfolder:\n",
    "        Folders.append(folder)\n",
    "\n",
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe that will store the ball y positions and the arena and corridor numbers as metadata\n",
    "Dataset = pd.DataFrame(columns=[\"Fly\", \"yball\", \"arena\", \"corridor\"])\n",
    "\n",
    "# Loop over all the .analysis.h5 files in the folder and store the ball y positions and the arena and corridor numbers as metadata\n",
    "Flynum = 0\n",
    "# Loop over all the foldes that don't have \"Dark\" in the name\n",
    "for folder in Folders:\n",
    "    print(f\"Adding experiment {folder} to the dataset...\")\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "        \n",
    "        print (metadata_dict)\n",
    "        \n",
    "    for file in folder.glob(\"**/*.analysis.h5\"):\n",
    "        #print(file)\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            locations = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "        locations.shape\n",
    "        \n",
    "        if \"Flipped\" in folder.name:\n",
    "            yball[:, 0, 0] = -yball[:, 0, 0]\n",
    "\n",
    "        else:\n",
    "            yball : np.ndarray = locations[:, :, 1, :]\n",
    "        \n",
    "        # Get the filename from the path\n",
    "        foldername = folder.name\n",
    "\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        corridor = file.parent.name\n",
    "        \n",
    "        # Get the metadata for this arena\n",
    "        arena_key = arena.capitalize()\n",
    "        arena_metadata = {var: pd.Categorical([metadata_dict[var][arena_key]]) for var in metadata_dict}\n",
    "        \n",
    "        Flynum += 1\n",
    "        \n",
    "        # Store the ball y positions and the arena and corridor numbers as metadata\n",
    "        data = {\"Fly\": pd.Categorical([\"Fly\" + str(Flynum)]),\n",
    "                \"yball\": [list(yball[:, 0, 0])], \n",
    "                \"experiment\": pd.Categorical([foldername]),\n",
    "                \"arena\": pd.Categorical([arena]), \n",
    "                \"corridor\": pd.Categorical([corridor])}\n",
    "        data.update(arena_metadata)\n",
    "\n",
    "        # Use pandas.concat instead of DataFrame.append\n",
    "        Dataset = pd.concat([Dataset, pd.DataFrame(data)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack yball positions\n",
    "\n",
    "Dataset = Dataset.explode(\"yball\")\n",
    "\n",
    "Dataset['Frame'] = Dataset.groupby('Fly').cumcount()\n",
    "\n",
    "Dataset['time'] = Dataset['Frame'] / 30\n",
    "\n",
    "Dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences of \"Fed\" with \"fed\" in the 'FeedingState' column\n",
    "Dataset['FeedingState'] = Dataset['FeedingState'].replace('Fed', 'fed')\n",
    "\n",
    "# If there is 'Flipped' in the foldername, replace the correspondint 'Orientation' with 'flipped'\n",
    "Dataset.loc[Dataset['experiment'].str.contains('Flipped'), 'Orientation'] = 'flipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "checksave(\n",
    "    path=savepath.joinpath(\"230822_FeedingState.feather\"),\n",
    "    object=\"dataframe\",\n",
    "    file=Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bootstrapped confidence intervals with multi-threading and a global progress bar\n",
    "Confints_BS = Dataset.groupby(['Genotype','time'])['yball'].apply(draw_bs_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting start and end of maze for each video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with one video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Videopath = Path('/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos/230721_Feedingstate_4_PM_Videos_Tracked/arena1/corridor1/corridor1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the first frame of the video\n",
    "cap = cv2.VideoCapture(Videopath.as_posix())\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Error: Could not read frame from video\")\n",
    "elif frame is None:\n",
    "    print(\"Error: Frame is None\")\n",
    "else:\n",
    "    # Convert to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Display the frame\n",
    "    plt.imshow(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the summed pixel values for each row of pixels\n",
    "rows = frame.sum(axis=1)\n",
    "\n",
    "plt.plot(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum of rows\n",
    "min_row = rows.argmin()\n",
    "\n",
    "# plot rows with the minimum row marked in red\n",
    "plt.plot(rows)\n",
    "plt.axvline(min_row, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frame with a nar drawn at the min location\n",
    "plt.imshow(frame)\n",
    "plt.axhline(min_row - 30, color='red')\n",
    "plt.axhline(min_row - 320, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frame but move the bars locations. For the first bar, move it to the right, for the second bar, move it to the left.\n",
    "\n",
    "plt.imshow(frame)\n",
    "for i, peak in enumerate(peaks):\n",
    "    if i % 2 == 0:\n",
    "        plt.axhline(peak + 50, color='red', alpha=0.5)\n",
    "    else:\n",
    "        plt.axhline(peak - 50, color='red', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with all videos of an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "\n",
    "# Set the path to the main folder\n",
    "main_folder = Path('/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos/230721_Feedingstate_4_PM_Videos_Tracked')\n",
    "\n",
    "# Create a list to store the frames and minimum row indices\n",
    "frames = []\n",
    "min_rows = []\n",
    "\n",
    "# Set the threshold value\n",
    "threshold = 100\n",
    "\n",
    "# Recursively traverse the directory tree\n",
    "for file in main_folder.rglob('*.mp4'):\n",
    "    # Set the path to the video file\n",
    "    Videopath = file\n",
    "    \n",
    "    # open the first frame of the video\n",
    "    cap = cv2.VideoCapture(Videopath.as_posix())\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame from video {Videopath}\")\n",
    "    elif frame is None:\n",
    "        print(f\"Error: Frame is None for video {Videopath}\")\n",
    "    else:\n",
    "        # Convert to grayscale\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply a median filter to smooth out noise and small variations\n",
    "        frame = median_filter(frame, size=3)\n",
    "        \n",
    "        # Apply a Gaussian filter to smooth out noise and small variations\n",
    "        frame = gaussian_filter(frame, sigma=1)\n",
    "        \n",
    "        # Compute the summed pixel values and apply a threshold\n",
    "        summed_pixel_values = frame.sum(axis=1)\n",
    "        summed_pixel_values[summed_pixel_values < threshold] = 0\n",
    "        \n",
    "        # Find the index of the minimum value in the thresholded summed pixel values\n",
    "        min_row = np.argmin(summed_pixel_values)\n",
    "        \n",
    "        # Store the frame and minimum row index\n",
    "        frames.append(frame)\n",
    "        min_rows.append(min_row)\n",
    "\n",
    "# Set the number of rows and columns for the grid\n",
    "nrows = 9\n",
    "ncols = 6\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20, 20))\n",
    "\n",
    "# Loop over the frames and minimum row indices\n",
    "for i, (frame, min_row) in enumerate(zip(frames, min_rows)):\n",
    "    # Get the row and column index for this subplot\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "    \n",
    "    # Plot the frame on this subplot\n",
    "    axs[row, col].imshow(frame, cmap='gray', vmin=0, vmax=255)\n",
    "    \n",
    "    # Plot the horizontal lines on this subplot\n",
    "    axs[row, col].axhline(min_row, color='yellow')\n",
    "\n",
    "    axs[row, col].axhline(min_row - 30, color='red')\n",
    "    axs[row, col].axhline(min_row - 320, color='blue')\n",
    "\n",
    "# Remove the axis of each subplot and draw them closer together\n",
    "for ax in axs.flat:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "\n",
    "# Set the path to the main folder\n",
    "main_folder = Path('/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos/230721_Feedingstate_4_PM_Videos_Tracked')\n",
    "\n",
    "# Create a list to store the frames, minimum row indices, and video paths\n",
    "frames = []\n",
    "min_rows = []\n",
    "video_paths = []\n",
    "\n",
    "# Set the threshold value\n",
    "threshold = 100\n",
    "\n",
    "# Recursively traverse the directory tree\n",
    "for file in main_folder.rglob('*.mp4'):\n",
    "    # Set the path to the video file\n",
    "    Videopath = file\n",
    "    \n",
    "    # open the first frame of the video\n",
    "    cap = cv2.VideoCapture(Videopath.as_posix())\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame from video {Videopath}\")\n",
    "    elif frame is None:\n",
    "        print(f\"Error: Frame is None for video {Videopath}\")\n",
    "    else:\n",
    "        # Convert to grayscale\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply a median filter to smooth out noise and small variations\n",
    "        frame = median_filter(frame, size=3)\n",
    "        \n",
    "        # Apply a Gaussian filter to smooth out noise and small variations\n",
    "        frame = gaussian_filter(frame, sigma=1)\n",
    "        \n",
    "        # Compute the summed pixel values and apply a threshold\n",
    "        summed_pixel_values = frame.sum(axis=1)\n",
    "        summed_pixel_values[summed_pixel_values < threshold] = 0\n",
    "        \n",
    "        # Find the index of the minimum value in the thresholded summed pixel values\n",
    "        min_row = np.argmin(summed_pixel_values)\n",
    "        \n",
    "        # Store the frame, minimum row index, and video path\n",
    "        frames.append(frame)\n",
    "        min_rows.append(min_row)\n",
    "        video_paths.append(Videopath)\n",
    "\n",
    "# Set the number of rows and columns for the grid\n",
    "nrows = 9\n",
    "ncols = 6\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20, 20))\n",
    "\n",
    "# Loop over the frames, minimum row indices, and video paths\n",
    "for i, (frame, min_row, Videopath) in enumerate(zip(frames, min_rows, video_paths)):\n",
    "    # Get the row and column index for this subplot\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "    \n",
    "    # Plot the frame on this subplot\n",
    "    axs[row, col].imshow(frame, cmap='gray', vmin=0, vmax=255)\n",
    "    \n",
    "    # Plot the horizontal lines on this subplot\n",
    "    axs[row, col].axhline(min_row - 30, color='red')\n",
    "    axs[row, col].axhline(min_row - 320, color='blue')\n",
    "    \n",
    "    # Save a .npy file with the start and end coordinates in the video folder\n",
    "    np.save(Videopath.parent / 'coordinates.npy', [min_row - 30, min_row - 320])\n",
    "\n",
    "# Remove the axis of each subplot and draw them closer together\n",
    "for ax in axs.flat:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "# Save the grid image in the main folder\n",
    "plt.savefig(main_folder / 'coordinates_grid.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import with relative yball computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the path to the data folder\n",
    "data_folder = Path('/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos')\n",
    "\n",
    "# Build a dataframe that will store the ball y positions and the arena and corridor numbers as metadata\n",
    "Dataset = pd.DataFrame(columns=[\"Fly\", \"yball\", \"arena\", \"corridor\", \"start\", \"end\"])\n",
    "\n",
    "# Loop over all the .analysis.h5 files in the folder and store the ball y positions and the arena and corridor numbers as metadata\n",
    "Flynum = 0\n",
    "# Loop over all the folders that don't have \"Dark\" in the name\n",
    "for folder in Folders:\n",
    "    print(f\"Adding experiment {folder} to the dataset...\")\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "        \n",
    "        print (metadata_dict)\n",
    "        \n",
    "    for file in folder.glob(\"**/*.analysis.h5\"):\n",
    "        #print(file)\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            locations = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "        locations.shape\n",
    "        \n",
    "        if \"Flipped\" in folder.name:\n",
    "            yball[:, 0, 0] = -yball[:, 0, 0]\n",
    "\n",
    "        else:\n",
    "            yball : np.ndarray = locations[:, :, 1, :]\n",
    "        \n",
    "        # Get the filename from the path\n",
    "        foldername = folder.name\n",
    "\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        corridor = file.parent.name\n",
    "        \n",
    "        # Get the metadata for this arena\n",
    "        arena_key = arena.capitalize()\n",
    "        arena_metadata = {var: pd.Categorical([metadata_dict[var][arena_key]]) for var in metadata_dict}\n",
    "        \n",
    "        Flynum += 1\n",
    "        \n",
    "        # Load the start and end coordinates from coordinates.npy\n",
    "        start, end = np.load(file.parent / 'coordinates.npy')\n",
    "        \n",
    "        # Store the ball y positions, start and end coordinates, and the arena and corridor numbers as metadata\n",
    "        data = {\"Fly\": pd.Categorical([\"Fly\" + str(Flynum)]),\n",
    "                \"yball\": [list(yball[:, 0, 0])], \n",
    "                \"experiment\": pd.Categorical([foldername]),\n",
    "                \"arena\": pd.Categorical([arena]), \n",
    "                \"corridor\": pd.Categorical([corridor]),\n",
    "                \"start\": pd.Categorical([start]),\n",
    "                \"end\": pd.Categorical([end])}\n",
    "        data.update(arena_metadata)\n",
    "\n",
    "        # Use pandas.concat instead of DataFrame.append\n",
    "        Dataset = pd.concat([Dataset, pd.DataFrame(data)], ignore_index=True) \n",
    "\n",
    "# Explode yball column to have one row per timepoint\n",
    "Dataset = Dataset.explode('yball')\n",
    "Dataset['yball'] = Dataset['yball'].astype(float)\n",
    "\n",
    "# Compute yball_relative relative to start\n",
    "Dataset['yball_relative'] = abs(Dataset['yball'] - Dataset['start'])\n",
    "\n",
    "Dataset[\"Frame\"] = Dataset.groupby(\"Fly\").cumcount()\n",
    "\n",
    "Dataset[\"time\"] = Dataset[\"Frame\"] / 30\n",
    "\n",
    "Dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences of \"Fed\" with \"fed\" in the 'FeedingState' column\n",
    "Dataset[\"FeedingState\"] = Dataset[\"FeedingState\"].replace(\"Fed\", \"fed\")\n",
    "\n",
    "# Add \"flipped\" to the list of categories for the Orientation column\n",
    "Dataset['Orientation'] = Dataset['Orientation'].cat.add_categories(['flipped'])\n",
    "\n",
    "# If there is 'Flipped' in the foldername, replace the corresponding 'Orientation' with 'flipped'\n",
    "Dataset.loc[Dataset[\"experiment\"].str.contains(\"Flipped\"), \"Orientation\"] = \"flipped\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedDF_TNT = Dataset.groupby(['Genotype','time',])['yball_relative'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the sample size for each Genotype group based on the number of individual flies\n",
    "sample_size = Dataset.groupby('Genotype')['Fly'].nunique()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a line plot of the 'yball' column over time, colored by the 'Genotype' column\n",
    "sns.lineplot(data=GroupedDF_TNT, x='time', y='yball_relative', hue='Genotype', linewidth=1)\n",
    "\n",
    "# Invert the y-axis\n",
    "#plt.gca().invert_yaxis()\n",
    "\n",
    "# Modify the labels of the legend to include the sample size for each Genotype group\n",
    "legend = plt.legend()\n",
    "for text, genotype in zip(legend.texts, sample_size.index):\n",
    "    text.set_text(f'{genotype} (n = {sample_size[genotype]})')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference between consecutive yball_relative values for each fly\n",
    "Dataset['yball_relative_diff'] = Dataset.groupby('Fly')['yball_relative'].diff()\n",
    "\n",
    "# Compute the cumulative_push and cumulative_pull for each fly\n",
    "Dataset['cumulative_push'] = Dataset.apply(lambda x: x['yball_relative_diff'] if x['yball_relative_diff'] > 0 else 0, axis=1).groupby(Dataset['Fly']).cumsum()\n",
    "Dataset['cumulative_pull'] = Dataset.apply(lambda x: -x['yball_relative_diff'] if x['yball_relative_diff'] < 0 else 0, axis=1).groupby(Dataset['Fly']).cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
