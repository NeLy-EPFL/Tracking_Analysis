{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "import platform\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../../..\")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.palettes import Spectral11\n",
    "from bokeh.io import output_notebook\n",
    "import iqplot\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "\n",
    "from Utilities.Utils import *\n",
    "from Utilities.Processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a list of the directories containing the tracking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFolder\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    DataPath = Path(\"/Volumes/Ramdya-Lab/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "# Linux Datapath\n",
    "if platform.system() == \"Linux\":\n",
    "    DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "\n",
    "print(DataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the folders I want to use. For instance, I want to use the folders that have the \"tnt\" in the name as I will explore velocities for different crossings with UAS-TNT. I'm also only getting flies tested in the afternoon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision + Starvation state effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folders = []\n",
    "for folder in DataPath.iterdir():\n",
    "    minfolder = str(folder).lower()\n",
    "    if \"feedingstate\" in minfolder and \"tracked\" in minfolder and \"pm\" in minfolder:\n",
    "        Folders.append(folder)\n",
    "\n",
    "Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the tracking data and generating the dataset\n",
    "\n",
    "In this part, we import the Metatadata .json file and the tracking data .h5 file. Then we compute smoothed fly y positions and generate time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.DataFrame()\n",
    "\n",
    "# Loop over all the .analysis.h5 files in the folder and store the ball y positions and the arena and corridor numbers as metadata\n",
    "Flynum = 0\n",
    "# Loop over all the folders that don't have \"Dark\" in the name\n",
    "for folder in Folders:\n",
    "    print(f\"Adding experiment {folder} to the dataset...\")\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "        \n",
    "        #print (metadata_dict)\n",
    "        \n",
    "    for file in folder.glob(\"**/*flytrack*.analysis.h5\"):\n",
    "        \n",
    "        flypath = file\n",
    "        with h5py.File(flypath.as_posix(), \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            fly_locs = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "            \n",
    "    for file in folder.glob(\"**/*.analysis.h5\"):\n",
    "        #print(file)\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            locations = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "        locations.shape\n",
    "        \n",
    "        if \"Flipped\" in folder.name:\n",
    "            yball : np.ndarray = np.flip(locations[:, :, 1, :])\n",
    "\n",
    "        else:\n",
    "            yball : np.ndarray = locations[:, :, 1, :]   \n",
    "             \n",
    "        yfly = fly_locs[:, :, 1, :]\n",
    "        \n",
    "        # Get the filename from the path\n",
    "        foldername = folder.name\n",
    "\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        corridor = file.parent.name\n",
    "        \n",
    "        # Get the metadata for this arena\n",
    "        arena_key = arena.capitalize()\n",
    "        arena_metadata = {var: pd.Categorical([metadata_dict[var][arena_key]]) for var in metadata_dict}\n",
    "        \n",
    "        Flynum += 1\n",
    "        \n",
    "        # Load the start and end coordinates from coordinates.npy\n",
    "        start, end = np.load(file.parent / 'coordinates.npy')\n",
    "        \n",
    "        # Store the ball y positions, start and end coordinates, and the arena and corridor numbers as metadata\n",
    "        data = {\"Fly\": pd.Categorical([\"Fly\" + str(Flynum)]),\n",
    "                #\"yfly\": [list(yfly[:, 0, 0])], \n",
    "                \"yball\": [list(yball[:, 0, 0])],\n",
    "                \"experiment\": pd.Categorical([foldername]),\n",
    "                \"arena\": pd.Categorical([arena]), \n",
    "                \"corridor\": pd.Categorical([corridor]),\n",
    "                \"start\": pd.Categorical([start]),\n",
    "                \"end\": pd.Categorical([end])}\n",
    "        data.update(arena_metadata)\n",
    "\n",
    "        # Use pandas.concat instead of DataFrame.append\n",
    "        Dataset = pd.concat([Dataset, pd.DataFrame(data)], ignore_index=True) \n",
    "\n",
    "# Explode yfly column to have one row per timepoint\n",
    "\n",
    "#Dataset.drop(columns=[\"Genotye\", \"Date\",], inplace=True)\n",
    "\n",
    "# Dataset = Dataset.explode('yfly')\n",
    "# Dataset['yfly'] = Dataset['yfly'].astype(float)\n",
    "\n",
    "Dataset = Dataset.explode('yball')\n",
    "Dataset['yball'] = Dataset['yball'].astype(float)\n",
    "\n",
    "# Filter parameters\n",
    "cutoff = 0.0015  # desired cutoff frequency of the filter, Hz ,      slightly higher than actual 1.2 Hz\n",
    "order = 1  # sin wave can be approx represented as quadratic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dataset['yfly_smooth'] = butter_lowpass_filter(Dataset['yfly_relative'], cutoff, order)\n",
    "# print('Filtering yfly relative to start...')\n",
    "# Dataset['yfly_SG'] = savgol_lowpass_filter(Dataset['yfly'], 221, 1)\n",
    "\n",
    "# Compute yball_relative relative to start\n",
    "Dataset['yball_relative'] = abs(Dataset['yball'] - Dataset['start'])\n",
    "\n",
    "# Fill missing values using linear interpolation\n",
    "Dataset['yball_relative'] = Dataset['yball_relative'].interpolate(method='linear')\n",
    "\n",
    "Dataset['yball_relative_SG'] = savgol_lowpass_filter(Dataset['yball_relative'], 221, 1)\n",
    "\n",
    "print('Defining frame and time columns...')\n",
    "Dataset[\"Frame\"] = Dataset.groupby(\"Fly\").cumcount()\n",
    "\n",
    "Dataset[\"time\"] = Dataset[\"Frame\"] / 30\n",
    "\n",
    "# Remove the original yfly column\n",
    "\n",
    "print('Removing Frame column...')\n",
    "Dataset.drop(columns=[\"Frame\",], inplace=True)\n",
    "\n",
    "print('Resetting index...')\n",
    "Dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedData = Dataset.groupby([\"time\", \"Light\", \"FeedingState\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "GroupedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Define a function to return alpha value based on Light condition\n",
    "def alpha_val(light_condition):\n",
    "    return 0.5 if light_condition == 'off' else 1.0\n",
    "\n",
    "# Apply the function to the dataset to create a new column 'alpha'\n",
    "GroupedData['alpha'] = GroupedData['Light'].apply(alpha_val)\n",
    "\n",
    "color_dict = {\"fed\": \"blue\", \"starved\": \"orange\", \"starved_noWater\": \"green\"}\n",
    "\n",
    "# Create an empty overlay\n",
    "overlay = hv.NdOverlay({})\n",
    "\n",
    "# Create separate overlays for each FeedingState\n",
    "for feeding_state, color in color_dict.items():\n",
    "    # Filter data for the current FeedingState\n",
    "    data = GroupedData[GroupedData['FeedingState'] == feeding_state]\n",
    "    \n",
    "    # Create curves and overlay them\n",
    "    curves = hv.Curve(data, kdims=['time'], vdims=['yball_relative_SG', 'Light', 'alpha'])\n",
    "    curves = curves.groupby('Light').overlay().opts(opts.Curve(color=color, alpha='alpha'))\n",
    "    \n",
    "    # Add to the overall overlay\n",
    "    overlay[feeding_state] = curves\n",
    "\n",
    "overlay = overlay.opts( \n",
    "        height=750,\n",
    "        width=1000,\n",
    "        #alpha=1,\n",
    "        #line_width=2,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Average relative distance pushed (px)\",\n",
    "        show_grid=True,\n",
    "        fontscale=2,\n",
    "        title=\"\",)\n",
    "# Display the plot\n",
    "overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "\n",
    "# Define a function to map FeedingState to color\n",
    "def feedingstate_to_color(feedingstate):\n",
    "    color_dict = {\"fed\": \"blue\", \"starved\": \"orange\", \"starved_noWater\": \"green\"}\n",
    "    return color_dict.get(feedingstate, 0)\n",
    "\n",
    "# Define a function to map Light condition to alpha\n",
    "def light_to_alpha(light):\n",
    "    return 0.5 if light == 'off' else 1.0  # replace with your actual logic of mapping light to alpha\n",
    "\n",
    "# Apply the mappings to FeedingState and Light condition\n",
    "GroupedData['FeedingStateColor'] = GroupedData['FeedingState'].apply(feedingstate_to_color)\n",
    "#print(GroupedData['FeedingStateColor'])\n",
    "GroupedData['LightAlpha'] = GroupedData['Light'].apply(light_to_alpha)\n",
    "\n",
    "# Define a function to map FeedingState to new labels\n",
    "def feedingstate_to_label(feedingstate):\n",
    "    label_dict = {\"fed\": \"fed\", \"starved\": \"starved with water\", \"starved_noWater\": \"starved without water\"}\n",
    "    return label_dict.get(feedingstate, feedingstate)\n",
    "\n",
    "# Apply the mapping to FeedingState\n",
    "GroupedData['FeedingStateLabel'] = GroupedData['FeedingState'].apply(feedingstate_to_label)\n",
    "\n",
    "# Calculate the sample size\n",
    "sample_size = Dataset.groupby(['Light','FeedingState'])['Fly'].nunique()\n",
    "\n",
    "# Define a function to map FeedingState, Light, and sample size to new labels\n",
    "def feedingstate_light_to_label(row):\n",
    "    label = row['FeedingStateLabel']\n",
    "    light = row['Light']\n",
    "    size = sample_size.loc[light, row['FeedingState']]\n",
    "    return f\"{label}, {light} (n={size})\"\n",
    "\n",
    "# Apply the mapping to FeedingState, Light, and sample size\n",
    "GroupedData['Feeding state, Light'] = GroupedData.apply(feedingstate_light_to_label, axis=1)\n",
    "\n",
    "# Create the curves and apply the options\n",
    "curves = hv.Curve(GroupedData, kdims=['time'], vdims=['yball_relative_SG','FeedingStateLabel', 'Light', 'FeedingStateColor', 'LightAlpha', 'alpha', 'Feeding state, Light']).groupby(['Feeding state, Light']).overlay()\n",
    "curves.opts(opts.Curve(color='FeedingStateColor', alpha='LightAlpha', \n",
    "        height=750,\n",
    "        width=1000,\n",
    "        line_width=1,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Average relative distance pushed (px)\",\n",
    "        show_grid=True,\n",
    "        fontscale=1.5,\n",
    "        title=\"\",\n",
    "        ))\n",
    "curves.opts(legend_position='bottom_right')\n",
    "\n",
    "curves\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot as a png\n",
    "hv.save(\n",
    "    curves,\n",
    "    \"/mnt/labserver/DURRIEU_Matthias/Pictures/FyssenReport/feeding_and_light.png\",\n",
    "    fmt=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot as a html\n",
    "hv.save(\n",
    "    curves,\n",
    "    \"/mnt/labserver/DURRIEU_Matthias/Pictures/FyssenReport/feeding_and_light.html\",\n",
    "    fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to save the datasets \n",
    "\n",
    "DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "\n",
    "GroupedData.to_feather(DataPath / \"230928_GroupedDataFeedingStateLightMean.feather\")\n",
    "\n",
    "Dataset.to_feather(DataPath / \"230928_DatasetFeedingStateLight.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFolder\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    DataPath = Path(\"/Volumes/Ramdya-Lab/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "# Linux Datapath\n",
    "if platform.system() == \"Linux\":\n",
    "    DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Videos\")\n",
    "\n",
    "print(DataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folders = []\n",
    "for folder in DataPath.iterdir():\n",
    "    minfolder = str(folder).lower()\n",
    "    if \"tnt\" in minfolder and \"tracked\" in minfolder and \"pm\" in minfolder:\n",
    "        Folders.append(folder)\n",
    "\n",
    "Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the tracking data and generating the dataset\n",
    "\n",
    "In this part, we import the Metatadata .json file and the tracking data .h5 file. Then we compute smoothed fly y positions and generate time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.DataFrame()\n",
    "\n",
    "# Loop over all the .analysis.h5 files in the folder and store the ball y positions and the arena and corridor numbers as metadata\n",
    "Flynum = 0\n",
    "# Loop over all the folders that don't have \"Dark\" in the name\n",
    "for folder in Folders:\n",
    "    #print(f\"Adding experiment {folder} to the dataset...\")\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "        \n",
    "        print (metadata_dict)\n",
    "        \n",
    "    for file in folder.glob(\"**/*flytrack*.analysis.h5\"):\n",
    "        \n",
    "        flypath = file\n",
    "        with h5py.File(flypath.as_posix(), \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            fly_locs = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "            \n",
    "    for file in folder.glob(\"**/*.analysis.h5\"):\n",
    "        #print(file)\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            dset_names = list(f.keys())\n",
    "            locations = f[\"tracks\"][:].T\n",
    "            node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "        locations.shape\n",
    "        \n",
    "        if \"Flipped\" in folder.name:\n",
    "            yball : np.ndarray = np.flip(locations[:, :, 1, :])\n",
    "\n",
    "        else:\n",
    "            yball : np.ndarray = locations[:, :, 1, :]   \n",
    "             \n",
    "        yfly = fly_locs[:, :, 1, :]\n",
    "        \n",
    "        # Get the filename from the path\n",
    "        foldername = folder.name\n",
    "\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        corridor = file.parent.name\n",
    "        \n",
    "        # Get the metadata for this arena\n",
    "        arena_key = arena.capitalize()\n",
    "        arena_metadata = {var: pd.Categorical([metadata_dict[var][arena_key]]) for var in metadata_dict}\n",
    "        \n",
    "        Flynum += 1\n",
    "        \n",
    "        # Load the start and end coordinates from coordinates.npy\n",
    "        start, end = np.load(file.parent / 'coordinates.npy')\n",
    "        \n",
    "        # Store the ball y positions, start and end coordinates, and the arena and corridor numbers as metadata\n",
    "        data = {\"Fly\": pd.Categorical([\"Fly\" + str(Flynum)]),\n",
    "                #\"yfly\": [list(yfly[:, 0, 0])], \n",
    "                \"yball\": [list(yball[:, 0, 0])],\n",
    "                \"experiment\": pd.Categorical([foldername]),\n",
    "                \"arena\": pd.Categorical([arena]), \n",
    "                \"corridor\": pd.Categorical([corridor]),\n",
    "                \"start\": pd.Categorical([start]),\n",
    "                \"end\": pd.Categorical([end])}\n",
    "        data.update(arena_metadata)\n",
    "\n",
    "        # Use pandas.concat instead of DataFrame.append\n",
    "        Dataset = pd.concat([Dataset, pd.DataFrame(data)], ignore_index=True) \n",
    "\n",
    "# Explode yfly column to have one row per timepoint\n",
    "\n",
    "#Dataset.drop(columns=[\"Genotye\", \"Date\",], inplace=True)\n",
    "\n",
    "# Dataset = Dataset.explode('yfly')\n",
    "# Dataset['yfly'] = Dataset['yfly'].astype(float)\n",
    "\n",
    "Dataset = Dataset.explode('yball')\n",
    "Dataset['yball'] = Dataset['yball'].astype(float)\n",
    "\n",
    "# Filter parameters\n",
    "cutoff = 0.0015  # desired cutoff frequency of the filter, Hz ,      slightly higher than actual 1.2 Hz\n",
    "order = 1  # sin wave can be approx represented as quadratic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dataset['yfly_smooth'] = butter_lowpass_filter(Dataset['yfly_relative'], cutoff, order)\n",
    "# print('Filtering yfly relative to start...')\n",
    "# Dataset['yfly_SG'] = savgol_lowpass_filter(Dataset['yfly'], 221, 1)\n",
    "\n",
    "# Compute yball_relative relative to start\n",
    "Dataset['yball_relative'] = abs(Dataset['yball'] - Dataset['start'])\n",
    "\n",
    "# Fill missing values using linear interpolation\n",
    "Dataset['yball_relative'] = Dataset['yball_relative'].interpolate(method='linear')\n",
    "\n",
    "Dataset['yball_relative_SG'] = savgol_lowpass_filter(Dataset['yball_relative'], 221, 1)\n",
    "\n",
    "print('Defining frame and time columns...')\n",
    "Dataset[\"Frame\"] = Dataset.groupby(\"Fly\").cumcount()\n",
    "\n",
    "Dataset[\"time\"] = Dataset[\"Frame\"] / 30\n",
    "\n",
    "# Remove the original yfly column\n",
    "\n",
    "print('Removing Frame column...')\n",
    "Dataset.drop(columns=[\"Frame\",], inplace=True)\n",
    "\n",
    "print('Resetting index...')\n",
    "Dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupOps = Dataset.groupby(\n",
    "    [\n",
    "        \"time\",\n",
    "        \"Genotype\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupData = GroupOps.mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part below is too heavy to be executed in a notebook. I just ran it in a separate script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confints = GroupOps[\"yball_relative_SG\"].apply(lambda x: draw_bs_ci(x, n_reps=300))\n",
    "\n",
    "# DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "\n",
    "# #GroupOps.to_feather(DataPath / \"230928_GroupedDataTNT.feather\")\n",
    "\n",
    "# Dataset.to_feather(DataPath / \"230928_DatasetTNT.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should now be possible to load the confints from the script output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "\n",
    "Confints = pd.read_feather(DataPath / \"230928_Confints.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confints_process = Confints.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confints[[\"ci_lower\", \"ci_upper\"]] = Confints[\"yball_relative_SG\"].tolist()\n",
    "\n",
    "Confints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GroupData[\"ci_minus\"] and GroupData[\"ci_plus\"] as the columns containing the values of Confints\n",
    "GroupData[\"ci_lower\"] = Confints_process[\"ci_lower\"]\n",
    "GroupData[\"ci_upper\"] = Confints_process[\"ci_upper\"]\n",
    "\n",
    "# Define GroupData[\"ci_minus\"] and GroupData[\"ci_plus\"] as the columns containing the values of Confints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.palettes import Spectral11\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Call once to configure Bokeh to display plots inline in the notebook.\n",
    "output_notebook()\n",
    "\n",
    "# Create a new plot with a title and axis labels\n",
    "p = figure(title=\"yball over time\", x_axis_label='time', y_axis_label='yball_relative_SG')\n",
    "\n",
    "# Get the list of genotypes\n",
    "genotypes = GroupData['Genotype'].unique()\n",
    "\n",
    "genotypes = ['PR', 'TNTxTH', 'TNTxE-PG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each genotype, create a line plot with confidence intervals\n",
    "for i, genotype in enumerate(genotypes):\n",
    "    df_genotype = GroupData[GroupData['Genotype'] == genotype]\n",
    "    source = ColumnDataSource(df_genotype)\n",
    "    \n",
    "    # Draw the line for yball_relative_SG\n",
    "    p.line('time', 'yball_relative_SG', source=source, line_width=2, color=Spectral11[i], legend_label=genotype)\n",
    "    \n",
    "    # Draw the upper and lower bounds for the confidence interval\n",
    "    p.varea(x='time', y1='ci_lower', y2='ci_upper', source=source, fill_color=Spectral11[i], fill_alpha=0.4)\n",
    "\n",
    "# Show the results\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing interaction metrics and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we reuse the code from ConcatClips.py to get the interaction events from each video, and using these we can quantify metrics listed in 'Plotting ideas' in my notebook. These are :\n",
    "\n",
    "* Number of events\n",
    "* Event at which the ball is brought to the end\n",
    "* Time at which the ball is brought to the end\n",
    "* Amount of significant events\n",
    "* Push vs pull \n",
    "* plot the interactions chronology\n",
    "* time between interactions\n",
    "* First meaningful interaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.Ballpushing_utils import *\n",
    "\n",
    "Dataset_list = []\n",
    "Flycount = 0\n",
    "\n",
    "for folder in Folders:\n",
    "    #print(f\"Processing {folder}...\")\n",
    "    # Read the metadata.json file\n",
    "    with open(folder / \"Metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        variables = metadata[\"Variable\"]\n",
    "        metadata_dict = {}\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {}\n",
    "            for arena in range(1, 10):\n",
    "                arena_key = f\"Arena{arena}\"\n",
    "                var_index = variables.index(var)\n",
    "                metadata_dict[var][arena_key] = metadata[arena_key][var_index]\n",
    "\n",
    "        # In the metadata_dict, make all they Arena subkeys lower case\n",
    "\n",
    "        for var in variables:\n",
    "            metadata_dict[var] = {k.lower(): v for k, v in metadata_dict[var].items()}\n",
    "        #print(metadata_dict)\n",
    "\n",
    "        files = list(folder.glob(\"**/*.mp4\"))\n",
    "\n",
    "    for file in files:\n",
    "        #print(file.name)\n",
    "        # Get the arena and corridor numbers from the parent (corridor) and grandparent (arena) folder names\n",
    "        arena = file.parent.parent.name\n",
    "        # print(arena)\n",
    "        corridor = file.parent.name\n",
    "\n",
    "        # Get the Genotype and Dates from the metadata, arena should have a upper case first letter\n",
    "\n",
    "        Genotype = metadata_dict[\"Genotype\"][arena]\n",
    "        #print(f\"Genotype: {Genotype} for arena {arena}\")\n",
    "\n",
    "        Date = metadata_dict[\"Date\"][arena]\n",
    "        # print(f\"Date: {Date} for arena {arena}\")\n",
    "\n",
    "        Light = metadata_dict[\"Light\"][arena]\n",
    "        FeedingState = metadata_dict[\"FeedingState\"][arena]\n",
    "        Period = metadata_dict[\"Period\"][arena]\n",
    "\n",
    "        start, end = np.load(file.parent / 'coordinates.npy')\n",
    "        \n",
    "        dir = file.parent\n",
    "\n",
    "        # Define flypath as the *flytrack*.analysis.h5 file in the same folder as the video\n",
    "        try:\n",
    "            flypath = list(dir.glob(\"*flytrack*.analysis.h5\"))[0]\n",
    "            #print(flypath.name)\n",
    "        except IndexError:\n",
    "            #print(f\"No fly tracking file found for {file.name}, skipping...\")\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # Define ballpath as the *tracked*.analysis.h5 file in the same folder as the video\n",
    "        try:\n",
    "            ballpath = list(dir.glob(\"*tracked*.analysis.h5\"))[0]\n",
    "            #print(ballpath.name)\n",
    "        except IndexError:\n",
    "            #print(f\"No ball tracking file found for {file.name}, skipping...\")\n",
    "            \n",
    "            continue\n",
    "\n",
    "        vidpath = file\n",
    "        vidname = f\"{Genotype}_{Date}_Light_{Light}_{FeedingState}_{Period}_{arena}_{corridor}\"\n",
    "\n",
    "        try:\n",
    "            # Extract interaction events and mark them in the DataFrame\n",
    "            data = extract_interaction_events(ballpath, flypath, mark_in_df=True)\n",
    "            data[\"start\"] = start\n",
    "            data[\"end\"] = end\n",
    "            data[\"Genotype\"] = Genotype\n",
    "            data[\"Date\"] = Date\n",
    "            data[\"arena\"] = arena\n",
    "            data[\"corridor\"] = corridor\n",
    "            Flycount += 1\n",
    "            data[\"Fly\"] = f'Fly {Flycount}'\n",
    "            # Compute yball_relative relative to start\n",
    "            data['yball_relative'] = abs(data['yball_smooth'] - data['start'])\n",
    "\n",
    "            # Fill missing values using linear interpolation\n",
    "            data['yball_relative'] = data['yball_relative'].interpolate(method='linear')\n",
    "            \n",
    "            \n",
    "            # Append the data to the all_data DataFrame\n",
    "            Dataset_list.append(data)\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            traceback_message = traceback.format_exc()\n",
    "            #print(f\"Error processing video {vidname}: {error_message}\")\n",
    "            #print(traceback_message)\n",
    "\n",
    "# Concatenate all dataframes in the list into a single dataframe\n",
    "Dataset = pd.concat(Dataset_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Fly, compute how many unique values of Event that are not None there are\n",
    "\n",
    "# Group the data by Fly and Event\n",
    "GroupedData = Dataset.groupby([\"Fly\", \"Genotype\"]).nunique(['Event']).reset_index()\n",
    "\n",
    "# Count the number of unique values of Event for each Fly\n",
    "#Counts = GroupedData[\"Event\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iqplot\n",
    "\n",
    "p_NumbEvents = iqplot.stripbox(data=GroupedData, q=\"Event\", cats=\"Genotype\", title=\"Number of events\", frame_width= 750, frame_height= 500, tooltips=[(\"Event\", \"@{Event}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "\n",
    "show(p_NumbEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Savepath = Path(\"/mnt/labserver/DURRIEU_Matthias/Pictures/InteractionEvents/\")\n",
    "\n",
    "bokeh.io.save(p_NumbEvents, filename=Savepath / \"NumbEvents.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_unique_events = Dataset.groupby(['Fly', 'Genotype'])['Event'].nunique().groupby('Genotype').mean()\n",
    "\n",
    "average_unique_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event at which the ball is brough to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_threshold = 0\n",
    "\n",
    "# Group by 'Fly' column and find the minimum 'ball_position' for each group\n",
    "max_positions = Dataset.groupby('Fly')['yball_relative'].max()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result = []\n",
    "\n",
    "# For each Fly, find the first event where 'ball_position' is less than or equal to min_position + threshold\n",
    "for fly, max_positions in max_positions.items():\n",
    "    fly_data = Dataset[Dataset['Fly'] == fly]\n",
    "    event = fly_data[fly_data['yball_relative'] >= max_positions - end_threshold]['Event'].iloc[0]\n",
    "    result.append({'Fly': fly, 'Event': event})\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "result_df = pd.DataFrame(result)\n",
    "\n",
    "# Merge the result_df with the original Dataset\n",
    "Dataset = pd.merge(Dataset, result_df, on=['Fly', 'Event'], how='left')\n",
    "\n",
    "# Create the 'IsFinal' column, which is True if 'Event' is in result_df and False otherwise\n",
    "Dataset['IsFinal'] = Dataset['Event'].notna()\n",
    "\n",
    "Dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset[\"Event\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Event' is NaN\n",
    "Dataset = Dataset.dropna(subset=['Event'])\n",
    "\n",
    "# Now extract the event number from the 'Event' column\n",
    "Dataset['EventNumber'] = Dataset['Event'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Filter rows where 'IsFinal' is True\n",
    "final_events = Dataset[Dataset['IsFinal'] == True]\n",
    "\n",
    "final_events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each Fly, find the minimum 'EventNumber' where 'IsFinal' is True\n",
    "result = []\n",
    "for fly in final_events['Fly'].unique():\n",
    "    fly_data = final_events[final_events['Fly'] == fly]\n",
    "    min_event_number = fly_data['EventNumber'].min()\n",
    "    result.append({'Fly': fly, 'MinEventNumber': min_event_number})\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "result_df = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'result_df' with the original Dataset to get the 'Genotype' column\n",
    "result_df = pd.merge(result_df, Dataset[['Fly', 'Genotype']].drop_duplicates(), on='Fly', how='left')\n",
    "\n",
    "# Plot the data\n",
    "p_finalevent = iqplot.stripbox(data=result_df, q=\"MinEventNumber\", cats=\"Genotype\", title=\"Event at which the fly reaches the end of the corridor\", frame_width= 750, frame_height= 500, tooltips=[(\"MinEventNumber\", \"@{MinEventNumber}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "\n",
    "bokeh.io.show(p_finalevent)\n",
    "\n",
    "#TODO: Figure out why this doesn't work properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the plot as html\n",
    "bokeh.io.save(\n",
    "    p_finalevent,\n",
    "    Savepath / \"FinalEvent.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the time at which the final event occurs\n",
    "\n",
    "GroupFly_Time = final_events.groupby([\"Fly\", \"Genotype\"])[\"time\"].first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_finaltime = iqplot.stripbox(data=GroupFly_Time, q=\"time\", cats=\"Genotype\", title=\"Time of the event at which the fly reaches the end of the corridor\", frame_width= 750, frame_height= 500,  tooltips=[(\"time\", \"@{time}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "\n",
    "bokeh.io.show(p_finaltime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_finaltime,\n",
    "    Savepath / \"FinalTime.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = Path(\"/mnt/labserver/DURRIEU_Matthias/Experimental_data/MultiMazeRecorder/Datasets\")\n",
    "\n",
    "Dataset.to_feather(DataPath / \"231002_DatasetTNT_InteractionEvents.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_yball_variation(event_df, threshold=10):\n",
    "    yball_segment = event_df[\"yball_smooth\"]\n",
    "    variation = yball_segment.max() - yball_segment.min()\n",
    "    return variation > threshold\n",
    "\n",
    "# Apply the function and reset the index\n",
    "significant_events = Dataset.groupby([\"Fly\", \"Event\"]).apply(check_yball_variation).reset_index()\n",
    "\n",
    "# Rename the 0 column to 'SignificantEvent'\n",
    "significant_events.rename(columns={0: 'SignificantEvent'}, inplace=True)\n",
    "\n",
    "# Merge the significant_events DataFrame with the original Dataset\n",
    "Dataset = pd.merge(Dataset, significant_events, on=['Fly', 'Event'], how='left')\n",
    "\n",
    "Dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique events that are significant for each 'Fly' and 'Genotype'\n",
    "unique_significant_events = Dataset[Dataset['SignificantEvent'] == True].groupby(['Fly', 'Genotype'])['Event'].nunique()\n",
    "\n",
    "print(unique_significant_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_unique_significant_events = iqplot.stripbox(data=unique_significant_events.reset_index(), q=\"Event\", cats=\"Genotype\", title=\"Number of significant events\", frame_width= 750, frame_height= 500,  tooltips=[(\"Event\", \"@{Event}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "\n",
    "bokeh.io.show(p_unique_significant_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_unique_significant_events,\n",
    "    Savepath / \"NumbSignificantEvents.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the significant events\n",
    "significant_events = Dataset[Dataset['SignificantEvent'] == True]\n",
    "\n",
    "# For each 'Fly', find the first significant event and get its time\n",
    "first_significant_event_times = significant_events.groupby('Fly')['time'].idxmin()\n",
    "\n",
    "# Use these indices to get the corresponding rows from the original DataFrame\n",
    "first_significant_events = Dataset.loc[first_significant_event_times]\n",
    "\n",
    "first_significant_events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each 'Fly' and 'Genotype', find the first significant event and get its time\n",
    "first_significant_event_times = significant_events.groupby(['Fly', 'Genotype'])['time'].idxmin()\n",
    "\n",
    "# Use these indices to get the corresponding rows from the original DataFrame\n",
    "first_significant_events = Dataset.loc[first_significant_event_times]\n",
    "\n",
    "# Plot the data\n",
    "p_first_significant_event_times = iqplot.stripbox(data=first_significant_events.reset_index(), q=\"time\", cats=\"Genotype\", title=\"Time of first significant event\", frame_width= 750, frame_height= 500,   tooltips=[(\"time\", \"@{time}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "bokeh.io.show(p_first_significant_event_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_first_significant_event_times,\n",
    "    Savepath / \"FirstSignificantEvent_Time.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the significant events\n",
    "significant_events = Dataset[Dataset['SignificantEvent'] == True]\n",
    "\n",
    "# For each 'Fly', find the first significant event and get its time\n",
    "first_significant_event_times = significant_events.groupby('Fly')['time'].idxmin()\n",
    "\n",
    "# Use these indices to get the corresponding rows from the original DataFrame\n",
    "first_significant_events = Dataset.loc[first_significant_event_times]\n",
    "\n",
    "first_significant_events[\"EventNumber\"] = first_significant_events.loc[:,\"Event\"].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "first_significant_events.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "p_first_significant_event_numbers = iqplot.stripbox(data=first_significant_events.reset_index(), q=\"EventNumber\", cats=\"Genotype\", title=\"Number of first significant event\", frame_width= 750, frame_height= 500, tooltips=[(\"EventNumber\", \"@{EventNumber}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "bokeh.io.show(p_first_significant_event_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_first_significant_event_numbers,\n",
    "    Savepath / \"FirstSignificantEvent_Number.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Dataset['Event'] = Dataset['Event'].replace('None', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['IsEvent'] = Dataset['Event'].notna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['TimeSinceLastEvent'] = Dataset.groupby('Fly').apply(lambda x: x['time'] - x.loc[x['IsEvent'], 'time'].shift()).reset_index(level=0, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each 'Fly', calculate the time difference between consecutive events\n",
    "Dataset['TimeSinceLastEvent'] = Dataset[Dataset['IsEvent']].groupby('Fly')['time'].diff()\n",
    "\n",
    "# Calculate the average time between events for each 'Fly' and 'Genotype'\n",
    "average_time_between_events = Dataset.groupby(['Fly', 'Genotype'])['TimeSinceLastEvent'].mean().reset_index()\n",
    "\n",
    "# Plot the data\n",
    "p_average_time_between_events = iqplot.stripbox(data=average_time_between_events, q=\"TimeSinceLastEvent\", cats=\"Genotype\", title=\"Average time between events\", frame_width= 750, frame_height= 500, tooltips=[(\"TimeSinceLastEvent\", \"@{TimeSinceLastEvent}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "bokeh.io.show(p_average_time_between_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_average_time_between_events,\n",
    "    Savepath / \"AverageTimeBetweenEvents.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['IsNonEvent'] = Dataset['Event'].isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time difference between consecutive rows\n",
    "Dataset['TimeDifference'] = Dataset.groupby('Fly')['time'].diff()\n",
    "\n",
    "# For non-event rows, sum these time differences\n",
    "Dataset['NonEventTime'] = Dataset.loc[Dataset['IsNonEvent'], 'TimeDifference']\n",
    "\n",
    "# Calculate the cumulative time spent in non-events for each 'Fly' and 'Genotype'\n",
    "cumulative_time_in_non_events = Dataset.groupby(['Fly', 'Genotype'])['NonEventTime'].sum().reset_index()\n",
    "\n",
    "# Plot the data\n",
    "p_cumulative_time_in_non_events = iqplot.stripbox(data=cumulative_time_in_non_events, q=\"NonEventTime\", cats=\"Genotype\", title=\"Cumulative time spent not interacting\", frame_width= 750, frame_height= 500, tooltips=[(\"NonEventTime\", \"@{NonEventTime}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "bokeh.io.show(p_cumulative_time_in_non_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_cumulative_time_in_non_events,\n",
    "    Savepath / \"CumulativeTimeNonEvents.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each 'Fly', 'Event', and 'Genotype', get the timestamp of the first frame\n",
    "first_frame_timestamps = Dataset.groupby(['Fly', 'Event', 'Genotype'])['time'].first().reset_index()\n",
    "\n",
    "# Plot the data\n",
    "p_first_frame_timestamps = iqplot.histogram(data=first_frame_timestamps, q=\"time\", cats=\"Genotype\", title=\"Timestamp of first frame of each event\", frame_width= 750, frame_height= 500, )\n",
    "bokeh.io.show(p_first_frame_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_first_frame_timestamps,\n",
    "    Savepath / \"FirstFrameTimestamps.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference in 'yball_relative' between the start and end of each event\n",
    "yball_relative_diff = Dataset.groupby(['Fly', 'Event'])['yball_relative'].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index(name='yball_relative_diff')\n",
    "\n",
    "# Merge yball_relative_diff with Dataset\n",
    "Dataset = pd.merge(Dataset, yball_relative_diff, on=['Fly', 'Event'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify whether each event is a 'Push', 'Pull', or 'None'\n",
    "Dataset['Event_Type'] = np.where(Dataset['yball_relative_diff'] > 10, 'Push', np.where(Dataset['yball_relative_diff'] < -10, 'Pull', 'None'))\n",
    "\n",
    "# Count the number of each event type for each 'Fly' and 'Genotype'\n",
    "event_type_counts = Dataset.groupby(['Fly', 'Genotype', 'Event_Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Count the number of unique events with event type for each fly and genotype\n",
    "unique_event_type_counts = event_type_counts.groupby(['Fly', 'Genotype'])['Event_Type'].nunique().reset_index(name='Count')\n",
    "\n",
    "# Filter the DataFrame to include only the 'Pull' events\n",
    "pull_events = Dataset[Dataset['Event_Type'] == 'Pull']\n",
    "\n",
    "# Count the number of 'Pull' events for each 'Fly' and 'Genotype'\n",
    "pull_event_counts = pull_events.groupby(['Fly', 'Genotype']).size().reset_index(name='Count')\n",
    "\n",
    "# Count the number of unique 'Pull' events for each 'Fly' and 'Genotype'\n",
    "unique_pull_event_counts = pull_events.groupby(['Fly', 'Genotype'])['Event'].nunique().reset_index(name='Count')\n",
    "\n",
    "# Plot the data\n",
    "\n",
    "\n",
    "p_pull_event_counts = iqplot.stripbox(data=unique_pull_event_counts, q=\"Count\", cats=\"Genotype\", title=\"Number of 'Pull' events\", frame_width= 750, frame_height= 500, tooltips=[(\"Count\", \"@{Count}\"),(\"Genotype\", \"@{Genotype}\")], spread=\"jitter\")\n",
    "bokeh.io.show(p_pull_event_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.save(\n",
    "    p_pull_event_counts,\n",
    "    Savepath / \"PullEventCounts.html\",\n",
    "    #fmt=\"html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Pull and push within event\n",
    "\n",
    "#TODO : Make a dashboard with all the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackinganalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
