{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from icecream import ic\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import utils_behavior\n",
    "\n",
    "from utils_behavior import Ballpushing_utils\n",
    "from utils_behavior import Utils\n",
    "from utils_behavior import Processing\n",
    "from utils_behavior import HoloviewsTemplates\n",
    "\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "import pwlf\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the data\n",
    "\n",
    "Datapath = Utils.get_data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir = Path(\"/mnt/upramdya_data/MD/BallPushing_Learning/Datasets/250318_Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already existing dataset: \n",
    "\n",
    "#Subset = feather.read_feather(\"/mnt/upramdya_data/MD/MultiMazeRecorder/Datasets/Learning/240809_BallposData.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find folders with \"Learning or learning\" in the name as a list\n",
    "\n",
    "folders = [f for f in Datapath.glob(\"*Learning*\")]\n",
    "\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Ballpushing_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestFly_path = \"/mnt/upramdya_data/MD/MultiMazeRecorder/Videos/240723_Learning_Videos_Tracked/arena5/corridor3\"\n",
    "\n",
    "TestFly = Ballpushing_utils.Fly(TestFly_path, experiment_type=\"Learning\", as_individual=True)\n",
    "\n",
    "TestFly.tracking_data.valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestData = Ballpushing_utils.Dataset(TestFly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestData.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestTrial2 = TestData.data[TestData.data[\"trial\"] == 2]\n",
    "\n",
    "TestTrial2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestTrial2[\"trial_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing new module\n",
    "\n",
    "Exps = [Ballpushing_utils.Experiment(f, experiment_type=\"Learning\") for f in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotated_data = Ballpushing_utils.Dataset(Exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotated_data.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset in case of crash\n",
    "\n",
    "Annotated_data.data.to_feather(OutputDir / \"250320_Annotated_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "AnnotatedData = feather.read_feather(OutputDir / \"250320_Annotated_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the experiments\n",
    "\n",
    "Exps = [Ballpushing_utils.Experiment(f) for f in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Ballpushing_utils.Dataset(Exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a feather file\n",
    "\n",
    "Data.data.to_feather(OutputDir / \"250318_BallposData.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting ball positions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path(\"/mnt/upramdya_data/MD/BallPushing_Learning/Plots/Durations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Annotated_data.data, x=\"trial_time\", y=\"distance_ball_0\", hue=\"trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset = Annotated_data.data[Annotated_data.data[\"trial\"] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Subset, x=\"trial_time\", y=\"distance_ball_0\", hue=\"trial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting subsequent trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data columns\n",
    "  \n",
    "Data.data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subset of the data keeping only yball, yball_smooth,\n",
    "\n",
    "Subset = Data.data[[\"frame\",\"time\", \"y_ball_0\", \"distance_ball_0\", \"fly\", \"Peak\", \"Date\"]]\n",
    "\n",
    "# Save this subset to a feather file\n",
    "\n",
    "#feather.write_feather(Subset, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Datasets/Learning/240809_BallposData.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the first fly and store it to use it for the next steps\n",
    "\n",
    "fly = Subset[\"fly\"].iloc[0]\n",
    "\n",
    "#TestData = Subset[Subset[\"fly\"] == fly]\n",
    "\n",
    "# Now take one fly and plot the yball_relative\n",
    "\n",
    "#Subset[Subset[\"fly\"] == fly].hvplot(x=\"time\", y=\"yball_relative\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative of the yball_relative\n",
    "\n",
    "Subset[\"yball_relative_derivative\"] = Subset[\"distance_ball_0\"].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Negative peak detection on the derivative\n",
    "\n",
    "Peaks = find_peaks(-Subset[\"yball_relative_derivative\"], height=0.23, distance=500)\n",
    "\n",
    "#Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"time\" and \"Frame\" columns from the dataset\n",
    "frame_to_time_mapping = dict(zip(Subset[\"frame\"], Subset[\"time\"]))\n",
    "\n",
    "# Convert Peaks data to lists\n",
    "x_peaks = Peaks[0].tolist()\n",
    "y_peaks = Peaks[1][\"peak_heights\"].tolist()\n",
    "\n",
    "# Map frame indices to time values\n",
    "x_peaks_time = [frame_to_time_mapping[frame] for frame in x_peaks]\n",
    "\n",
    "# Plot the derivative and the peaks\n",
    "scatter_plot = Subset.hvplot(x=\"time\", y=\"yball_relative_derivative\", kind=\"scatter\")\n",
    "peaks_plot = hv.Scatter(\n",
    "    (x_peaks_time, y_peaks), kdims=[\"time\"], vdims=[\"peak_heights\"], label=\"Peaks\"\n",
    ")\n",
    "\n",
    "# Combine the plots\n",
    "combined_plot = scatter_plot * peaks_plot\n",
    "#combined_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Group by individual flies\n",
    "grouped = Subset.groupby(\"fly\")\n",
    "\n",
    "\n",
    "# Function to process each group\n",
    "def process_group(group):\n",
    "    # Find peaks in the derivative\n",
    "    peaks, _ = find_peaks(\n",
    "        -group[\"yball_relative_derivative\"], height=0.3, distance=500\n",
    "    )\n",
    "\n",
    "    # Debug: Print the peaks detected for each fly\n",
    "    print(f\"Fly: {group['fly'].iloc[0]}, Peaks: {peaks}\")\n",
    "\n",
    "    # Initialize the \"Trial\" column\n",
    "    group[\"Trial\"] = 0\n",
    "\n",
    "    # Assign trial numbers based on peak positions\n",
    "    trial_number = 1\n",
    "    previous_peak = 0\n",
    "    for peak in peaks:\n",
    "        group.iloc[previous_peak : peak + 1, group.columns.get_loc(\"Trial\")] = (\n",
    "            trial_number\n",
    "        )\n",
    "        trial_number += 1\n",
    "        previous_peak = peak + 1\n",
    "\n",
    "    # Assign the last trial number to the remaining rows\n",
    "    group.iloc[previous_peak:, group.columns.get_loc(\"Trial\")] = trial_number\n",
    "\n",
    "    # Debug: Print the trial numbers assigned for each fly\n",
    "    print(group[[\"fly\", \"frame\", \"Trial\"]].head(10))\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the function to each group and combine the results\n",
    "Trials = grouped.apply(process_group).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many unique trials there are grouped by fly\n",
    "Trials.groupby(\"fly\")[\"Trial\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop any fly that has less than 2 trials\n",
    "\n",
    "Filtered = Trials.groupby(\"fly\").filter(lambda x: x[\"Trial\"].nunique() > 1)\n",
    "\n",
    "Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered.groupby(\"fly\")[\"Trial\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Trial and time, then compute the mean of yball_relative\n",
    "averaged_data = (\n",
    "    Filtered.groupby([\"Trial\", \"time\"])[\"distance_ball_0\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "# Plot the averaged yball_relative values\n",
    "# averaged_data.hvplot(\n",
    "#     x=\"time\", y=\"yball_relative\", by=\"Trial\", kind=\"line\", legend=\"top_left\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among the flies that have more than 1 trial, we need to drop the Frames that belong to a plateau, meaning the frames where the yball_relative is above a certain threshold\n",
    "\n",
    "\n",
    "# Function to clean each trial\n",
    "def clean_trial(group):\n",
    "    # Drop the first 500 frames\n",
    "    group = group.iloc[500:]\n",
    "\n",
    "    # Find the index where yball_relative reaches its max value for the first time\n",
    "    max_index = group[\"distance_ball_0\"].idxmax()\n",
    "\n",
    "    # Trim the trial data to end at this maximum value\n",
    "    group = group.loc[:max_index]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the function to each group and combine the results\n",
    "cleaned_data = (\n",
    "    Filtered.groupby([\"fly\", \"Trial\"]).apply(clean_trial).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Display the cleaned dataset\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of unique values for each trial\n",
    "\n",
    "trial_counts = cleaned_data.groupby(\"Trial\")[\"fly\"].nunique()\n",
    "\n",
    "trial_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only trials that have atleast 30 flies\n",
    "\n",
    "cleaned_data = cleaned_data[\n",
    "    cleaned_data[\"Trial\"].isin(trial_counts[trial_counts >= 30].index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute trial duration\n",
    "def compute_trial_duration(group):\n",
    "    duration = group[\"time\"].max() - group[\"time\"].min()\n",
    "    return pd.Series({\"duration\": duration})\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the trial durations\n",
    "trial_durations = (\n",
    "    cleaned_data.groupby([\"fly\", \"Trial\"]).apply(compute_trial_duration).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute trial duration\n",
    "def compute_trial_duration(group):\n",
    "    duration = group[\"time\"].max() - group[\"time\"].min()\n",
    "    return pd.Series({\"duration\": duration})\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the trial durations\n",
    "trial_durations = (\n",
    "    AnnotatedData.groupby([\"fly\", \"trial\"]).apply(compute_trial_duration).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to compute trial duration as max value of trial_time grouped by trial\n",
    "\n",
    "\n",
    "def compute_trial_duration(group):\n",
    "    duration = group[\"trial_time\"].max() - group[\"trial_time\"].min()\n",
    "    return pd.Series({\"duration\": duration})\n",
    "\n",
    "\n",
    "# Compute trial duration for each trial\n",
    "\n",
    "trial_durations = (\n",
    "    AnnotatedData.groupby([\"fly\", \"trial\"]).apply(compute_trial_duration).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to keep only trials 1 to 4\n",
    "\n",
    "Subset = trial_durations[trial_durations[\"trial\"] <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    data=Subset,\n",
    "    x=\"trial\",\n",
    "    y=\"duration\",\n",
    "    color=\"black\",\n",
    "    showcaps=False,\n",
    "    boxprops={\"facecolor\": \"none\", \"edgecolor\": \"black\"},\n",
    "    whiskerprops={\"color\": \"black\"},\n",
    ")\n",
    "sns.stripplot(data=Subset, x=\"trial\", y=\"duration\", hue=\"trial\", size=4, alpha=0.5, palette = \"viridis\")\n",
    "\n",
    "# Add lines connecting trials for each fly\n",
    "for fly in Subset[\"fly\"].unique():\n",
    "    fly_data = Subset[Subset[\"fly\"] == fly]\n",
    "    plt.plot(\n",
    "        fly_data[\"trial\"].values - 1,\n",
    "        fly_data[\"duration\"].values,\n",
    "        color=\"gray\",\n",
    "        alpha=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate mean duration and confidence intervals\n",
    "mean_durations = trial_durations.groupby('trial')['duration'].mean()\n",
    "conf_intervals = trial_durations.groupby('trial')['duration'].apply(lambda x: Processing.draw_bs_ci(x.values))\n",
    "\n",
    "# Plot mean duration with confidence intervals\n",
    "trials = mean_durations.index\n",
    "mean_values = mean_durations.values\n",
    "lower_bounds = conf_intervals.apply(lambda x: x[0]).values\n",
    "upper_bounds = conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "plt.plot(trials, mean_values, color='blue', label='Mean Duration', marker='o')\n",
    "plt.fill_between(trials, lower_bounds, upper_bounds, color='blue', alpha=0.2, label='95% CI')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking only at flies that manage atleast 4 trials\n",
    "\n",
    "This is to check whether these flies do improve or not (as opposed to : these would be super flies that were always good compared to the others that just can't go above 2 or 3 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnotatedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnotatedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Filtered = AnnotatedData[AnnotatedData[\"trial\"] <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_4trials_Filtered = Data_Filtered.groupby(\"fly\").filter(\n",
    "    lambda x: x[\"trial\"].nunique() >= 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to compute trial duration as max value of trial_time grouped by trial\n",
    "\n",
    "def compute_trial_duration(group):\n",
    "    duration = group[\"trial_time\"].max() - group[\"trial_time\"].min()\n",
    "    return pd.Series({\"duration\": duration})\n",
    "\n",
    "# Compute trial duration for each trial\n",
    "\n",
    "trial_durations = (\n",
    "    Data_4trials_Filtered.groupby([\"fly\", \"trial\"])\n",
    "    .apply(compute_trial_duration)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trial durations\n",
    "\n",
    "sns.boxplot(data=trial_durations, x=\"trial\", y=\"duration\")\n",
    "sns.stripplot(data=trial_durations, x=\"trial\", y=\"duration\", color=\"black\", size=2, alpha=0.5)\n",
    "\n",
    "# Add lines connecting trials for each fly\n",
    "\n",
    "for fly in trial_durations[\"fly\"].unique():\n",
    "    \n",
    "    fly_data = trial_durations[trial_durations[\"fly\"] == fly]\n",
    "    plt.plot(fly_data[\"trial\"].values-1, fly_data[\"duration\"].values, color=\"gray\", alpha=0.5)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample size for each trial\n",
    "\n",
    "sample_sizes = trial_durations.groupby(\"trial\")[\"fly\"].nunique()\n",
    "\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing delta improvement fly by fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure data is sorted by fly and trial\n",
    "trial_durations = trial_durations.sort_values(['fly', 'trial'])\n",
    "\n",
    "# Create a new dataframe to store improvement data\n",
    "improvement_df = pd.DataFrame()\n",
    "\n",
    "# Calculate improvement for each fly\n",
    "for fly, group in trial_durations.groupby('fly'):\n",
    "    # Make a copy of the group to avoid SettingWithCopyWarning\n",
    "    fly_data = group.copy()\n",
    "\n",
    "    # Sort by trial number\n",
    "    fly_data = fly_data.sort_values(\"trial\")\n",
    "\n",
    "    # Calculate the difference in duration between consecutive trials\n",
    "    # Negative values mean the fly got faster (improved)\n",
    "    fly_data['improvement'] = fly_data['duration'].diff()\n",
    "\n",
    "    # Create a column for the trial transition (e.g., \"1 to 2\")\n",
    "    fly_data[\"Trial_transition\"] = fly_data[\"trial\"].apply(lambda x: f\"{x-1} to {x}\")\n",
    "\n",
    "    # Add to our result dataframe\n",
    "    improvement_df = pd.concat([improvement_df, fly_data], ignore_index=True)\n",
    "\n",
    "# Drop the rows with NaN improvement (first trial for each fly)\n",
    "improvement_df = improvement_df.dropna(subset=['improvement'])\n",
    "\n",
    "# For easier interpretation, create a column where positive values mean improvement\n",
    "improvement_df['improvement_intuitive'] = -improvement_df['improvement']\n",
    "\n",
    "# Visualize the improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=improvement_df, x='Trial_transition', y='improvement_intuitive')\n",
    "sns.stripplot(data=improvement_df, x='Trial_transition', y='improvement_intuitive', \n",
    "              color='black', size=2, alpha=0.5)\n",
    "\n",
    "# Add lines connecting trials for each fly\n",
    "for fly, group in improvement_df.groupby('fly'):\n",
    "    plt.plot(group['Trial_transition'], group['improvement_intuitive'], color='gray', alpha=0.5)\n",
    "\n",
    "plt.title('Improvement in Trial Duration')\n",
    "plt.ylabel('Improvement (seconds) - Positive is better')\n",
    "plt.xlabel('Trial Transition')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a reference line at y=0\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data is sorted by fly and trial\n",
    "trial_durations = trial_durations.sort_values(['fly', 'Trial'])\n",
    "\n",
    "# Create a figure with appropriate size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create boxplot for overall distribution\n",
    "sns.boxplot(data=trial_durations, x='Trial', y='duration', color='lightgray')\n",
    "\n",
    "# Add individual points\n",
    "sns.stripplot(data=trial_durations, x='Trial', y='duration', \n",
    "              color='black', size=4, alpha=0.5, jitter=True)\n",
    "\n",
    "# Add lines connecting trials for each fly\n",
    "for fly_name, fly_data in trial_durations.groupby('fly'):\n",
    "    # Sort by trial to ensure correct line connections\n",
    "    fly_data = fly_data.sort_values('Trial')\n",
    "    \n",
    "    # Plot the line for this specific fly\n",
    "    plt.plot(fly_data['Trial']-1, fly_data['duration'], 'o-', \n",
    "             linewidth=1, alpha=0.7, markersize=0,\n",
    "             label=f\"Fly {fly_name}\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Trial Duration with Individual Fly Tracking', fontsize=14)\n",
    "plt.ylabel('Duration (seconds)', fontsize=12)\n",
    "plt.xlabel('Trial Number', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# If there are many flies, you might want to exclude the legend\n",
    "# Uncomment the line below to include the legend\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement for each fly\n",
    "improvement_df = pd.DataFrame()\n",
    "\n",
    "for fly, group in Subset.groupby('fly'):\n",
    "    # Make a copy and sort by trial\n",
    "    fly_data = group.copy().sort_values(\"trial\")\n",
    "\n",
    "    # Calculate improvement (difference in duration)\n",
    "    fly_data['improvement'] = fly_data['duration'].diff()\n",
    "\n",
    "    # Create trial transition labels\n",
    "    fly_data[\"Trial_transition\"] = fly_data[\"trial\"].apply(lambda x: f\"{x-1} to {x}\")\n",
    "\n",
    "    # Add to our results\n",
    "    improvement_df = pd.concat([improvement_df, fly_data], ignore_index=True)\n",
    "\n",
    "# Drop the first trial for each fly (no improvement data available)\n",
    "improvement_df = improvement_df.dropna(subset=['improvement'])\n",
    "\n",
    "# For intuitive interpretation: negative becomes positive (faster is better)\n",
    "improvement_df['improvement_intuitive'] = -improvement_df['improvement']\n",
    "\n",
    "# Calculate mean improvement and bootstrapped confidence intervals\n",
    "mean_improvement = improvement_df.groupby('Trial_transition')['improvement_intuitive'].mean()\n",
    "conf_intervals = improvement_df.groupby('Trial_transition')['improvement_intuitive'].apply(lambda x: Processing.draw_bs_ci(x.values))\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean improvement with confidence intervals\n",
    "trial_transitions = mean_improvement.index\n",
    "mean_values = mean_improvement.values\n",
    "lower_bounds = conf_intervals.apply(lambda x: x[0]).values\n",
    "upper_bounds = conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "plt.plot(trial_transitions, mean_values, color='blue', label='Mean Improvement', marker='o')\n",
    "plt.fill_between(trial_transitions, lower_bounds, upper_bounds, color='blue', alpha=0.2, label='95% CI')\n",
    "\n",
    "plt.title('Average Improvement in Trial Duration with 95% Confidence Intervals', fontsize=14)\n",
    "plt.ylabel('Improvement (seconds) - Positive is better', fontsize=12)\n",
    "plt.xlabel('Trial Transition', fontsize=12)\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Reference line\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation and coefficient of variation per trial\n",
    "variability = Subset.groupby('trial')['duration'].agg(['mean', 'std'])\n",
    "variability['CV'] = variability['std'] / variability['mean']\n",
    "\n",
    "# Print the variability DataFrame to inspect the values\n",
    "print(variability)\n",
    "\n",
    "# Plot variability metrics in separate subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Plot standard deviation\n",
    "ax[0].plot(variability.index, variability['std'], label='Standard Deviation', marker='o')\n",
    "ax[0].set_title('Standard Deviation of Trial Durations Across Trials')\n",
    "ax[0].set_xlabel('Trial Number')\n",
    "ax[0].set_ylabel('Standard Deviation')\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot coefficient of variation\n",
    "ax[1].plot(variability.index, variability['CV'], label='Coefficient of Variation', marker='o')\n",
    "ax[1].set_title('Coefficient of Variation of Trial Durations Across Trials')\n",
    "ax[1].set_xlabel('Trial Number')\n",
    "ax[1].set_ylabel('Coefficient of Variation')\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation and coefficient of variation per trial\n",
    "variability = Subset.groupby('trial')['duration'].agg(['mean', 'std'])\n",
    "variability['CV'] = variability['std'] / variability['mean']\n",
    "\n",
    "# Print the variability DataFrame to inspect the values\n",
    "print(variability)\n",
    "\n",
    "# Calculate bootstrapped confidence intervals for standard deviation\n",
    "std_conf_intervals = Subset.groupby(\"trial\")[\"duration\"].apply(\n",
    "    lambda x: Processing.draw_bs_ci(x.values, func=np.std)\n",
    ")\n",
    "\n",
    "# Plot variability metrics in separate subplots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "# Plot standard deviation with confidence intervals\n",
    "trials = variability.index\n",
    "std_values = variability['std'].values\n",
    "std_lower_bounds = std_conf_intervals.apply(lambda x: x[0]).values\n",
    "std_upper_bounds = std_conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "ax[0].plot(trials, std_values, label='Standard Deviation', marker='o')\n",
    "ax[0].fill_between(trials, std_lower_bounds, std_upper_bounds, color='blue', alpha=0.2, label='95% CI')\n",
    "ax[0].set_title('Standard Deviation of Trial Durations Across Trials')\n",
    "ax[0].set_xlabel('Trial Number')\n",
    "ax[0].set_ylabel('Standard Deviation')\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot coefficient of variation\n",
    "ax[1].plot(variability.index, variability['CV'], label='Coefficient of Variation', marker='o')\n",
    "ax[1].set_title('Coefficient of Variation of Trial Durations Across Trials')\n",
    "ax[1].set_xlabel('Trial Number')\n",
    "ax[1].set_ylabel('Coefficient of Variation')\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot mean duration with confidence intervals\n",
    "mean_durations = Subset.groupby(\"trial\")[\"duration\"].mean()\n",
    "mean_conf_intervals = Subset.groupby(\"trial\")[\"duration\"].apply(\n",
    "    lambda x: Processing.draw_bs_ci(x.values)\n",
    ")\n",
    "\n",
    "mean_values = mean_durations.values\n",
    "mean_lower_bounds = mean_conf_intervals.apply(lambda x: x[0]).values\n",
    "mean_upper_bounds = mean_conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "ax[2].plot(trials, mean_values, color='blue', label='Mean Duration', marker='o')\n",
    "ax[2].fill_between(trials, mean_lower_bounds, mean_upper_bounds, color='blue', alpha=0.2, label='95% CI')\n",
    "ax[2].set_title('Mean Duration with 95% Confidence Intervals')\n",
    "ax[2].set_xlabel('Trial Number')\n",
    "ax[2].set_ylabel('Mean Duration')\n",
    "ax[2].legend()\n",
    "ax[2].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between consecutive trials for each fly\n",
    "correlations = []\n",
    "for fly, group in trial_durations.groupby('fly'):\n",
    "    group = group.sort_values('Trial')\n",
    "    if len(group) > 1:  # Ensure there are at least two trials\n",
    "        corr = group['duration'].autocorr()\n",
    "        correlations.append({'fly': fly, 'correlation': corr})\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "correlation_df = pd.DataFrame(correlations)\n",
    "\n",
    "# Plot correlation coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=correlation_df, x='fly', y='correlation', palette='viridis')\n",
    "plt.title('Correlation Between Consecutive Trials for Each Fly')\n",
    "plt.xlabel('Fly')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "# Group data by trial and extract durations\n",
    "groups = [group['duration'].values for _, group in trial_durations.groupby('Trial')]\n",
    "\n",
    "# Perform Levene's test for equality of variances\n",
    "stat, p_value = levene(*groups)\n",
    "print(f\"Levene's Test Statistic: {stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "slopes = []\n",
    "for fly, group in trial_durations.groupby('fly'):\n",
    "    group = group.sort_values('Trial')\n",
    "    if len(group) > 1:  # Ensure there are enough trials for regression\n",
    "        X = group['Trial'].values.reshape(-1, 1)\n",
    "        y = group['duration'].values\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        slope = model.coef_[0]  # Extract slope (rate of change)\n",
    "        slopes.append({'fly': fly, 'slope': slope})\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "slopes_df = pd.DataFrame(slopes)\n",
    "\n",
    "# Plot slopes of improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=slopes_df, x='fly', y='slope', palette='coolwarm')\n",
    "plt.title('Rate of Improvement Across Flies')\n",
    "plt.xlabel('Fly')\n",
    "plt.ylabel('Slope of Duration Change (seconds per trial)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual trajectories and mean trend line\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for fly_name, fly_data in trial_durations.groupby('fly'):\n",
    "    plt.plot(fly_data['Trial'], fly_data['duration'], 'o-', alpha=0.6, label=f\"Fly {fly_name}\")\n",
    "\n",
    "mean_trend = trial_durations.groupby('Trial')['duration'].mean()\n",
    "plt.plot(mean_trend.index, mean_trend.values, 'k--', label='Mean Trend', linewidth=2)\n",
    "\n",
    "plt.title('Harmonization of Trial Durations Across Flies')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 trials subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure data is sorted by fly and trial\n",
    "trial_durations = trial_durations.sort_values(['fly', 'trial'])\n",
    "\n",
    "# Create a new dataframe to store improvement data\n",
    "improvement_df = pd.DataFrame()\n",
    "\n",
    "# Calculate improvement for each fly\n",
    "for fly, group in trial_durations.groupby('fly'):\n",
    "    # Make a copy of the group to avoid SettingWithCopyWarning\n",
    "    fly_data = group.copy()\n",
    "    \n",
    "    # Sort by trial number\n",
    "    fly_data = fly_data.sort_values('trial')\n",
    "    \n",
    "    # Calculate the difference in duration between consecutive trials\n",
    "    # Negative values mean the fly got faster (improved)\n",
    "    fly_data['improvement'] = fly_data['duration'].diff()\n",
    "    \n",
    "    # Create a column for the trial transition (e.g., \"1 to 2\")\n",
    "    fly_data['Trial_transition'] = fly_data['trial'].apply(lambda x: f\"{x-1} to {x}\")\n",
    "    \n",
    "    # Add to our result dataframe\n",
    "    improvement_df = pd.concat([improvement_df, fly_data], ignore_index=True)\n",
    "\n",
    "# Drop the rows with NaN improvement (first trial for each fly)\n",
    "improvement_df = improvement_df.dropna(subset=['improvement'])\n",
    "\n",
    "# For easier interpretation, create a column where positive values mean improvement\n",
    "improvement_df['improvement_intuitive'] = -improvement_df['improvement']\n",
    "\n",
    "# Visualize the improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=improvement_df, x='Trial_transition', y='improvement_intuitive')\n",
    "sns.stripplot(data=improvement_df, x='Trial_transition', y='improvement_intuitive', \n",
    "              color='black', size=2, alpha=0.5)\n",
    "\n",
    "# Add lines connecting trials for each fly\n",
    "for fly, group in improvement_df.groupby('fly'):\n",
    "    plt.plot(group['Trial_transition'], group['improvement_intuitive'], color='gray', alpha=0.5)\n",
    "\n",
    "plt.title('Improvement in Trial Duration')\n",
    "plt.ylabel('Improvement (seconds) - Positive is better')\n",
    "plt.xlabel('Trial Transition')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a reference line at y=0\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement for each fly\n",
    "improvement_df = pd.DataFrame()\n",
    "\n",
    "for fly, group in trial_durations.groupby(\"fly\"):\n",
    "    # Make a copy and sort by trial\n",
    "    fly_data = group.copy().sort_values(\"trial\")\n",
    "\n",
    "    # Calculate improvement (difference in duration)\n",
    "    fly_data[\"improvement\"] = fly_data[\"duration\"].diff()\n",
    "\n",
    "    # Create trial transition labels\n",
    "    fly_data[\"Trial_transition\"] = fly_data[\"trial\"].apply(lambda x: f\"{x-1} to {x}\")\n",
    "\n",
    "    # Add to our results\n",
    "    improvement_df = pd.concat([improvement_df, fly_data], ignore_index=True)\n",
    "\n",
    "# Drop the first trial for each fly (no improvement data available)\n",
    "improvement_df = improvement_df.dropna(subset=[\"improvement\"])\n",
    "\n",
    "# For intuitive interpretation: negative becomes positive (faster is better)\n",
    "improvement_df[\"improvement_intuitive\"] = -improvement_df[\"improvement\"]\n",
    "\n",
    "# Calculate mean improvement and bootstrapped confidence intervals\n",
    "mean_improvement = improvement_df.groupby(\"Trial_transition\")[\n",
    "    \"improvement_intuitive\"\n",
    "].mean()\n",
    "conf_intervals = improvement_df.groupby(\"Trial_transition\")[\n",
    "    \"improvement_intuitive\"\n",
    "].apply(lambda x: Processing.draw_bs_ci(x.values))\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean improvement with confidence intervals\n",
    "trial_transitions = mean_improvement.index\n",
    "mean_values = mean_improvement.values\n",
    "lower_bounds = conf_intervals.apply(lambda x: x[0]).values\n",
    "upper_bounds = conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "plt.plot(\n",
    "    trial_transitions, mean_values, color=\"orange\", label=\"Mean Improvement\", marker=\"o\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    trial_transitions,\n",
    "    lower_bounds,\n",
    "    upper_bounds,\n",
    "    color=\"orange\",\n",
    "    alpha=0.2,\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Average Improvement in Trial Duration with 95% Confidence Intervals\", fontsize=14\n",
    ")\n",
    "plt.ylabel(\"Improvement (seconds) - Positive is better\", fontsize=12)\n",
    "plt.xlabel(\"Trial Transition\", fontsize=12)\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"--\")  # Reference line\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation and coefficient of variation per trial\n",
    "variability = trial_durations.groupby('trial')['duration'].agg(['mean', 'std'])\n",
    "variability['CV'] = variability['std'] / variability['mean']\n",
    "\n",
    "# Print the variability DataFrame to inspect the values\n",
    "print(variability)\n",
    "\n",
    "# Calculate bootstrapped confidence intervals for standard deviation\n",
    "std_conf_intervals = trial_durations.groupby('trial')['duration'].apply(lambda x: Processing.draw_bs_ci(x.values, func=np.std))\n",
    "\n",
    "# Plot variability metrics in separate subplots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "# Plot standard deviation with confidence intervals\n",
    "trials = variability.index\n",
    "std_values = variability['std'].values\n",
    "std_lower_bounds = std_conf_intervals.apply(lambda x: x[0]).values\n",
    "std_upper_bounds = std_conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "ax[0].plot(trials, std_values, label='Standard Deviation', marker='o', color=\"orange\")\n",
    "ax[0].fill_between(trials, std_lower_bounds, std_upper_bounds, color='orange', alpha=0.2, label='95% CI')\n",
    "ax[0].set_title('Standard Deviation of Trial Durations Across Trials')\n",
    "ax[0].set_xlabel('Trial Number')\n",
    "ax[0].set_ylabel('Standard Deviation')\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot coefficient of variation\n",
    "ax[1].plot(variability.index, variability['CV'], label='Coefficient of Variation', marker='o')\n",
    "ax[1].set_title('Coefficient of Variation of Trial Durations Across Trials')\n",
    "ax[1].set_xlabel('Trial Number')\n",
    "ax[1].set_ylabel('Coefficient of Variation')\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot mean duration with confidence intervals\n",
    "mean_durations = trial_durations.groupby('trial')['duration'].mean()\n",
    "mean_conf_intervals = trial_durations.groupby('trial')['duration'].apply(lambda x: Processing.draw_bs_ci(x.values))\n",
    "\n",
    "mean_values = mean_durations.values\n",
    "mean_lower_bounds = mean_conf_intervals.apply(lambda x: x[0]).values\n",
    "mean_upper_bounds = mean_conf_intervals.apply(lambda x: x[1]).values\n",
    "\n",
    "ax[2].plot(trials, mean_values, color='orange', label='Mean Duration', marker='o')\n",
    "ax[2].fill_between(trials, mean_lower_bounds, mean_upper_bounds, color='orange', alpha=0.2, label='95% CI')\n",
    "ax[2].set_title('Mean Duration with 95% Confidence Intervals')\n",
    "ax[2].set_xlabel('Trial Number')\n",
    "ax[2].set_ylabel('Mean Duration')\n",
    "ax[2].legend()\n",
    "ax[2].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulated trials solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate cumulative trials solved over time\n",
    "def generate_cumulative_trials(fly_data):\n",
    "    # Sort by time\n",
    "    fly_data = fly_data.sort_values('time')\n",
    "    \n",
    "    # Find points where trial number changes (a new trial starts)\n",
    "    # Each change means the previous trial was completed\n",
    "    trial_changes = fly_data.loc[fly_data['Trial'].diff().fillna(0) > 0].copy()\n",
    "    \n",
    "    # Create a cumulative trials dataframe\n",
    "    time_points = []\n",
    "    trial_counts = []\n",
    "    \n",
    "    # Add starting point (0 trials at beginning of experiment)\n",
    "    time_points.append(fly_data['time'].min())\n",
    "    trial_counts.append(0)\n",
    "    \n",
    "    # Add each completion point\n",
    "    for _, row in trial_changes.iterrows():\n",
    "        time_points.append(row['time'])\n",
    "        # When we see trial N start, it means N-1 trials have been completed\n",
    "        trials_completed = row['Trial'] - 1\n",
    "        trial_counts.append(trials_completed)\n",
    "    \n",
    "    # Create dataframe with time series\n",
    "    cumulative_df = pd.DataFrame({\n",
    "        'time': time_points,\n",
    "        'cumulative_trials': trial_counts,\n",
    "        'fly': fly_data['fly'].iloc[0]\n",
    "    })\n",
    "    \n",
    "    return cumulative_df\n",
    "\n",
    "# Apply to each fly and combine results\n",
    "cumulative_trials_data = []\n",
    "for fly, fly_data in Filtered.groupby('fly'):\n",
    "    cum_data = generate_cumulative_trials(fly_data)\n",
    "    cumulative_trials_data.append(cum_data)\n",
    "\n",
    "cumulative_df = pd.concat(cumulative_trials_data).reset_index(drop=True)\n",
    "\n",
    "# Plot cumulative trials over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot individual fly trajectories\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    plt.plot(group['time'], group['cumulative_trials'], 'o-', \n",
    "             linewidth=2, markersize=5, alpha=0.7,\n",
    "             label=f\"Fly {fly}\")\n",
    "\n",
    "# Add reference line showing constant rate (optional)\n",
    "# max_time = cumulative_df['time'].max()\n",
    "# max_trials = cumulative_df['cumulative_trials'].max()\n",
    "# plt.plot([0, max_time], [0, max_trials], 'k--', alpha=0.3, label='Constant Rate')\n",
    "\n",
    "plt.title('Cumulative Trials Solved Over Time', fontsize=14)\n",
    "plt.xlabel('Time (seconds)', fontsize=12)\n",
    "plt.ylabel('Cumulative Trials Completed', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the original dataset (before cleaning/cropping)\n",
    "original_data = Filtered.copy()  # Using Filtered which has the Trial column but before cleaning\n",
    "\n",
    "# Function to generate cumulative trials using the original dataset\n",
    "def generate_cumulative_trials_from_original(data):\n",
    "    # Group by fly\n",
    "    all_cumulative_data = []\n",
    "    \n",
    "    for fly, fly_data in data.groupby('fly'):\n",
    "        # Sort by time\n",
    "        fly_data = fly_data.sort_values('time')\n",
    "        \n",
    "        # Create a new dataframe to track trial completions\n",
    "        # A trial is completed when we transition from trial N to trial N+1\n",
    "        trial_completions = []\n",
    "        \n",
    "        # Get unique trials for this fly in order\n",
    "        unique_trials = sorted(fly_data['Trial'].unique())\n",
    "        \n",
    "        # Track completion time for each trial\n",
    "        for i in range(len(unique_trials)-1):\n",
    "            current_trial = unique_trials[i]\n",
    "            next_trial = unique_trials[i+1]\n",
    "            \n",
    "            # Find the last frame of current trial\n",
    "            last_frame_current = fly_data[fly_data['Trial'] == current_trial]['time'].max()\n",
    "            \n",
    "            # This is when the trial was completed\n",
    "            trial_completions.append({\n",
    "                'fly': fly,\n",
    "                'time': last_frame_current,\n",
    "                'completed_trial': current_trial\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if trial_completions:\n",
    "            completions_df = pd.DataFrame(trial_completions)\n",
    "            \n",
    "            # Sort by time and calculate cumulative trials\n",
    "            completions_df = completions_df.sort_values('time')\n",
    "            completions_df['cumulative_trials'] = range(1, len(completions_df) + 1)\n",
    "            \n",
    "            # Add starting point (0 trials at time 0)\n",
    "            start_row = pd.DataFrame({\n",
    "                'fly': [fly],\n",
    "                'time': [fly_data['time'].min()],\n",
    "                'completed_trial': [0],\n",
    "                'cumulative_trials': [0]\n",
    "            })\n",
    "            \n",
    "            # Combine and add to results\n",
    "            fly_cumulative = pd.concat([start_row, completions_df]).reset_index(drop=True)\n",
    "            all_cumulative_data.append(fly_cumulative)\n",
    "    \n",
    "    # Combine all flies\n",
    "    if all_cumulative_data:\n",
    "        return pd.concat(all_cumulative_data).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate cumulative trials data\n",
    "cumulative_df = generate_cumulative_trials_from_original(original_data)\n",
    "\n",
    "# Plot cumulative trials over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot individual fly trajectories\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    plt.plot(group['time'], group['cumulative_trials'], 'o-', \n",
    "             linewidth=2, markersize=5, alpha=0.7,\n",
    "             label=f\"Fly {fly}\")\n",
    "\n",
    "plt.title('Cumulative Trials Solved Over Time', fontsize=14)\n",
    "plt.xlabel('Time (seconds)', fontsize=12)\n",
    "plt.ylabel('Cumulative Trials Completed', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the original dataset that has Trial annotations\n",
    "original_data = Filtered.copy()\n",
    "\n",
    "# Function to generate cumulative trials for all flies pooled together\n",
    "def generate_pooled_cumulative_trials(data):\n",
    "    # Create a list to store all flies' cumulative data with normalized time\n",
    "    all_fly_data = []\n",
    "    \n",
    "    for fly, fly_data in data.groupby('fly'):\n",
    "        # Sort by time\n",
    "        fly_data = fly_data.sort_values('time')\n",
    "        \n",
    "        # Find transitions between trials (when Trial value changes)\n",
    "        fly_data['trial_change'] = fly_data['Trial'].diff().fillna(0) != 0\n",
    "        transitions = fly_data[fly_data['trial_change']].copy()\n",
    "        \n",
    "        if len(transitions) > 0:\n",
    "            # Calculate cumulative trials\n",
    "            transitions['completed_trial'] = transitions['Trial'] - 1\n",
    "            transitions = transitions[transitions['completed_trial'] > 0].copy()\n",
    "            \n",
    "            # Add starting point (0 trials at beginning)\n",
    "            start_time = fly_data['time'].min()\n",
    "            start_row = pd.DataFrame({\n",
    "                'fly': [fly],\n",
    "                'time': [start_time],\n",
    "                'completed_trial': [0]\n",
    "            })\n",
    "            \n",
    "            # Combine starting point with transitions\n",
    "            fly_cumulative = pd.concat([\n",
    "                start_row, \n",
    "                transitions[['fly', 'time', 'completed_trial']]\n",
    "            ]).reset_index(drop=True)\n",
    "            \n",
    "            # Normalize time to start at 0\n",
    "            fly_cumulative['time_normalized'] = fly_cumulative['time'] - start_time\n",
    "            \n",
    "            # Add to collection\n",
    "            all_fly_data.append(fly_cumulative)\n",
    "    \n",
    "    # Combine all flies data\n",
    "    if all_fly_data:\n",
    "        return pd.concat(all_fly_data).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate cumulative trials data\n",
    "cumulative_data = generate_pooled_cumulative_trials(original_data)\n",
    "\n",
    "# Create time bins for pooling (e.g., every 5 seconds)\n",
    "bin_size = 5  # seconds\n",
    "max_time = cumulative_data['time_normalized'].max()\n",
    "time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "\n",
    "# Function to find the cumulative trials at each time bin for each fly\n",
    "def get_trials_at_timepoints(fly_data, timepoints):\n",
    "    results = []\n",
    "    for t in timepoints:\n",
    "        # Find the maximum trials completed before or at this time\n",
    "        trials_at_time = fly_data[fly_data['time_normalized'] <= t]['completed_trial'].max()\n",
    "        results.append({\n",
    "            'fly': fly_data['fly'].iloc[0],\n",
    "            'time_bin': t,\n",
    "            'cumulative_trials': trials_at_time if not np.isnan(trials_at_time) else 0\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply to each fly and combine\n",
    "binned_data = []\n",
    "for fly, group in cumulative_data.groupby('fly'):\n",
    "    fly_bins = get_trials_at_timepoints(group, time_bins)\n",
    "    binned_data.append(fly_bins)\n",
    "\n",
    "binned_df = pd.concat(binned_data).reset_index(drop=True)\n",
    "\n",
    "# Calculate mean and confidence intervals at each time bin\n",
    "from scipy import stats\n",
    "\n",
    "summary_stats = binned_df.groupby('time_bin').agg(\n",
    "    mean_trials=('cumulative_trials', 'mean'),\n",
    "    std_trials=('cumulative_trials', 'std'),\n",
    "    n_flies=('fly', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "summary_stats['ci_95'] = summary_stats.apply(\n",
    "    lambda row: 1.96 * row['std_trials'] / np.sqrt(row['n_flies']) \n",
    "    if row['n_flies'] > 0 else 0, axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a linear regression model for the mean trials\n",
    "\n",
    "# Define the linear regression model\n",
    "\n",
    "def linear_model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "# Fit the linear model to the data\n",
    "\n",
    "popt, _ = curve_fit(linear_model, summary_stats['time_bin'], summary_stats['mean_trials'])\n",
    "\n",
    "# Calculate the R-squared value\n",
    "\n",
    "r2 = r2_score(summary_stats['mean_trials'], linear_model(summary_stats['time_bin'], *popt))\n",
    "\n",
    "print(f\"Linear Model: y = {popt[0]:.2f} * x + {popt[1]:.2f}, R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a logistic regression model for the mean trials\n",
    "\n",
    "# Define the logistic regression model\n",
    "\n",
    "def logistic_model(x, a, b, c):\n",
    "    return c / (1 + np.exp(-(x - b) / a))\n",
    "\n",
    "# Fit the logistic model to the data\n",
    "\n",
    "popt_log, _ = curve_fit(logistic_model, summary_stats['time_bin'], summary_stats['mean_trials'], maxfev=10000)\n",
    "\n",
    "# Calculate the R-squared value\n",
    "\n",
    "r2_log = r2_score(summary_stats['mean_trials'], logistic_model(summary_stats['time_bin'], *popt_log))\n",
    "\n",
    "print(f\"Logistic Model: y = {popt_log[2]:.2f} / (1 + exp(-(x - {popt_log[1]:.2f}) / {popt_log[0]:.2f}), R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a piecewise linear regression model for the mean trials\n",
    "\n",
    "# Define the piecewise linear regression model\n",
    "\n",
    "def piecewise_linear(x, a, b, c):\n",
    "    # Convert to numpy array to ensure proper handling\n",
    "    x = np.asarray(x)\n",
    "    \n",
    "    # Create output array of the same shape\n",
    "    result = np.zeros_like(x)\n",
    "    \n",
    "    # Apply the piecewise function manually\n",
    "    mask = x < c\n",
    "    result[mask] = a * x[mask] + b      # First piece: ax + b when x < c\n",
    "    result[~mask] = a * c + b           # Second piece: constant when x >= c\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Fit the piecewise linear model to the data\n",
    "\n",
    "popt_pw, _ = curve_fit(piecewise_linear, summary_stats['time_bin'], summary_stats['mean_trials'])\n",
    "\n",
    "# Calculate the R-squared value\n",
    "\n",
    "r2_pw = r2_score(summary_stats['mean_trials'], piecewise_linear(summary_stats['time_bin'], *popt_pw))\n",
    "\n",
    "print(f\"Piecewise Linear Model: y = {popt_pw[0]:.2f} * x + {popt_pw[1]:.2f} (x < {popt_pw[2]:.2f}), R-squared: {r2_pw:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate cumulative trials for all flies pooled together\n",
    "def generate_pooled_cumulative_trials(data):\n",
    "    # Create a list to store all flies' cumulative data with normalized time\n",
    "    all_fly_data = []\n",
    "\n",
    "    for fly, fly_data in data.groupby('fly'):\n",
    "        # Sort by time\n",
    "        fly_data = fly_data.sort_values('time')\n",
    "\n",
    "        # Find transitions between trials (when Trial value changes)\n",
    "        fly_data['trial_change'] = fly_data['trial'].diff().fillna(0) != 0\n",
    "        transitions = fly_data[fly_data['trial_change']].copy()\n",
    "\n",
    "        if len(transitions) > 0:\n",
    "            # Calculate cumulative trials\n",
    "            transitions['completed_trial'] = transitions['trial'] - 1\n",
    "            transitions = transitions[transitions['completed_trial'] > 0].copy()\n",
    "\n",
    "            # Add starting point (0 trials at beginning)\n",
    "            start_time = fly_data['time'].min()\n",
    "            start_row = pd.DataFrame({\n",
    "                'fly': [fly],\n",
    "                'time': [start_time],\n",
    "                'completed_trial': [0]\n",
    "            })\n",
    "\n",
    "            # Combine starting point with transitions\n",
    "            fly_cumulative = pd.concat([\n",
    "                start_row, \n",
    "                transitions[['fly', 'time', 'completed_trial']]\n",
    "            ]).reset_index(drop=True)\n",
    "\n",
    "            # Normalize time to start at 0\n",
    "            fly_cumulative['time_normalized'] = fly_cumulative['time'] - start_time\n",
    "\n",
    "            # Add to collection\n",
    "            all_fly_data.append(fly_cumulative)\n",
    "\n",
    "    # Combine all flies data\n",
    "    if all_fly_data:\n",
    "        return pd.concat(all_fly_data).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate cumulative trials data\n",
    "cumulative_data = generate_pooled_cumulative_trials(Data_Filtered)\n",
    "\n",
    "# Create time bins for pooling (e.g., every 5 seconds)\n",
    "bin_size = 5  # seconds\n",
    "max_time = cumulative_data['time_normalized'].max()\n",
    "time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "\n",
    "# Function to find the cumulative trials at each time bin for each fly\n",
    "def get_trials_at_timepoints(fly_data, timepoints):\n",
    "    results = []\n",
    "    for t in timepoints:\n",
    "        # Find the maximum trials completed before or at this time\n",
    "        trials_at_time = fly_data[fly_data['time_normalized'] <= t]['completed_trial'].max()\n",
    "        results.append({\n",
    "            'fly': fly_data['fly'].iloc[0],\n",
    "            'time_bin': t,\n",
    "            'cumulative_trials': trials_at_time if not np.isnan(trials_at_time) else 0\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply to each fly and combine\n",
    "binned_data = []\n",
    "for fly, group in cumulative_data.groupby('fly'):\n",
    "    fly_bins = get_trials_at_timepoints(group, time_bins)\n",
    "    binned_data.append(fly_bins)\n",
    "\n",
    "binned_df = pd.concat(binned_data).reset_index(drop=True)\n",
    "\n",
    "# Calculate mean and confidence intervals at each time bin\n",
    "from scipy import stats\n",
    "\n",
    "summary_stats = binned_df.groupby('time_bin').agg(\n",
    "    mean_trials=('cumulative_trials', 'mean'),\n",
    "    std_trials=('cumulative_trials', 'std'),\n",
    "    n_flies=('fly', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "summary_stats['ci_95'] = summary_stats.apply(\n",
    "    lambda row: 1.96 * row['std_trials'] / np.sqrt(row['n_flies']) \n",
    "    if row['n_flies'] > 0 else 0, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pooled cumulative trials over time with confidence intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot mean line\n",
    "plt.plot(summary_stats['time_bin'], summary_stats['mean_trials'], 'b-', \n",
    "         linewidth=2, label='Mean Cumulative Trials', color=\"blue\")\n",
    "\n",
    "# Add confidence interval\n",
    "plt.fill_between(\n",
    "    summary_stats[\"time_bin\"],\n",
    "    summary_stats[\"mean_trials\"] - summary_stats[\"ci_95\"],\n",
    "    summary_stats[\"mean_trials\"] + summary_stats[\"ci_95\"],\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "\n",
    "# Overlay individual fly trajectories with lower opacity\n",
    "for fly, group in binned_df.groupby('fly'):\n",
    "    plt.plot(group['time_bin'], group['cumulative_trials'], '-', \n",
    "             linewidth=1, alpha=0.2, color='gray')\n",
    "\n",
    "# Add the regression line to the plot\n",
    "\n",
    "plt.plot(summary_stats['time_bin'], linear_model(summary_stats['time_bin'], *popt), 'r--',\n",
    "         linewidth=2, label=f'Linear Fit (R^2={r2:.2f})')\n",
    "\n",
    "# Add the logistic regression line to the plot\n",
    "\n",
    "plt.plot(summary_stats['time_bin'], logistic_model(summary_stats['time_bin'], *popt_log), 'g--',\n",
    "         linewidth=2, label=f'Logistic Fit (R^2={r2_log:.2f})')\n",
    "\n",
    "plt.title('Pooled Cumulative Trials Solved Over Time', fontsize=14)\n",
    "plt.xlabel('Time Since Start (seconds)', fontsize=12)\n",
    "plt.ylabel('Average Cumulative Trials Completed', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 trials control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate cumulative trials for all flies pooled together\n",
    "def generate_pooled_cumulative_trials(data):\n",
    "    # Create a list to store all flies' cumulative data with normalized time\n",
    "    all_fly_data = []\n",
    "    \n",
    "    for fly, fly_data in data.groupby('fly'):\n",
    "        # Sort by time\n",
    "        fly_data = fly_data.sort_values('time')\n",
    "        \n",
    "        # Find transitions between trials (when Trial value changes)\n",
    "        fly_data['trial_change'] = fly_data['trial'].diff().fillna(0) != 0\n",
    "        transitions = fly_data[fly_data['trial_change']].copy()\n",
    "        \n",
    "        if len(transitions) > 0:\n",
    "            # Calculate cumulative trials\n",
    "            transitions['completed_trial'] = transitions['trial'] - 1\n",
    "            transitions = transitions[transitions['completed_trial'] > 0].copy()\n",
    "            \n",
    "            # Add starting point (0 trials at beginning)\n",
    "            start_time = fly_data['time'].min()\n",
    "            start_row = pd.DataFrame({\n",
    "                'fly': [fly],\n",
    "                'time': [start_time],\n",
    "                'completed_trial': [0]\n",
    "            })\n",
    "            \n",
    "            # Combine starting point with transitions\n",
    "            fly_cumulative = pd.concat([\n",
    "                start_row, \n",
    "                transitions[['fly', 'time', 'completed_trial']]\n",
    "            ]).reset_index(drop=True)\n",
    "            \n",
    "            # Normalize time to start at 0\n",
    "            fly_cumulative['time_normalized'] = fly_cumulative['time'] - start_time\n",
    "            \n",
    "            # Add to collection\n",
    "            all_fly_data.append(fly_cumulative)\n",
    "    \n",
    "    # Combine all flies data\n",
    "    if all_fly_data:\n",
    "        return pd.concat(all_fly_data).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate cumulative trials data\n",
    "cumulative_data = generate_pooled_cumulative_trials(Data_4trials_Filtered)\n",
    "\n",
    "# Create time bins for pooling (e.g., every 5 seconds)\n",
    "bin_size = 5  # seconds\n",
    "max_time = cumulative_data['time_normalized'].max()\n",
    "time_bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "\n",
    "# Function to find the cumulative trials at each time bin for each fly\n",
    "def get_trials_at_timepoints(fly_data, timepoints):\n",
    "    results = []\n",
    "    for t in timepoints:\n",
    "        # Find the maximum trials completed before or at this time\n",
    "        trials_at_time = fly_data[fly_data['time_normalized'] <= t]['completed_trial'].max()\n",
    "        results.append({\n",
    "            'fly': fly_data['fly'].iloc[0],\n",
    "            'time_bin': t,\n",
    "            'cumulative_trials': trials_at_time if not np.isnan(trials_at_time) else 0\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply to each fly and combine\n",
    "binned_data = []\n",
    "for fly, group in cumulative_data.groupby('fly'):\n",
    "    fly_bins = get_trials_at_timepoints(group, time_bins)\n",
    "    binned_data.append(fly_bins)\n",
    "\n",
    "binned_df = pd.concat(binned_data).reset_index(drop=True)\n",
    "\n",
    "# Calculate mean and confidence intervals at each time bin\n",
    "from scipy import stats\n",
    "\n",
    "summary_stats = binned_df.groupby('time_bin').agg(\n",
    "    mean_trials=('cumulative_trials', 'mean'),\n",
    "    std_trials=('cumulative_trials', 'std'),\n",
    "    n_flies=('fly', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "summary_stats['ci_95'] = summary_stats.apply(\n",
    "    lambda row: 1.96 * row['std_trials'] / np.sqrt(row['n_flies']) \n",
    "    if row['n_flies'] > 0 else 0, axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pooled cumulative trials over time with confidence intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot mean line\n",
    "plt.plot(summary_stats['time_bin'], summary_stats['mean_trials'], 'b-', \n",
    "         linewidth=2, label='Mean Cumulative Trials', color=\"orange\")\n",
    "\n",
    "# Add confidence interval\n",
    "plt.fill_between(\n",
    "    summary_stats['time_bin'],\n",
    "    summary_stats['mean_trials'] - summary_stats['ci_95'],\n",
    "    summary_stats['mean_trials'] + summary_stats['ci_95'],\n",
    "    color='orange', alpha=0.2, label='95% Confidence Interval'\n",
    ")\n",
    "\n",
    "# Overlay individual fly trajectories with lower opacity\n",
    "for fly, group in binned_df.groupby('fly'):\n",
    "    plt.plot(group['time_bin'], group['cumulative_trials'], '-', \n",
    "             linewidth=1, alpha=0.2, color='gray')\n",
    "\n",
    "# Add the regression line to the plot\n",
    "\n",
    "plt.plot(summary_stats['time_bin'], linear_model(summary_stats['time_bin'], *popt), 'r--',\n",
    "         linewidth=2, label=f'Linear Fit (R^2={r2:.2f})')\n",
    "\n",
    "# Add the logistic regression line to the plot\n",
    "\n",
    "plt.plot(summary_stats['time_bin'], logistic_model(summary_stats['time_bin'], *popt_log), 'g--',\n",
    "         linewidth=2, label=f'Logistic Fit (R^2={r2_log:.2f})')\n",
    "\n",
    "plt.title('Pooled Cumulative Trials Solved Over Time', fontsize=14)\n",
    "plt.xlabel('Time Since Start (seconds)', fontsize=12)\n",
    "plt.ylabel('Average Cumulative Trials Completed', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data when only considering the first trials seems to be much more meaningful actually, as the later trials are kind of making things blurry. Let's Try both conditions with only the subset trials just to be sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating learners and non learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define curve functions\n",
    "def linear_curve(x, a, b):\n",
    "    return a * x + b  # Constant learning rate\n",
    "\n",
    "def sigmoid_curve(x, L_max, k, t0):\n",
    "    # Logistic sigmoid function with 3 parameters\n",
    "    return L_max / (1 + np.exp(-k * (x - t0)))\n",
    "\n",
    "# Process each fly\n",
    "fit_results = []\n",
    "\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    # Extract data points\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Ensure data is float64 to avoid numerical issues\n",
    "    x = x.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    \n",
    "    # Fit linear curve\n",
    "    linear_params, _ = curve_fit(linear_curve, x, y)\n",
    "    linear_fit = linear_curve(x, *linear_params)\n",
    "    linear_r2 = r2_score(y, linear_fit)\n",
    "    \n",
    "    # Initial guess for sigmoid params (important for convergence)\n",
    "    p0 = [max(y) * 1.1, 1, np.median(x)]\n",
    "    \n",
    "    # Multiple attempts with different initial parameters if needed\n",
    "    try:\n",
    "        # First attempt with default parameters\n",
    "        sigmoid_params, _ = curve_fit(sigmoid_curve, x, y, p0=p0, maxfev=10000)\n",
    "        sigmoid_fit = sigmoid_curve(x, *sigmoid_params)\n",
    "        sigmoid_r2 = r2_score(y, sigmoid_fit)\n",
    "        \n",
    "        # If R² is negative, try alternative parameter guesses\n",
    "        if sigmoid_r2 < 0:\n",
    "            # Alternative parameter guesses\n",
    "            alt_p0 = [max(y), 0.1, np.median(x)]\n",
    "            sigmoid_params, _ = curve_fit(sigmoid_curve, x, y, p0=alt_p0, maxfev=10000)\n",
    "            sigmoid_fit = sigmoid_curve(x, *sigmoid_params)\n",
    "            sigmoid_r2 = r2_score(y, sigmoid_fit)\n",
    "            \n",
    "    except:\n",
    "        # For flies where sigmoid fitting fails, use linear fit R² instead\n",
    "        print(f\"Warning: Sigmoid fitting failed for fly {fly}. Using linear model.\")\n",
    "        sigmoid_params = None\n",
    "        sigmoid_r2 = linear_r2  # Use linear R² instead of -Inf\n",
    "    \n",
    "    # Store results with proper handling for failed fits\n",
    "    fit_results.append({\n",
    "        'fly': fly,\n",
    "        'linear_r2': linear_r2,\n",
    "        'sigmoid_r2': sigmoid_r2 if sigmoid_params is not None else linear_r2,\n",
    "        'r2_diff': (sigmoid_r2 - linear_r2) if sigmoid_params is not None else 0,\n",
    "        'linear_params': linear_params,\n",
    "        'sigmoid_params': sigmoid_params,\n",
    "        'sigmoid_fit_failed': sigmoid_params is None\n",
    "    })\n",
    "    \n",
    "    print(f\"Fly {fly}: Linear R^2 = {linear_r2:.2f}, Sigmoid R^2 = {sigmoid_r2:.2f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "fit_results_df = pd.DataFrame(fit_results)\n",
    "\n",
    "# Classify flies - learners have better sigmoid fit\n",
    "r2_improvement_threshold = 0.05  # Minimum difference to classify as learner\n",
    "# Only consider flies where sigmoid fitting succeeded\n",
    "fit_results_df['is_learner'] = (fit_results_df['r2_diff'] > r2_improvement_threshold) & (~fit_results_df['sigmoid_fit_failed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many flies are classified as learners vs. non-learners\n",
    "\n",
    "learner_counts = fit_results_df['is_learner'].value_counts()\n",
    "print(learner_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot each fly with appropriate color coding\n",
    "for _, row in fit_results_df.iterrows():\n",
    "    fly = row['fly']\n",
    "    is_learner = row['is_learner']\n",
    "    \n",
    "    # Get fly data\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Set color and label\n",
    "    color = 'green' if is_learner else 'red'\n",
    "    label = f\"Fly{fly} ({'Learner' if is_learner else 'Non-Learner'})\"\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x, y, color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot the best-fit curve\n",
    "    x_smooth = np.linspace(min(x), max(x), 100)\n",
    "    \n",
    "    if is_learner and row['sigmoid_params'] is not None:\n",
    "        # Plot sigmoid curve for learners\n",
    "        y_smooth = sigmoid_curve(x_smooth, *row['sigmoid_params'])\n",
    "        plt.plot(x_smooth, y_smooth, '--', color=color, alpha=0.8, label=label)\n",
    "    else:\n",
    "        # Linear fit for non-learners\n",
    "        y_smooth = linear_curve(x_smooth, *row['linear_params'])\n",
    "        plt.plot(x_smooth, y_smooth, '--', color=color, alpha=0.8, label=label)\n",
    "\n",
    "plt.title('Learners vs Non-Learners Based on Curve Fitting', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Cumulative Trials Solved', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define curve functions\n",
    "def linear_curve(x, a, b):\n",
    "    return a * x + b  # Constant learning rate\n",
    "\n",
    "def logistic_curve(x, L_max, k, t0):\n",
    "    # Logistic sigmoid function with 3 parameters\n",
    "    return L_max / (1 + np.exp(-k * (x - t0)))\n",
    "\n",
    "# Process each fly\n",
    "fit_results = []\n",
    "\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    # Extract data points\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Ensure data is float64 to avoid numerical issues\n",
    "    x = x.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    \n",
    "    # Fit linear curve\n",
    "    linear_params, _ = curve_fit(linear_curve, x, y)\n",
    "    linear_fit = linear_curve(x, *linear_params)\n",
    "    linear_r2 = r2_score(y, linear_fit)\n",
    "    \n",
    "    # Initial guess for logistic params (important for convergence)\n",
    "    p0 = [max(y) * 1.2, 0.5, np.median(x)]\n",
    "    bounds = ([0, 0, min(x)], [max(y) * 2, 10, max(x)])\n",
    "    \n",
    "    # Fit logistic curve with robust error handling\n",
    "    logistic_successful = False\n",
    "    try:\n",
    "        # Try with default parameters\n",
    "        logistic_params, _ = curve_fit(logistic_curve, x, y, p0=p0, bounds=bounds, maxfev=10000)\n",
    "        logistic_fit = logistic_curve(x, *logistic_params)\n",
    "        logistic_r2 = r2_score(y, logistic_fit)\n",
    "        \n",
    "        # Check if the R² is valid\n",
    "        if logistic_r2 > 0:\n",
    "            logistic_successful = True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Initial logistic fit failed for fly {fly}: {e}\")\n",
    "    \n",
    "    # Try alternative parameters if first attempt failed\n",
    "    if not logistic_successful:\n",
    "        try:\n",
    "            # Try with alternative parameters\n",
    "            alt_p0 = [max(y), 0.1, np.mean(x)]\n",
    "            logistic_params, _ = curve_fit(logistic_curve, x, y, p0=alt_p0, bounds=bounds, maxfev=10000)\n",
    "            logistic_fit = logistic_curve(x, *logistic_params)\n",
    "            logistic_r2 = r2_score(y, logistic_fit)\n",
    "            logistic_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative logistic fit failed for fly {fly}: {e}\")\n",
    "            logistic_params = None\n",
    "            logistic_r2 = -1  # Use -1 instead of -Inf\n",
    "    \n",
    "    # Calculate improvement in fit\n",
    "    r2_diff = logistic_r2 - linear_r2 if logistic_successful else 0\n",
    "    \n",
    "    # Store results\n",
    "    fit_results.append({\n",
    "        'fly': fly,\n",
    "        'linear_r2': linear_r2,\n",
    "        'logistic_r2': logistic_r2 if logistic_successful else -1,\n",
    "        'r2_diff': r2_diff,\n",
    "        'linear_params': linear_params,\n",
    "        'logistic_params': logistic_params,\n",
    "        'logistic_successful': logistic_successful\n",
    "    })\n",
    "    \n",
    "    print(f\"Fly {fly}: Linear R² = {linear_r2:.4f}, Logistic R² = {logistic_r2:.4f}, Diff = {r2_diff:.4f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "fit_results_df = pd.DataFrame(fit_results)\n",
    "\n",
    "# Classify flies - learners have better logistic fit by a threshold\n",
    "r2_improvement_threshold = 0.03  # Minimum difference to classify as learner\n",
    "fit_results_df['is_learner'] = (fit_results_df['r2_diff'] > r2_improvement_threshold) & (fit_results_df['logistic_successful'])\n",
    "\n",
    "# Summarize classification results\n",
    "num_learners = fit_results_df['is_learner'].sum()\n",
    "num_non_learners = len(fit_results_df) - num_learners\n",
    "print(f\"\\nClassification results:\")\n",
    "print(f\"Learners: {num_learners} flies\")\n",
    "print(f\"Non-learners: {num_non_learners} flies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all fits\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create a color palette\n",
    "learner_color = 'green'\n",
    "non_learner_color = 'red'\n",
    "\n",
    "# First, plot non-learners (so learners appear on top)\n",
    "for _, row in fit_results_df[~fit_results_df['is_learner']].iterrows():\n",
    "    fly = row['fly']\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x, y, color=non_learner_color, alpha=0.5)\n",
    "    \n",
    "    # Plot linear fit (which is better for non-learners)\n",
    "    x_smooth = np.linspace(min(x), max(x), 100)\n",
    "    y_linear = linear_curve(x_smooth, *row['linear_params'])\n",
    "    plt.plot(x_smooth, y_linear, '-', color=non_learner_color, alpha=0.3)\n",
    "\n",
    "# Then plot learners\n",
    "for _, row in fit_results_df[fit_results_df['is_learner']].iterrows():\n",
    "    fly = row['fly']\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x, y, color=learner_color, alpha=0.5)\n",
    "    \n",
    "    # Plot logistic fit\n",
    "    if row['logistic_successful']:\n",
    "        x_smooth = np.linspace(min(x), max(x), 100)\n",
    "        y_logistic = logistic_curve(x_smooth, *row['logistic_params'])\n",
    "        plt.plot(x_smooth, y_logistic, '-', color=learner_color, alpha=0.3)\n",
    "\n",
    "# Add representative curves for legend\n",
    "x_demo = np.linspace(0, 1, 100)\n",
    "plt.plot([], [], '-', color=learner_color, label='Learners (Logistic Fit)')\n",
    "plt.plot([], [], '-', color=non_learner_color, label='Non-Learners (Linear Fit)')\n",
    "\n",
    "plt.title('Classification of Flies Based on Learning Curve Shape', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Cumulative Trials Solved', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show a few representative examples in detail\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Representative Examples of Learners vs Non-Learners', fontsize=16)\n",
    "\n",
    "# Get 2 best learners and 2 best non-learners\n",
    "top_learners = fit_results_df[fit_results_df['is_learner']].nlargest(2, 'r2_diff')\n",
    "top_non_learners = fit_results_df[~fit_results_df['is_learner']].nsmallest(2, 'r2_diff')\n",
    "\n",
    "# Plot each example\n",
    "for i, (idx, row) in enumerate(pd.concat([top_learners, top_non_learners]).iterrows()):\n",
    "    fly = row['fly']\n",
    "    is_learner = row['is_learner']\n",
    "    \n",
    "    # Calculate subplot position\n",
    "    r, c = i // 2, i % 2\n",
    "    ax = axs[r, c]\n",
    "    \n",
    "    # Get fly data\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(x, y, color='blue', s=50, label='Data Points')\n",
    "    \n",
    "    # Plot curves\n",
    "    x_smooth = np.linspace(min(x), max(x), 100)\n",
    "    \n",
    "    # Always plot linear fit\n",
    "    y_linear = linear_curve(x_smooth, *row['linear_params'])\n",
    "    ax.plot(x_smooth, y_linear, '--', color='orange', label=f'Linear (R²={row[\"linear_r2\"]:.4f})')\n",
    "    \n",
    "    # Plot logistic fit if available\n",
    "    if row['logistic_successful']:\n",
    "        y_logistic = logistic_curve(x_smooth, *row['logistic_params'])\n",
    "        ax.plot(x_smooth, y_logistic, '-', color='green', label=f'Logistic (R²={row[\"logistic_r2\"]:.4f})')\n",
    "    \n",
    "    # Add title and label\n",
    "    ax.set_title(f\"Fly {fly}: {'Learner' if is_learner else 'Non-Learner'}\", fontsize=14)\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('Cumulative Trials', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple curve functions\n",
    "def linear_curve(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "def logistic_curve(x, L_max, k, t0):\n",
    "    return L_max / (1 + np.exp(-k * (x - t0)))\n",
    "\n",
    "def piecewise_linear(x, x0, y0, k1, k2):\n",
    "    return np.where(x < x0, y0 + k1 * (x - x0), y0 + k2 * (x - x0))\n",
    "\n",
    "# Create empty list to store results\n",
    "fit_results = []\n",
    "\n",
    "# Process each fly\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    # Extract data points\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Fit all three models\n",
    "    models = {}\n",
    "    \n",
    "    # Linear fit\n",
    "    try:\n",
    "        linear_params, _ = curve_fit(linear_curve, x, y)\n",
    "        linear_fit = linear_curve(x, *linear_params)\n",
    "        linear_r2 = r2_score(y, linear_fit)\n",
    "        models['linear'] = {'params': linear_params, 'r2': linear_r2, 'fit': linear_fit}\n",
    "    except:\n",
    "        models['linear'] = {'r2': -1, 'params': None}\n",
    "    \n",
    "    # Logistic fit\n",
    "    try:\n",
    "        p0 = [max(y) * 1.2, 0.5, np.median(x)]\n",
    "        bounds = ([0, 0, min(x)], [max(y) * 2, 10, max(x)])\n",
    "        logistic_params, _ = curve_fit(logistic_curve, x, y, p0=p0, bounds=bounds, maxfev=10000)\n",
    "        logistic_fit = logistic_curve(x, *logistic_params)\n",
    "        logistic_r2 = r2_score(y, logistic_fit)\n",
    "        models['logistic'] = {'params': logistic_params, 'r2': logistic_r2, 'fit': logistic_fit}\n",
    "    except:\n",
    "        models['logistic'] = {'r2': -1, 'params': None}\n",
    "    \n",
    "    # Piecewise linear fit\n",
    "    try:\n",
    "        x0_guess = (min(x) + max(x)) / 2\n",
    "        y0_guess = np.median(y)\n",
    "        p0 = [x0_guess, y0_guess, 0.5, 1.5]\n",
    "        piecewise_params, _ = curve_fit(piecewise_linear, x, y, p0=p0, maxfev=10000)\n",
    "        piecewise_fit = piecewise_linear(x, *piecewise_params)\n",
    "        piecewise_r2 = r2_score(y, piecewise_fit)\n",
    "        models['piecewise'] = {'params': piecewise_params, 'r2': piecewise_r2, 'fit': piecewise_fit}\n",
    "    except:\n",
    "        models['piecewise'] = {'r2': -1, 'params': None}\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = max(models.items(), key=lambda x: x[1]['r2'])\n",
    "    best_model_name = best_model[0]\n",
    "    \n",
    "    # Calculate r2_improvement for non-linear models\n",
    "    logistic_improvement = models['logistic']['r2'] - models['linear']['r2'] \n",
    "    piecewise_improvement = models['piecewise']['r2'] - models['linear']['r2']\n",
    "    \n",
    "    # Get the maximum improvement\n",
    "    max_improvement = max(logistic_improvement, piecewise_improvement)\n",
    "    \n",
    "    # Store results including the improvement metrics\n",
    "    result_dict = {\n",
    "        'fly': fly,\n",
    "        'linear_r2': models['linear']['r2'],\n",
    "        'logistic_r2': models['logistic']['r2'],\n",
    "        'piecewise_r2': models['piecewise']['r2'],\n",
    "        'linear_params': models['linear']['params'],\n",
    "        'logistic_params': models['logistic']['params'],\n",
    "        'piecewise_params': models['piecewise']['params'],\n",
    "        'best_model': best_model_name,\n",
    "        'fly_type': 'Non-learner' if best_model_name == 'linear' else 'Learner',\n",
    "        'r2_improvement': max_improvement,\n",
    "        'logistic_improvement': logistic_improvement,\n",
    "        'piecewise_improvement': piecewise_improvement\n",
    "    }\n",
    "    \n",
    "    fit_results.append(result_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "fit_results_df = pd.DataFrame(fit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning criteria\n",
    "r2_improvement_threshold = 0.03  # Minimum R² improvement to consider non-linear models\n",
    "\n",
    "# Classify fly based on model comparison\n",
    "if (models['logistic']['r2'] > models['linear']['r2'] + r2_improvement_threshold or\n",
    "    models['piecewise']['r2'] > models['linear']['r2'] + r2_improvement_threshold):\n",
    "    fly_type = 'Learner'\n",
    "else:\n",
    "    fly_type = 'Non-learner'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'fly_type' column based on the 'is_learner' classification\n",
    "fit_results_df['fly_type'] = 'Non-learner'  # Default value\n",
    "fit_results_df.loc[fit_results_df['is_learner'], 'fly_type'] = 'Learner'\n",
    "\n",
    "# Further categorize the learners based on their best model\n",
    "# Only apply this to rows where is_learner is True and best_model exists\n",
    "if 'best_model' in fit_results_df.columns:\n",
    "    mask = (fit_results_df['is_learner']) & (fit_results_df['best_model'] == 'piecewise')\n",
    "    fit_results_df.loc[mask, 'fly_type'] = 'Piecewise Learner'\n",
    "    \n",
    "    mask = (fit_results_df['is_learner']) & (fit_results_df['best_model'] == 'logistic')\n",
    "    fit_results_df.loc[mask, 'fly_type'] = 'Logistic Learner'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a main summary plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define colors for different types\n",
    "learner_color = 'green'\n",
    "non_learner_color = 'red'\n",
    "piecewise_learner_color = 'blue'\n",
    "\n",
    "# Plot non-learners first (so learners appear on top)\n",
    "for _, row in fit_results_df[fit_results_df['fly_type'] == 'Non-learner'].iterrows():\n",
    "    fly = row['fly']\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x, y, color=non_learner_color, alpha=0.5, s=20)\n",
    "    \n",
    "    # Plot linear fit\n",
    "    x_smooth = np.linspace(min(x), max(x), 100)\n",
    "    y_linear = linear_curve(x_smooth, *row['linear_params'])\n",
    "    plt.plot(x_smooth, y_linear, '-', color=non_learner_color, alpha=0.3)\n",
    "\n",
    "# Plot learners with differentiation by best model\n",
    "for _, row in fit_results_df[fit_results_df['fly_type'] == 'Learner'].iterrows():\n",
    "    fly = row['fly']\n",
    "    group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "    x = group['time'].values\n",
    "    y = group['cumulative_trials'].values\n",
    "    \n",
    "    # Set color based on best model\n",
    "    model_color = piecewise_learner_color if row['best_model'] == 'piecewise' else learner_color\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x, y, color=model_color, alpha=0.5, s=20)\n",
    "    \n",
    "    # Plot the best fit\n",
    "    x_smooth = np.linspace(min(x), max(x), 100)\n",
    "    if row['best_model'] == 'logistic':\n",
    "        y_fit = logistic_curve(x_smooth, *row['logistic_params'])\n",
    "        plt.plot(x_smooth, y_fit, '-', color=learner_color, alpha=0.3)\n",
    "    elif row['best_model'] == 'piecewise':\n",
    "        y_fit = piecewise_linear(x_smooth, *row['piecewise_params'])\n",
    "        plt.plot(x_smooth, y_fit, '-', color=piecewise_learner_color, alpha=0.3)\n",
    "\n",
    "# Add legend elements using empty plots\n",
    "plt.plot([], [], 'o-', color=non_learner_color, label='Non-Learner (Linear)')\n",
    "plt.plot([], [], 'o-', color=learner_color, label='Logistic Learner')\n",
    "plt.plot([], [], 'o-', color=piecewise_learner_color, label='Piecewise Learner')\n",
    "\n",
    "plt.title('Cumulative Trials Solved: Learners vs. Non-Learners', fontsize=16)\n",
    "plt.xlabel('Time (seconds)', fontsize=14)\n",
    "plt.ylabel('Cumulative Trials Completed', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now create plots for representative examples\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16, 15))\n",
    "fig.suptitle('Representative Examples of Different Learning Patterns', fontsize=18)\n",
    "\n",
    "# Get best examples of each type\n",
    "logistic_learners = fit_results_df[(fit_results_df['fly_type'] == 'Learner') & (fit_results_df['best_model'] == 'logistic')]\n",
    "piecewise_learners = fit_results_df[(fit_results_df['fly_type'] == 'Learner') & (fit_results_df['best_model'] == 'piecewise')]\n",
    "non_learners = fit_results_df[fit_results_df['fly_type'] == 'Non-learner']\n",
    "\n",
    "# Select top 2 from each category (or fewer if not enough examples)\n",
    "top_logistic = logistic_learners.nlargest(2, 'r2_improvement')\n",
    "top_piecewise = piecewise_learners.nlargest(2, 'r2_improvement')\n",
    "top_non_learners = non_learners.nlargest(2, 'linear_r2')\n",
    "\n",
    "# Combine examples\n",
    "example_sets = [\n",
    "    (\"Logistic Learners\", top_logistic, 0),\n",
    "    (\"Piecewise Learners\", top_piecewise, 1),\n",
    "    (\"Non-Learners\", top_non_learners, 2)\n",
    "]\n",
    "\n",
    "# Plot each example set\n",
    "for title, examples, row_idx in example_sets:\n",
    "    for col_idx, (_, row) in enumerate(examples.iterrows()):\n",
    "        if col_idx > 1:  # Only showing 2 examples per category\n",
    "            break\n",
    "            \n",
    "        ax = axs[row_idx, col_idx]\n",
    "        fly = row['fly']\n",
    "        \n",
    "        # Get fly data\n",
    "        group = cumulative_df[cumulative_df['fly'] == fly]\n",
    "        x = group['time'].values\n",
    "        y = group['cumulative_trials'].values\n",
    "        \n",
    "        # Plot data points\n",
    "        ax.scatter(x, y, color='black', s=40, label='Data Points')\n",
    "        \n",
    "        # Plot all three model fits for comparison\n",
    "        x_smooth = np.linspace(min(x), max(x), 100)\n",
    "        \n",
    "        # Linear fit\n",
    "        y_linear = linear_curve(x_smooth, *row['linear_params'])\n",
    "        ax.plot(x_smooth, y_linear, '--', color='red', label=f'Linear (R²={row[\"linear_r2\"]:.3f})')\n",
    "        \n",
    "        # Logistic fit if available\n",
    "        if 'logistic_params' in row and row['logistic_params'] is not None:\n",
    "            y_logistic = logistic_curve(x_smooth, *row['logistic_params'])\n",
    "            ax.plot(x_smooth, y_logistic, '-', color='green', label=f'Logistic (R²={row[\"logistic_r2\"]:.3f})')\n",
    "        \n",
    "        # Piecewise fit if available\n",
    "        if 'piecewise_params' in row and row['piecewise_params'] is not None:\n",
    "            y_piecewise = piecewise_linear(x_smooth, *row['piecewise_params'])\n",
    "            ax.plot(x_smooth, y_piecewise, '-', color='blue', label=f'Piecewise (R²={row[\"piecewise_r2\"]:.3f})')\n",
    "        \n",
    "        # Highlight the best fit with thicker line\n",
    "        best_model = row['best_model']\n",
    "        if best_model == 'linear':\n",
    "            ax.plot(x_smooth, y_linear, '-', color='red', linewidth=3, alpha=0.6)\n",
    "        elif best_model == 'logistic':\n",
    "            ax.plot(x_smooth, y_logistic, '-', color='green', linewidth=3, alpha=0.6)\n",
    "        elif best_model == 'piecewise':\n",
    "            ax.plot(x_smooth, y_piecewise, '-', color='blue', linewidth=3, alpha=0.6)\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title(f\"Fly {fly}: {best_model.capitalize()} Model\", fontsize=14)\n",
    "        ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "        ax.set_ylabel('Cumulative Trials', fontsize=12)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learner count\n",
    "\n",
    "learner_counts = fit_results_df['fly_type'].value_counts()\n",
    "\n",
    "learner_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rate of trial completion (trials per second)\n",
    "learning_rates = []\n",
    "\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    # Use only the last point (total trials / total time)\n",
    "    total_trials = group['cumulative_trials'].max()\n",
    "    total_time = group['time'].max() - group['time'].min()\n",
    "    rate = total_trials / total_time if total_time > 0 else 0\n",
    "    \n",
    "    learning_rates.append({\n",
    "        'fly': fly,\n",
    "        'learning_rate': rate,\n",
    "        'total_trials': total_trials\n",
    "    })\n",
    "\n",
    "rates_df = pd.DataFrame(learning_rates)\n",
    "\n",
    "# Visualize learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=rates_df, x='fly', y='learning_rate')\n",
    "plt.title('Learning Rate (Trials per Second) by Fly')\n",
    "plt.xlabel('Fly')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate slope in different time segments\n",
    "def calculate_segment_slopes(fly_data, n_segments=2):\n",
    "    # Ensure data is sorted by time\n",
    "    fly_data = fly_data.sort_values('time')\n",
    "    \n",
    "    # Normalize time to start at 0\n",
    "    fly_data['time_normalized'] = fly_data['time'] - fly_data['time'].min()\n",
    "    \n",
    "    # Get the maximum time\n",
    "    max_time = fly_data['time_normalized'].max()\n",
    "    \n",
    "    # Define time segments\n",
    "    segment_boundaries = [i * max_time / n_segments for i in range(n_segments + 1)]\n",
    "    \n",
    "    slopes = []\n",
    "    for i in range(n_segments):\n",
    "        # Get data for this segment\n",
    "        start_time = segment_boundaries[i]\n",
    "        end_time = segment_boundaries[i+1]\n",
    "        \n",
    "        segment_data = fly_data[(fly_data['time_normalized'] >= start_time) & \n",
    "                               (fly_data['time_normalized'] <= end_time)]\n",
    "        \n",
    "        if len(segment_data) >= 2:  # Need at least 2 points for a slope\n",
    "            # Linear regression to calculate slope\n",
    "            X = segment_data['time_normalized'].values.reshape(-1, 1)\n",
    "            y = segment_data['cumulative_trials'].values\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            slope = model.coef_[0]\n",
    "            \n",
    "            slopes.append({\n",
    "                'fly': fly_data['fly'].iloc[0],\n",
    "                'segment': i+1,\n",
    "                'slope': slope\n",
    "            })\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "# Calculate slopes for each fly in different segments\n",
    "all_slopes = []\n",
    "for fly, group in cumulative_df.groupby('fly'):\n",
    "    slopes = calculate_segment_slopes(group, n_segments=2)  # Early vs Late\n",
    "    all_slopes.extend(slopes)\n",
    "\n",
    "slopes_df = pd.DataFrame(all_slopes)\n",
    "\n",
    "# Visualize early vs late slopes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=slopes_df, x='fly', y='slope', hue='segment')\n",
    "plt.title('Learning Rate Comparison: Early vs Late Trials')\n",
    "plt.xlabel('Fly')\n",
    "plt.ylabel('Learning Rate (Trials per Second)')\n",
    "plt.legend(title='Segment', labels=['Early', 'Late'])\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include the trials where at least 5 flies participated\n",
    "improvement_proportions_filtered = improvement_proportions[\n",
    "    trial_durations.groupby(\"Trial\")[\"fly\"].nunique() >= 5\n",
    "]\n",
    "\n",
    "# Plot the filtered results\n",
    "improvement_plot_filtered = (\n",
    "    hv.Curve(improvement_proportions_filtered)\n",
    "    .opts(\n",
    "        xlabel=\"Trial Number\",\n",
    "        ylabel=\"Proportion of Flies Improved\",\n",
    "        title=\"Proportion of Flies that Improved in Each Trial\",\n",
    "    )\n",
    "    .options(**HoloviewsTemplates.hv_slides[\"plot\"])\n",
    "    .opts(invert_axes=False, show_legend=True)\n",
    ")\n",
    "\n",
    "improvement_plot_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For flies for which \"Peak\" is NaN, give them the value \"morning\"\n",
    "Filtered[\"Peak\"] = Filtered[\"Peak\"].fillna(\"morning\")\n",
    "cleaned_data[\"Peak\"] = cleaned_data[\"Peak\"].fillna(\"morning\")\n",
    "\n",
    "# Ensure 'Trial' is treated as a categorical variable with ordered categories\n",
    "cleaned_data[\"Trial\"] = pd.Categorical(cleaned_data[\"Trial\"], ordered=True)\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the trial durations\n",
    "trial_durations_peak = (\n",
    "    cleaned_data.groupby([\"Peak\",\"fly\", \"Trial\"]).apply(compute_trial_duration).reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "Jitterbox_peak = HoloviewsTemplates.jitter_boxplot(\n",
    "    trial_durations_peak,\n",
    "    kdims=\"Trial\",\n",
    "    metric=\"duration\",\n",
    "    plot_options=HoloviewsTemplates.hv_slides,\n",
    "    groupby=\"Peak\",\n",
    "    render=\"grouped\"\n",
    ").opts(invert_axes=False, xlabel=\"Trial Number\", ylabel=\"Duration (s)\")\n",
    "\n",
    "Jitterbox_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as html\n",
    "\n",
    "hv.save(Jitterbox_peak, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240809_TrialDuration_Peak.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the improvement but grouped by Peak\n",
    "\n",
    "# Assuming trial_durations is your DataFrame\n",
    "# Ensure 'Trial' is treated as a categorical variable with ordered categories\n",
    "trial_durations_peak[\"Trial\"] = pd.Categorical(\n",
    "    trial_durations_peak[\"Trial\"], ordered=True\n",
    ")\n",
    "\n",
    "\n",
    "# Function to calculate improvement\n",
    "def calculate_improvement(df):\n",
    "    df = df.sort_values(by=\"Trial\")\n",
    "    df[\"improved\"] = df[\"duration\"].diff().lt(0)\n",
    "    return df\n",
    "\n",
    "# Apply the function to each group of 'fly' and 'Peak'\n",
    "trial_durations_peak = trial_durations_peak.groupby([\"fly\", \"Peak\"]).apply(\n",
    "    calculate_improvement\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the proportion of flies that improved in each trial for each Peak\n",
    "improvement_proportions = (\n",
    "    trial_durations_peak.groupby([\"Peak\", \"Trial\"])[\"improved\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(improvement_proportions)\n",
    "\n",
    "# Plot the results\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "# Create a separate plot for each Peak\n",
    "plots = []\n",
    "for peak in improvement_proportions[\"Peak\"].unique():\n",
    "    peak_data = improvement_proportions[improvement_proportions[\"Peak\"] == peak]\n",
    "    plot = hv.Curve(peak_data, kdims=\"Trial\", vdims=\"improved\").opts(\n",
    "        xlabel=\"Trial Number\",\n",
    "        ylabel=\"Proportion of Flies Improved\",\n",
    "        title=f\"Proportion of Flies that Improved in Each Trial for Peak {peak}\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    )\n",
    "    plots.append(plot)\n",
    "\n",
    "# Combine all plots into a single layout\n",
    "improvement_plot = hv.Layout(plots).cols(1)\n",
    "\n",
    "improvement_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Trial back to integers\n",
    "cleaned_data[\"Trial\"] = cleaned_data[\"Trial\"].astype(int)\n",
    "\n",
    "# Function to compute the end time of each trial\n",
    "def compute_trial_end_time(group):\n",
    "    end_time = group[\"time\"].max()\n",
    "    return pd.Series({\"end_time\": end_time})\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the end times\n",
    "trial_end_times = (\n",
    "    cleaned_data.groupby([\"fly\", \"Trial\"]).apply(compute_trial_end_time).reset_index()\n",
    ")\n",
    "\n",
    "# Sort by end_time\n",
    "trial_end_times = trial_end_times.sort_values(by=\"end_time\")\n",
    "\n",
    "# Compute the cumulative count of solved trials\n",
    "trial_end_times[\"cumulative_solved_trials\"] = (\n",
    "    trial_end_times[\"end_time\"].rank(method=\"first\").astype(int)\n",
    ")\n",
    "\n",
    "# Plot the cumulative count of solved trials over time\n",
    "cumulcurve = trial_end_times.hvplot.step(\n",
    "    x=\"end_time\",\n",
    "    y=\"cumulative_solved_trials\",\n",
    "    title=\"Cumulative Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Cumulative Solved Trials\",\n",
    ").opts(\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        alpha=1,\n",
    "        line_width=2,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Cumulative Solved Trials\",\n",
    "        show_grid=True,\n",
    "        fontscale=3,\n",
    "        title=\"Cumulative Solved Trials Over Time\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a PNG\n",
    "\n",
    "hv.save(cumulcurve, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240809_CumulativeSolvedTrials.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "from holoviews import opts  # Import opts from holoviews\n",
    "\n",
    "\n",
    "# Define logistic function\n",
    "def logistic(x, L, k, x0):\n",
    "    return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "# Filter data to include only relevant subset\n",
    "GroupData_morning = trial_end_times\n",
    "\n",
    "# Calculate the number of unique 'flies' in the dataset\n",
    "num_replicates = GroupData_morning[\"fly\"].nunique()\n",
    "\n",
    "# Initial guesses for L, k, x0\n",
    "p0 = [\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]),\n",
    "    1,\n",
    "    np.median(GroupData_morning[\"end_time\"]),\n",
    "]\n",
    "\n",
    "# Fit logistic function to data\n",
    "params, _ = curve_fit(\n",
    "    logistic,\n",
    "    GroupData_morning[\"end_time\"],\n",
    "    GroupData_morning[\"cumulative_solved_trials\"],\n",
    "    p0,\n",
    "    maxfev=10000,\n",
    ")\n",
    "\n",
    "# params contains the fitted values for L, k, x0\n",
    "L, k, x0 = params\n",
    "\n",
    "# Create a new DataFrame for the logistic fit curve\n",
    "logistic_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the logistic fit curve\n",
    "logistic_fit[\"cumulative_solved_trials\"] = logistic(logistic_fit[\"end_time\"], L, k, x0)\n",
    "\n",
    "# Calculate R-squared for logistic fit curve\n",
    "y_pred_logistic = logistic(GroupData_morning[\"end_time\"], L, k, x0)\n",
    "r_squared_logistic = r2_score(\n",
    "    GroupData_morning[\"cumulative_solved_trials\"], y_pred_logistic\n",
    ")\n",
    "\n",
    "# Calculate linear fit for the first half of the data\n",
    "first_half = GroupData_morning.iloc[: len(GroupData_morning) // 2]\n",
    "slope, intercept, _, _, _ = stats.linregress(\n",
    "    first_half[\"end_time\"], first_half[\"cumulative_solved_trials\"]\n",
    ")\n",
    "\n",
    "# Create a new DataFrame for the linear fit line\n",
    "linear_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the linear fit line\n",
    "linear_fit[\"cumulative_solved_trials\"] = slope * linear_fit[\"end_time\"] + intercept\n",
    "\n",
    "# Calculate R-squared for linear fit curve\n",
    "y_pred_linear = slope * GroupData_morning[\"end_time\"] + intercept\n",
    "r_squared_linear = r2_score(\n",
    "    GroupData_morning[\"cumulative_solved_trials\"], y_pred_linear\n",
    ")\n",
    "\n",
    "# Calculate center x value and min and maximum xy value\n",
    "center_x = (\n",
    "    GroupData_morning[\"end_time\"].max() - GroupData_morning[\"end_time\"].min()\n",
    ") / 2\n",
    "max_y = max(\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]),\n",
    "    max(logistic_fit[\"cumulative_solved_trials\"]),\n",
    ")\n",
    "\n",
    "max_x = GroupData_morning[\"end_time\"].max()\n",
    "min_y = GroupData_morning[\"cumulative_solved_trials\"].min()\n",
    "\n",
    "# Create your plot using GroupData_morning and add the fits and annotations\n",
    "cumulcurve_pool = (\n",
    "    hv.Curve(\n",
    "        data=GroupData_morning,\n",
    "        kdims=[\"end_time\"],\n",
    "        vdims=[\"cumulative_solved_trials\"],\n",
    "    )\n",
    "    .opts(\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        alpha=1,\n",
    "        line_width=2,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Cumulative Solved Trials\",\n",
    "        show_grid=True,\n",
    "        fontscale=3,\n",
    "        title=\"Cumulative Solved Trials Over Time\",\n",
    "    )\n",
    "    * hv.Curve(\n",
    "        data=logistic_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    "    ).opts(color=\"green\")\n",
    "    * hv.Curve(\n",
    "        data=linear_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    "    ).opts(color=\"red\")\n",
    "    # Uncomment the following lines to add text annotations\n",
    "    * hv.Text(center_x, max_y - 7, f\"Logistic fit R-squared: {r_squared_logistic:.2f}\").opts(text_color=\"green\")\n",
    "    * hv.Text(center_x, max_y, f\"Linear fit R-squared: {r_squared_linear:.2f}\").opts(text_color=\"red\")\n",
    "    * hv.Text(max_x - 100, min_y * num_replicates, f\"N = {num_replicates}\")\n",
    ").opts(\n",
    "    opts.Area(\n",
    "        show_legend=False,\n",
    "        show_frame=False,\n",
    "        fill_color=\"blue\",\n",
    "        line_color=\"black\",\n",
    "        line_width=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "cumulcurve_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "from holoviews import opts\n",
    "\n",
    "\n",
    "# Define double sigmoid function\n",
    "def double_sigmoid(x, L1, k1, x01, L2, k2, x02):\n",
    "    return (L1 / (1 + np.exp(-k1 * (x - x01)))) + (L2 / (1 + np.exp(-k2 * (x - x02))))\n",
    "\n",
    "\n",
    "# Improved initial guesses for L1, k1, x01, L2, k2, x02\n",
    "p0 = [\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]) / 2,\n",
    "    0.1,\n",
    "    np.median(GroupData_morning[\"end_time\"]) / 2,\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]) / 2,\n",
    "    0.1,\n",
    "    np.median(GroupData_morning[\"end_time\"]) * 1.5,\n",
    "]\n",
    "\n",
    "# Increase maxfev\n",
    "maxfev_value = 10000\n",
    "\n",
    "# Fit double sigmoid function to data\n",
    "params, _ = curve_fit(\n",
    "    double_sigmoid,\n",
    "    GroupData_morning[\"end_time\"],\n",
    "    GroupData_morning[\"cumulative_solved_trials\"],\n",
    "    p0,\n",
    "    maxfev=maxfev_value,\n",
    ")\n",
    "\n",
    "# params contains the fitted values for L1, k1, x01, L2, k2, x02\n",
    "L1, k1, x01, L2, k2, x02 = params\n",
    "\n",
    "# Calculate fitted values\n",
    "fitted_values = double_sigmoid(GroupData_morning[\"end_time\"], L1, k1, x01, L2, k2, x02)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = GroupData_morning[\"cumulative_solved_trials\"] - fitted_values\n",
    "\n",
    "# Calculate SS_res and SS_tot\n",
    "SS_res = np.sum(residuals**2)\n",
    "SS_tot = np.sum(\n",
    "    (\n",
    "        GroupData_morning[\"cumulative_solved_trials\"]\n",
    "        - np.mean(GroupData_morning[\"cumulative_solved_trials\"])\n",
    "    )\n",
    "    ** 2\n",
    ")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = 1 - (SS_res / SS_tot)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Create a new DataFrame for the double sigmoid fit curve\n",
    "double_sigmoid_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the double sigmoid fit curve\n",
    "double_sigmoid_fit[\"cumulative_solved_trials\"] = double_sigmoid(\n",
    "    double_sigmoid_fit[\"end_time\"], L1, k1, x01, L2, k2, x02\n",
    ")\n",
    "\n",
    "# Plot the original data and the double sigmoid fit curve\n",
    "cumulcurve_pool = hv.Curve(\n",
    "    data=GroupData_morning,\n",
    "    kdims=[\"end_time\"],\n",
    "    vdims=[\"cumulative_solved_trials\"],\n",
    ").opts(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    alpha=1,\n",
    "    line_width=2,\n",
    "    xlabel=\"Time(s)\",\n",
    "    ylabel=\"Cumulative Solved Trials\",\n",
    "    show_grid=True,\n",
    "    fontscale=3,\n",
    "    title=\"Cumulative Solved Trials Over Time\",\n",
    ") * hv.Curve(\n",
    "    data=double_sigmoid_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    ").opts(\n",
    "    color=\"green\"\n",
    ")\n",
    "\n",
    "#cumulcurve_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double sigmoid + Linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to include only relevant subset\n",
    "GroupData_morning = trial_end_times\n",
    "\n",
    "# Define double sigmoid function\n",
    "def double_sigmoid(x, L1, k1, x01, L2, k2, x02):\n",
    "    return (L1 / (1 + np.exp(-k1 * (x - x01)))) + (L2 / (1 + np.exp(-k2 * (x - x02))))\n",
    "\n",
    "\n",
    "# Improved initial guesses for L1, k1, x01, L2, k2, x02\n",
    "p0 = [\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]) / 2,\n",
    "    0.1,\n",
    "    np.median(GroupData_morning[\"end_time\"]) / 2,\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]) / 2,\n",
    "    0.1,\n",
    "    np.median(GroupData_morning[\"end_time\"]) * 1.5,\n",
    "]\n",
    "\n",
    "# Increase maxfev\n",
    "maxfev_value = 10000\n",
    "\n",
    "# Fit double sigmoid function to data\n",
    "params, _ = curve_fit(\n",
    "    double_sigmoid,\n",
    "    GroupData_morning[\"end_time\"],\n",
    "    GroupData_morning[\"cumulative_solved_trials\"],\n",
    "    p0,\n",
    "    maxfev=maxfev_value,\n",
    ")\n",
    "\n",
    "# params contains the fitted values for L1, k1, x01, L2, k2, x02\n",
    "L1, k1, x01, L2, k2, x02 = params\n",
    "\n",
    "# Calculate fitted values\n",
    "fitted_values = double_sigmoid(GroupData_morning[\"end_time\"], L1, k1, x01, L2, k2, x02)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = GroupData_morning[\"cumulative_solved_trials\"] - fitted_values\n",
    "\n",
    "# Calculate SS_res and SS_tot\n",
    "SS_res = np.sum(residuals**2)\n",
    "SS_tot = np.sum(\n",
    "    (\n",
    "        GroupData_morning[\"cumulative_solved_trials\"]\n",
    "        - np.mean(GroupData_morning[\"cumulative_solved_trials\"])\n",
    "    )\n",
    "    ** 2\n",
    ")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = 1 - (SS_res / SS_tot)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Create a new DataFrame for the double sigmoid fit curve\n",
    "double_sigmoid_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the double sigmoid fit curve\n",
    "double_sigmoid_fit[\"cumulative_solved_trials\"] = double_sigmoid(\n",
    "    double_sigmoid_fit[\"end_time\"], L1, k1, x01, L2, k2, x02\n",
    ")\n",
    "\n",
    "# Fit a linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(\n",
    "    GroupData_morning[[\"end_time\"]], GroupData_morning[\"cumulative_solved_trials\"]\n",
    ")\n",
    "\n",
    "# Calculate fitted values for the linear model\n",
    "linear_fitted_values = linear_model.predict(GroupData_morning[[\"end_time\"]])\n",
    "\n",
    "# Calculate R-squared for the linear model\n",
    "linear_r2 = r2_score(\n",
    "    GroupData_morning[\"cumulative_solved_trials\"], linear_fitted_values\n",
    ")\n",
    "print(f\"Linear R-squared: {linear_r2}\")\n",
    "\n",
    "# Create a new DataFrame for the linear fit curve\n",
    "linear_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": GroupData_morning[\"end_time\"],\n",
    "        \"cumulative_solved_trials\": linear_fitted_values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Plot the original data, the double sigmoid fit curve, and the linear fit curve\n",
    "cumulcurve_pool = (\n",
    "    hv.Curve(\n",
    "        data=GroupData_morning,\n",
    "        kdims=[\"end_time\"],\n",
    "        vdims=[\"cumulative_solved_trials\"],\n",
    "    ).opts(\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        alpha=1,\n",
    "        line_width=2,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Cumulative Solved Trials\",\n",
    "        show_grid=True,\n",
    "        fontscale=3,\n",
    "        title=\"Cumulative Solved Trials Over Time\",\n",
    "    )\n",
    "    * hv.Curve(\n",
    "        data=double_sigmoid_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    "    )\n",
    "    .opts(\n",
    "        color=\"green\",\n",
    "    )\n",
    "    .relabel(f\"Double Sigmoid Fit (R²={r_squared:.2f})\")\n",
    "    * hv.Curve(data=linear_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"])\n",
    "    .opts(\n",
    "        color=\"red\",\n",
    "        line_dash=\"dashed\",\n",
    "    )\n",
    "    .relabel(f\"Linear Fit (R²={linear_r2:.2f})\")\n",
    ")\n",
    "\n",
    "cumulcurve_pool = cumulcurve_pool.options(**HoloviewsTemplates.hv_slides[\"plot\"]).opts(\n",
    "    invert_axes=False, show_legend=True)\n",
    "\n",
    "hv.save(\n",
    "    cumulcurve_pool,\n",
    "    \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240809_CumulativeSolvedTrials_sigmoid_linear.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as PNG\n",
    "\n",
    "hv.save(cumulcurve_pool, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240809_CumulativeSolvedTrials.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "from holoviews import opts  # Import opts from holoviews\n",
    "\n",
    "\n",
    "# Define logistic (sigmoid) function\n",
    "def sigmoid(x, L, k, x0):\n",
    "    return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "# Filter data to include only relevant subset\n",
    "GroupData_morning = trial_end_times\n",
    "\n",
    "# Calculate the number of unique 'flies' in the dataset\n",
    "num_replicates = GroupData_morning[\"fly\"].nunique()\n",
    "\n",
    "# Initial guesses for L, k, x0\n",
    "p0 = [\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]),\n",
    "    1,\n",
    "    np.median(GroupData_morning[\"end_time\"]),\n",
    "]\n",
    "\n",
    "# Fit sigmoid function to data\n",
    "params, _ = curve_fit(\n",
    "    sigmoid,\n",
    "    GroupData_morning[\"end_time\"],\n",
    "    GroupData_morning[\"cumulative_solved_trials\"],\n",
    "    p0,\n",
    "    maxfev=5000,\n",
    ")\n",
    "\n",
    "# params contains the fitted values for L, k, x0\n",
    "L, k, x0 = params\n",
    "\n",
    "# Create a new DataFrame for the sigmoid fit curve\n",
    "sigmoid_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the sigmoid fit curve\n",
    "sigmoid_fit[\"cumulative_solved_trials\"] = sigmoid(sigmoid_fit[\"end_time\"], L, k, x0)\n",
    "\n",
    "# Calculate R-squared for sigmoid fit curve\n",
    "y_pred_sigmoid = sigmoid(GroupData_morning[\"end_time\"], L, k, x0)\n",
    "r_squared_sigmoid = r2_score(\n",
    "    GroupData_morning[\"cumulative_solved_trials\"], y_pred_sigmoid\n",
    ")\n",
    "\n",
    "# Calculate linear fit for the first half of the data\n",
    "first_half = GroupData_morning.iloc[: len(GroupData_morning) // 2]\n",
    "slope, intercept, _, _, _ = stats.linregress(\n",
    "    first_half[\"end_time\"], first_half[\"cumulative_solved_trials\"]\n",
    ")\n",
    "\n",
    "# Create a new DataFrame for the linear fit line\n",
    "linear_fit = pd.DataFrame(\n",
    "    {\n",
    "        \"end_time\": np.linspace(\n",
    "            GroupData_morning[\"end_time\"].min(),\n",
    "            GroupData_morning[\"end_time\"].max(),\n",
    "            100,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate y values for the linear fit line\n",
    "linear_fit[\"cumulative_solved_trials\"] = slope * linear_fit[\"end_time\"] + intercept\n",
    "\n",
    "# Calculate R-squared for linear fit curve\n",
    "y_pred_linear = slope * GroupData_morning[\"end_time\"] + intercept\n",
    "r_squared_linear = r2_score(\n",
    "    GroupData_morning[\"cumulative_solved_trials\"], y_pred_linear\n",
    ")\n",
    "\n",
    "# Calculate center x value and min and maximum xy value\n",
    "center_x = (\n",
    "    GroupData_morning[\"end_time\"].max() - GroupData_morning[\"end_time\"].min()\n",
    ") / 2\n",
    "max_y = max(\n",
    "    max(GroupData_morning[\"cumulative_solved_trials\"]),\n",
    "    max(sigmoid_fit[\"cumulative_solved_trials\"]),\n",
    ")\n",
    "\n",
    "max_x = GroupData_morning[\"end_time\"].max()\n",
    "min_y = GroupData_morning[\"cumulative_solved_trials\"].min()\n",
    "\n",
    "# Create your plot using GroupData_morning and add the fits and annotations\n",
    "cumulcurve_pool = (\n",
    "    hv.Curve(\n",
    "        data=GroupData_morning,\n",
    "        kdims=[\"end_time\"],\n",
    "        vdims=[\"cumulative_solved_trials\"],\n",
    "    )\n",
    "    .opts(\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        alpha=1,\n",
    "        line_width=2,\n",
    "        xlabel=\"Time(s)\",\n",
    "        ylabel=\"Cumulative Solved Trials\",\n",
    "        show_grid=True,\n",
    "        fontscale=3,\n",
    "        title=\"Cumulative Solved Trials Over Time\",\n",
    "    )\n",
    "    * hv.Curve(\n",
    "        data=sigmoid_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    "    ).opts(color=\"blue\")\n",
    "    * hv.Curve(\n",
    "        data=linear_fit, kdims=[\"end_time\"], vdims=[\"cumulative_solved_trials\"]\n",
    "    ).opts(color=\"red\")\n",
    "    # Uncomment the following lines to add text annotations\n",
    "    * hv.Text(center_x, max_y - 10, f\"Sigmoid fit R-squared: {r_squared_sigmoid:.2f}\").opts(text_color=\"blue\")\n",
    "    * hv.Text(center_x, max_y, f\"Linear fit R-squared: {r_squared_linear:.2f}\").opts(text_color=\"red\")\n",
    "    * hv.Text(max_x - 100, min_y * num_replicates, f\"N = {num_replicates}\")\n",
    ").opts(\n",
    "    opts.Area(\n",
    "        show_legend=False,\n",
    "        show_frame=False,\n",
    "        fill_color=\"blue\",\n",
    "        line_color=\"black\",\n",
    "        line_width=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "cumulcurve_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered[\"solved_trials\"] = Filtered[\"Trial\"] - 1\n",
    "\n",
    "# On clean data, get the average \"solved_trial\" grouped by time\n",
    "\n",
    "average_solved = Filtered.groupby(\"time\")[\"solved_trials\"].mean()\n",
    "\n",
    "# Plot the average \"solved_trials\" over time\n",
    "average_solved.hvplot(\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    height=400,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "\n",
    "# Convert Trial back to integers\n",
    "cleaned_data[\"Trial\"] = cleaned_data[\"Trial\"].astype(int)\n",
    "\n",
    "\n",
    "# Function to compute the end time of each trial\n",
    "def compute_trial_end_time(group):\n",
    "    end_time = group[\"time\"].max()\n",
    "    return pd.Series({\"end_time\": end_time})\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the end times\n",
    "trial_end_times = (\n",
    "    cleaned_data.groupby([\"fly\", \"Trial\"]).apply(compute_trial_end_time).reset_index()\n",
    ")\n",
    "\n",
    "# Sort by end_time\n",
    "trial_end_times = trial_end_times.sort_values(by=\"end_time\")\n",
    "\n",
    "# Compute the cumulative count of solved trials\n",
    "trial_end_times[\"cumulative_solved_trials\"] = (\n",
    "    trial_end_times[\"end_time\"].rank(method=\"first\").astype(int)\n",
    ")\n",
    "\n",
    "# Compute the average number of trials solved over time\n",
    "average_solved_trials = (\n",
    "    trial_end_times.groupby(\"end_time\")[\"cumulative_solved_trials\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "# Plot the average number of trials solved over time\n",
    "average_plot = average_solved_trials.hvplot.line(\n",
    "    x=\"end_time\",\n",
    "    y=\"cumulative_solved_trials\",\n",
    "    title=\"Average Number of Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Number of Solved Trials\",\n",
    ").opts(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    alpha=1,\n",
    "    line_width=2,\n",
    "    xlabel=\"Time(s)\",\n",
    "    ylabel=\"Average Number of Solved Trials\",\n",
    "    show_grid=True,\n",
    "    fontscale=3,\n",
    "    title=\"Average Number of Solved Trials Over Time\",\n",
    ")\n",
    "\n",
    "average_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "\n",
    "# Convert Trial back to integers\n",
    "cleaned_data[\"Trial\"] = cleaned_data[\"Trial\"].astype(int)\n",
    "\n",
    "\n",
    "# Function to compute the end time of each trial\n",
    "def compute_trial_end_time(group):\n",
    "    end_time = group[\"time\"].max()\n",
    "    return pd.Series({\"end_time\": end_time})\n",
    "\n",
    "\n",
    "# Apply the function to each group and compute the end times\n",
    "trial_end_times = (\n",
    "    cleaned_data.groupby([\"fly\", \"Trial\"]).apply(compute_trial_end_time).reset_index()\n",
    ")\n",
    "\n",
    "# Sort by end_time\n",
    "trial_end_times = trial_end_times.sort_values(by=\"end_time\")\n",
    "\n",
    "# Compute the cumulative count of solved trials\n",
    "trial_end_times[\"cumulative_solved_trials\"] = (\n",
    "    trial_end_times[\"end_time\"].rank(method=\"first\").astype(int)\n",
    ")\n",
    "\n",
    "# Compute the average number of trials solved over time\n",
    "average_solved_trials = (\n",
    "    trial_end_times.groupby(\"end_time\")[\"cumulative_solved_trials\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "# Compute the standard deviation of the cumulative solved trials at each time point\n",
    "std_solved_trials = (\n",
    "    trial_end_times.groupby(\"end_time\")[\"cumulative_solved_trials\"].std().reset_index()\n",
    ")\n",
    "\n",
    "# Merge the mean and standard deviation data\n",
    "average_solved_trials = average_solved_trials.merge(\n",
    "    std_solved_trials, on=\"end_time\", suffixes=(\"_mean\", \"_std\")\n",
    ")\n",
    "\n",
    "# Compute the 95% confidence intervals\n",
    "z_score = 1.96  # for 95% confidence interval\n",
    "average_solved_trials[\"upper_bound\"] = average_solved_trials[\n",
    "    \"cumulative_solved_trials_mean\"\n",
    "] + z_score * (average_solved_trials[\"cumulative_solved_trials_std\"] / np.sqrt(80))\n",
    "average_solved_trials[\"lower_bound\"] = average_solved_trials[\n",
    "    \"cumulative_solved_trials_mean\"\n",
    "] - z_score * (average_solved_trials[\"cumulative_solved_trials_std\"] / np.sqrt(80))\n",
    "\n",
    "# Plot the average number of trials solved over time with confidence intervals\n",
    "average_plot = average_solved_trials.hvplot.line(\n",
    "    x=\"end_time\",\n",
    "    y=\"cumulative_solved_trials_mean\",\n",
    "    title=\"Average Number of Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Number of Solved Trials\",\n",
    "    color=\"blue\",\n",
    "    label=\"Mean\",\n",
    ").opts(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    alpha=1,\n",
    "    line_width=2,\n",
    "    xlabel=\"Time(s)\",\n",
    "    ylabel=\"Average Number of Solved Trials\",\n",
    "    show_grid=True,\n",
    "    fontscale=3,\n",
    "    title=\"Average Number of Solved Trials Over Time\",\n",
    ")\n",
    "\n",
    "confidence_plot = average_solved_trials.hvplot.area(\n",
    "    x=\"end_time\",\n",
    "    y=\"upper_bound\",\n",
    "    y2=\"lower_bound\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "\n",
    "average_plot * confidence_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average trials solved per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column \"solved_trials\" that is equal to \"Trial\" - 1\n",
    "\n",
    "Filtered[\"solved_trials\"] = Filtered[\"Trial\"] - 1\n",
    "\n",
    "# Group by time and fly, then compute the mean of \"solved_trials\"\n",
    "\n",
    "average_solved = Filtered.groupby([\"time\"])[\"solved_trials\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the curve of the average \"solved_trials\" over time\n",
    "\n",
    "avgplot = average_solved.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"solved_trials\",\n",
    "    kind=\"line\",\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    height=400,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Save the plot\n",
    "\n",
    "hv.save(avgplot, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_AverageSolvedTrials.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[\"solved_trials\"] = cleaned_data[\"Trial\"] - 1\n",
    "\n",
    "# Group by time and fly, then compute the mean of \"solved_trials\"\n",
    "\n",
    "clean_average_solved = cleaned_data.groupby([\"time\"])[\"solved_trials\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_avgplot = clean_average_solved.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"solved_trials\",\n",
    "    kind=\"line\",\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    height=400,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "hv.save(clean_avgplot, \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with 'time' and 'solved_trials' columns\n",
    "# Group by 'time' and calculate mean and SEM\n",
    "grouped = cleaned_data.groupby(\"time\")[\"solved_trials\"].agg([\"mean\", \"sem\"]).reset_index()\n",
    "\n",
    "# Keep only times between 0 and 7100\n",
    "\n",
    "grouped = grouped[(grouped[\"time\"] >= 0) & (grouped[\"time\"] <= 7100)]\n",
    "\n",
    "# Calculate the confidence intervals (95% confidence level)\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-score for 95% confidence\n",
    "grouped[\"ci_lower\"] = grouped[\"mean\"] - z_score * grouped[\"sem\"]\n",
    "grouped[\"ci_upper\"] = grouped[\"mean\"] + z_score * grouped[\"sem\"]\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped.rename(columns={\"mean\": \"solved_trials\"}, inplace=True)\n",
    "\n",
    "# Plot the data with confidence intervals\n",
    "clean_avgplot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"solved_trials\",\n",
    "    kind=\"line\",\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    height=400,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Create the area plot for the confidence intervals\n",
    "ci_area = hv.Area(\n",
    "    (grouped[\"time\"], grouped[\"ci_lower\"], grouped[\"ci_upper\"]),\n",
    "    vdims=[\"ci_lower\", \"ci_upper\"],\n",
    ").opts(alpha=0.3, color=\"blue\")\n",
    "\n",
    "# Overlay the confidence interval area plot with the line plot\n",
    "combined_plot = ci_area * clean_avgplot\n",
    "\n",
    "# Save the combined plot\n",
    "hv.save(\n",
    "    combined_plot,\n",
    "    \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'time' and calculate mean and SEM\n",
    "grouped = (\n",
    "    cleaned_data.groupby(\"time\")[\"solved_trials\"].agg([\"mean\", \"sem\"]).reset_index()\n",
    ")\n",
    "\n",
    "# Keep only times between 0 and 7100\n",
    "grouped = grouped[(grouped[\"time\"] >= 0) & (grouped[\"time\"] <= 7100)]\n",
    "\n",
    "# Calculate the confidence intervals (95% confidence level)\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-score for 95% confidence\n",
    "grouped[\"ci_lower\"] = grouped[\"mean\"] - z_score * grouped[\"sem\"]\n",
    "grouped[\"ci_upper\"] = grouped[\"mean\"] + z_score * grouped[\"sem\"]\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped.rename(columns={\"mean\": \"solved_trials\"}, inplace=True)\n",
    "\n",
    "# Fit a linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(grouped[[\"time\"]], grouped[\"solved_trials\"])\n",
    "grouped[\"linear_fit\"] = linear_model.predict(grouped[[\"time\"]])\n",
    "linear_r2 = r2_score(grouped[\"solved_trials\"], grouped[\"linear_fit\"])\n",
    "\n",
    "\n",
    "# Define logistic function\n",
    "def logistic(x, L, x0, k):\n",
    "    return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "# Initial guesses for L, x0, k\n",
    "L_initial = max(grouped[\"solved_trials\"])\n",
    "x0_initial = np.median(grouped[\"time\"])\n",
    "k_initial = 1 / (max(grouped[\"time\"]) - min(grouped[\"time\"]))\n",
    "\n",
    "# Fit a logistic regression model\n",
    "popt, _ = curve_fit(\n",
    "    logistic,\n",
    "    grouped[\"time\"],\n",
    "    grouped[\"solved_trials\"],\n",
    "    p0=[L_initial, x0_initial, k_initial],\n",
    "    maxfev=5000,\n",
    ")\n",
    "grouped[\"logistic_fit\"] = logistic(grouped[\"time\"], *popt)\n",
    "logistic_r2 = r2_score(grouped[\"solved_trials\"], grouped[\"logistic_fit\"])\n",
    "\n",
    "# Plot the data with confidence intervals\n",
    "clean_avgplot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"solved_trials\",\n",
    "    kind=\"line\",\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    height=400,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Create the area plot for the confidence intervals\n",
    "ci_area = hv.Area(\n",
    "    (grouped[\"time\"], grouped[\"ci_lower\"], grouped[\"ci_upper\"]),\n",
    "    vdims=[\"ci_lower\", \"ci_upper\"],\n",
    ").opts(alpha=0.3, color=\"gray\")\n",
    "\n",
    "# Create the linear fit plot\n",
    "linear_fit_plot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"linear_fit\",\n",
    "    kind=\"line\",\n",
    "    color=\"red\",\n",
    "    line_dash=\"dashed\",\n",
    "    label=f\"Linear Fit (R²={linear_r2:.2f})\",\n",
    ")\n",
    "\n",
    "# Create the logistic fit plot\n",
    "logistic_fit_plot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"logistic_fit\",\n",
    "    kind=\"line\",\n",
    "    color=\"blue\",\n",
    "    line_dash=\"dotted\",\n",
    "    label=f\"Logistic Fit (R²={logistic_r2:.2f})\",\n",
    ")\n",
    "\n",
    "# Overlay the confidence interval area plot with the line plot and fits\n",
    "combined_plot = (ci_area * clean_avgplot * linear_fit_plot * logistic_fit_plot)\n",
    "\n",
    "# Save the combined plot\n",
    "hv.save(\n",
    "    combined_plot,\n",
    "    \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'time' and calculate mean and SEM\n",
    "grouped = (\n",
    "    cleaned_data.groupby(\"time\")[\"solved_trials\"].agg([\"mean\", \"sem\"]).reset_index()\n",
    ")\n",
    "\n",
    "# Keep only times between 0 and 7100\n",
    "grouped = grouped[(grouped[\"time\"] >= 0) & (grouped[\"time\"] <= 7100)]\n",
    "\n",
    "# Calculate the confidence intervals (95% confidence level)\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-score for 95% confidence\n",
    "grouped[\"ci_lower\"] = grouped[\"mean\"] - z_score * grouped[\"sem\"]\n",
    "grouped[\"ci_upper\"] = grouped[\"mean\"] + z_score * grouped[\"sem\"]\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped.rename(columns={\"mean\": \"solved_trials\"}, inplace=True)\n",
    "\n",
    "# Fit a linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(grouped[[\"time\"]], grouped[\"solved_trials\"])\n",
    "grouped[\"linear_fit\"] = linear_model.predict(grouped[[\"time\"]])\n",
    "linear_r2 = r2_score(grouped[\"solved_trials\"], grouped[\"linear_fit\"])\n",
    "\n",
    "# Fit a piecewise linear regression model\n",
    "piecewise_model = pwlf.PiecewiseLinFit(grouped[\"time\"], grouped[\"solved_trials\"])\n",
    "\n",
    "# Define the number of line segments\n",
    "num_segments = 4  # Adjust this number based on your data\n",
    "\n",
    "# Fit the model\n",
    "breaks = piecewise_model.fit(num_segments)\n",
    "\n",
    "# Predict the piecewise linear fit\n",
    "grouped[\"piecewise_fit\"] = piecewise_model.predict(grouped[\"time\"])\n",
    "piecewise_r2 = r2_score(grouped[\"solved_trials\"], grouped[\"piecewise_fit\"])\n",
    "\n",
    "# Plot the data with confidence intervals\n",
    "clean_avgplot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"solved_trials\",\n",
    "    kind=\"line\",\n",
    "    title=\"Average Solved Trials Over Time\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Average Solved Trials\",\n",
    "    line_width=3,\n",
    "    height=400,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Create the area plot for the confidence intervals\n",
    "ci_area = hv.Area(\n",
    "    (grouped[\"time\"], grouped[\"ci_lower\"], grouped[\"ci_upper\"]),\n",
    "    vdims=[\"ci_lower\", \"ci_upper\"],\n",
    ").opts(alpha=0.1, color=\"blue\")\n",
    "\n",
    "# Create the linear fit plot\n",
    "linear_fit_plot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"linear_fit\",\n",
    "    kind=\"line\",\n",
    "    line_width=3,\n",
    "    color=\"red\",\n",
    "    line_dash=\"dashed\",\n",
    "    label=f\"Linear Fit (R²={linear_r2:.2f})\",\n",
    ")\n",
    "\n",
    "# Create the piecewise linear fit plot\n",
    "piecewise_fit_plot = grouped.hvplot(\n",
    "    x=\"time\",\n",
    "    y=\"piecewise_fit\",\n",
    "    kind=\"line\",\n",
    "    line_width=3,\n",
    "    color=\"green\",\n",
    "    line_dash=\"dotdash\",\n",
    "    label=f\"Piecewise Linear Fit (R²={piecewise_r2:.2f})\",\n",
    ")\n",
    "\n",
    "# Overlay the confidence interval area plot with the line plot and fits\n",
    "combined_plot = ci_area * clean_avgplot * linear_fit_plot * piecewise_fit_plot\n",
    "\n",
    "# Save the combined plot\n",
    "hv.save(\n",
    "    combined_plot,\n",
    "    \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials_Linear.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_plot = combined_plot.options(**HoloviewsTemplates.hv_slides[\"plot\"]).opts(\n",
    "    invert_axes=False, show_legend=True\n",
    ")\n",
    "\n",
    "hv.save(\n",
    "    combined_plot,\n",
    "    \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials_Linear.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subplot = (clean_avgplot * ci_area).options(**HoloviewsTemplates.hv_slides[\"plot\"]).opts(\n",
    "    invert_axes=False, show_legend=True)\n",
    "\n",
    "hv.save(Subplot,\n",
    "        \"/mnt/upramdya_data/MD/MultiMazeRecorder/Plots/Learning/240815_CleanAverageSolvedTrials_Linear_Subplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackinganalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
