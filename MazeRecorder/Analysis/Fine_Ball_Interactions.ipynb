{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll explore how we can explore the interactions between the fly and the ball precisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_behavior import Sleap_utils\n",
    "from utils_behavior import Ballpushing_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Sleap_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some fly\n",
    "\n",
    "Fly1 = Ballpushing_utils.Fly(\"/mnt/upramdya_data/MD/MultiMazeRecorder/Videos/231115_TNT_Fine_2_Videos_Tracked/arena9/corridor6/\", as_individual=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the ball images\n",
    "\n",
    "Fly1.fly_skeleton\n",
    "\n",
    "# Display the frame\n",
    "#plt.imshow(some_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get interaction events between fly Rfront and Lfront and ball centre\n",
    "\n",
    "Events = Ballpushing_utils.find_interaction_events(Fly1.fly_skeleton, Fly1.balltrack.objects[0].dataset, [\"Rfront\", \"Lfront\"], [\"centre\"], threshold = [0,11], gap_between_events = 1/29, event_min_length = 1/29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each event, get the derivative of the ball position\n",
    "\n",
    "ball_velocities = []\n",
    "\n",
    "for event in Events:\n",
    "    # Get the ball positions for the event\n",
    "    ball_positions = Fly1.balltrack.objects[0].dataset[event[0]:event[1]]\n",
    "    # Get the derivative of the ball positions\n",
    "    \n",
    "    ball_velocity = abs(np.diff(ball_positions[\"y_centre\"], axis = 0))\n",
    "    \n",
    "    ball_velocities.append(ball_velocity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ball velocities as a function of event number\n",
    "\n",
    "plt.plot([np.mean(vel) for vel in ball_velocities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the annotated frames for event 4\n",
    "\n",
    "for frame in range(Events[4][0], Events[4][1]):\n",
    "    Fly1.flytrack.generate_annotated_frame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Sleap_utils.CombinedSleapTracks(video_path=\"/mnt/upramdya_data/MD/MultiMazeRecorder/Videos/231115_TNT_Fine_2_Videos_Tracked/arena9/corridor6/corridor6_preprocessed.mp4\", sleap_tracks_list=[Fly1, Ball1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLytrack = Fly1.dataset\n",
    "\n",
    "FLytrack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balltrack = Ball1.dataset\n",
    "\n",
    "Balltrack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold distance\n",
    "threshold = 11\n",
    "\n",
    "# Compute the Euclidean distance for Lfront and Rfront\n",
    "distances_Lfront = np.sqrt((FLytrack[\"x_Lfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Lfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "distances_Rfront = np.sqrt((FLytrack[\"x_Rfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Rfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "\n",
    "# Find frames where either Lfront or Rfront is within the threshold distance\n",
    "contacts = FLytrack[(distances_Lfront < threshold) | (distances_Rfront < threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the frames where the fly is in contact with the ball\n",
    "\n",
    "for index, row in contacts.iterrows():\n",
    "    frame = Fly1.generate_annotated_frame(index)\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    #print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contact_events(\n",
    "        FLytrack,\n",
    "        Balltrack,\n",
    "        threshold=11,\n",
    "        gap_between_events=4,\n",
    "        event_min_length=2,\n",
    "        fps=29\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This function finds contact events where the fly's legs are touching the ball for a minimum amount of time.\n",
    "\n",
    "    Parameters:\n",
    "    FLytrack (DataFrame): DataFrame containing the fly's tracking data.\n",
    "    Balltrack (DataFrame): DataFrame containing the ball's tracking data.\n",
    "    threshold (int): The distance threshold (in pixels) for the fly's legs to be considered in contact with the ball.\n",
    "    gap_between_events (int): The minimum gap required between two events, expressed in seconds.\n",
    "    event_min_length (int): The minimum length of an event, expressed in seconds.\n",
    "    fps (int): Frames per second of the video.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of contact events, where each event is represented as [start_frame, end_frame, duration].\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the gap between events and the minimum event length from seconds to frames\n",
    "    gap_between_events = gap_between_events * fps\n",
    "    event_min_length = event_min_length * fps\n",
    "\n",
    "    # Compute the Euclidean distance for Lfront and Rfront\n",
    "    distances_Lfront = np.sqrt((FLytrack[\"x_Lfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Lfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "    distances_Rfront = np.sqrt((FLytrack[\"x_Rfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Rfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "\n",
    "    # Find frames where either Lfront or Rfront is within the threshold distance\n",
    "    contact_frames = np.where((distances_Lfront < threshold) | (distances_Rfront < threshold))[0]\n",
    "\n",
    "    # If no contact frames are found, return an empty list\n",
    "    if len(contact_frames) == 0:\n",
    "        return []\n",
    "\n",
    "    # Find the distance between consecutive contact frames\n",
    "    distance_betw_frames = np.diff(contact_frames)\n",
    "\n",
    "    # Find the points where the distance between frames is greater than the gap between events\n",
    "    split_points = np.where(distance_betw_frames > gap_between_events)[0]\n",
    "\n",
    "    # Add the first and last points to the split points\n",
    "    split_points = np.insert(split_points, 0, -1)\n",
    "    split_points = np.append(split_points, len(contact_frames) - 1)\n",
    "\n",
    "    # Initialize the list of contact events\n",
    "    contact_events = []\n",
    "\n",
    "    # Iterate over the split points to find events\n",
    "    for f in range(0, len(split_points) - 1):\n",
    "        # Define the start and end of the region of interest (ROI)\n",
    "        start_roi = contact_frames[split_points[f] + 1]\n",
    "        end_roi = contact_frames[split_points[f + 1]]\n",
    "\n",
    "        # Calculate the duration of the event\n",
    "        duration = end_roi - start_roi\n",
    "\n",
    "        # If the duration of the event is greater than the minimum length, add the event to the list\n",
    "        if duration > event_min_length:\n",
    "            contact_events.append([start_roi, end_roi, duration])\n",
    "\n",
    "    return contact_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contact events\n",
    "contact_events = find_contact_events(FLytrack, Balltrack, threshold=16, gap_between_events=1/2, event_min_length=1/2, fps=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make clips of the contact events\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/Frontlegs_long\"\n",
    "\n",
    "# Make clips of the contact events\n",
    "for i, event in enumerate(contact_events):\n",
    "    # Define the start and end frames of the event\n",
    "    start_frame = event[0]\n",
    "    end_frame = event[1]\n",
    "\n",
    "    # Define the output file name\n",
    "    output_file = f\"{output_dir}/contact_event_{i}.mp4\"\n",
    "\n",
    "    # Make the clip\n",
    "    Fly1.generate_annotated_video(save=True, output_path=output_file, start=start_frame, end=end_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contact_events(\n",
    "        FLytrack,\n",
    "        Balltrack,\n",
    "        nodes=[\"Lfront\", \"Rfront\"],\n",
    "        threshold=11,\n",
    "        gap_between_events=4,\n",
    "        event_min_length=2,\n",
    "        fps=29\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This function finds contact events where the fly's specified nodes are touching the ball for a minimum amount of time.\n",
    "\n",
    "    Parameters:\n",
    "    FLytrack (DataFrame): DataFrame containing the fly's tracking data.\n",
    "    Balltrack (DataFrame): DataFrame containing the ball's tracking data.\n",
    "    nodes (list): List of nodes to check the distance with the ball (e.g., [\"Lfront\", \"Rfront\", \"Head\"]).\n",
    "    threshold (int): The distance threshold (in pixels) for the fly's nodes to be considered in contact with the ball.\n",
    "    gap_between_events (int): The minimum gap required between two events, expressed in seconds.\n",
    "    event_min_length (int): The minimum length of an event, expressed in seconds.\n",
    "    fps (int): Frames per second of the video.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of contact events, where each event is represented as [start_frame, end_frame, duration].\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the gap between events and the minimum event length from seconds to frames\n",
    "    gap_between_events = gap_between_events * fps\n",
    "    event_min_length = event_min_length * fps\n",
    "\n",
    "    # Initialize a list to store distances for all specified nodes\n",
    "    distances = []\n",
    "\n",
    "    # Compute the Euclidean distance for each specified node\n",
    "    for node in nodes:\n",
    "        distances_node = np.sqrt((FLytrack[f\"x_{node}\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[f\"y_{node}\"] - Balltrack[\"y_centre\"])**2)\n",
    "        distances.append(distances_node)\n",
    "\n",
    "    # Combine distances to find frames where any node is within the threshold distance\n",
    "    combined_distances = np.min(distances, axis=0)\n",
    "    contact_frames = np.where(combined_distances < threshold)[0]\n",
    "\n",
    "    # If no contact frames are found, return an empty list\n",
    "    if len(contact_frames) == 0:\n",
    "        return []\n",
    "\n",
    "    # Find the distance between consecutive contact frames\n",
    "    distance_betw_frames = np.diff(contact_frames)\n",
    "\n",
    "    # Find the points where the distance between frames is greater than the gap between events\n",
    "    split_points = np.where(distance_betw_frames > gap_between_events)[0]\n",
    "\n",
    "    # Add the first and last points to the split points\n",
    "    split_points = np.insert(split_points, 0, -1)\n",
    "    split_points = np.append(split_points, len(contact_frames) - 1)\n",
    "\n",
    "    # Initialize the list of contact events\n",
    "    contact_events = []\n",
    "\n",
    "    # Iterate over the split points to find events\n",
    "    for f in range(0, len(split_points) - 1):\n",
    "        # Define the start and end of the region of interest (ROI)\n",
    "        start_roi = contact_frames[split_points[f] + 1]\n",
    "        end_roi = contact_frames[split_points[f + 1]]\n",
    "\n",
    "        # Calculate the duration of the event\n",
    "        duration = end_roi - start_roi\n",
    "\n",
    "        # If the duration of the event is greater than the minimum length, add the event to the list\n",
    "        if duration > event_min_length:\n",
    "            contact_events.append([start_roi, end_roi, duration])\n",
    "\n",
    "    return contact_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check head contacts\n",
    "contact_events_head = find_contact_events(FLytrack, Balltrack, nodes=[\"Head\"], threshold=16, gap_between_events=1/2, event_min_length=1/2, fps=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_events_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make clips of the contact events\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/Head\"\n",
    "\n",
    "# Make clips of the contact events\n",
    "for i, event in enumerate(contact_events_head):\n",
    "    # Define the start and end frames of the event\n",
    "    start_frame = event[0]\n",
    "    end_frame = event[1]\n",
    "\n",
    "    # Define the output file name\n",
    "    output_file = f\"{output_dir}/contact_event_{i}.mp4\"\n",
    "\n",
    "    # Make the clip\n",
    "    Fly1.generate_annotated_video(save=True, output_path=output_file, start=start_frame, end=end_frame)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find contact events that would be in head but not in front legs\n",
    "\n",
    "specific_contact_events = []\n",
    "\n",
    "for event in contact_events_head:\n",
    "    start_frame = event[0]\n",
    "    end_frame = event[1]\n",
    "\n",
    "    # Check if the event is in the front legs contact events\n",
    "    is_in_front_legs = False\n",
    "    for event_front_legs in contact_events:\n",
    "        if start_frame >= event_front_legs[0] and end_frame <= event_front_legs[1]:\n",
    "            is_in_front_legs = True\n",
    "            break\n",
    "\n",
    "    # If the event is not in the front legs contact events, add it to the specific contact events\n",
    "    if not is_in_front_legs:\n",
    "        specific_contact_events.append(event)\n",
    "        \n",
    "        \n",
    "specific_contact_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try connected components methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def find_contact_events_connected_components(\n",
    "        video_path,\n",
    "        threshold=11,\n",
    "        gap_between_events=4,\n",
    "        event_min_length=2,\n",
    "        fps=30\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This function finds contact events where the fly's legs are touching the ball for a minimum amount of time using connected components analysis.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): Path to the video file.\n",
    "    threshold (int): The distance threshold (in pixels) for the fly's legs to be considered in contact with the ball.\n",
    "    gap_between_events (int): The minimum gap required between two events, expressed in seconds.\n",
    "    event_min_length (int): The minimum length of an event, expressed in seconds.\n",
    "    fps (int): Frames per second of the video.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of contact events, where each event is represented as [start_frame, end_frame, duration].\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the gap between events and the minimum event length from seconds to frames\n",
    "    gap_between_events = gap_between_events * fps\n",
    "    event_min_length = event_min_length * fps\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "\n",
    "    # Initialize the list of contact events\n",
    "    contact_events = []\n",
    "\n",
    "    # Initialize variables to store the current event\n",
    "    current_event_start = None\n",
    "    current_event_end = None\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Threshold the image\n",
    "        _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary)\n",
    "\n",
    "        # Check if there is only one shape (indicating contact)\n",
    "        if num_labels == 2:  # Background + one shape\n",
    "            if current_event_start is None:\n",
    "                current_event_start = frame_count\n",
    "            current_event_end = frame_count\n",
    "        else:\n",
    "            if current_event_start is not None:\n",
    "                duration = current_event_end - current_event_start\n",
    "                if duration > event_min_length:\n",
    "                    contact_events.append([current_event_start, current_event_end, duration])\n",
    "                current_event_start = None\n",
    "                current_event_end = None\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Handle the last event if it was still ongoing\n",
    "    if current_event_start is not None:\n",
    "        duration = current_event_end - current_event_start\n",
    "        if duration > event_min_length:\n",
    "            contact_events.append([current_event_start, current_event_end, duration])\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Merge close events\n",
    "    merged_events = []\n",
    "    for event in contact_events:\n",
    "        if not merged_events or event[0] - merged_events[-1][1] > gap_between_events:\n",
    "            merged_events.append(event)\n",
    "        else:\n",
    "            merged_events[-1][1] = event[1]\n",
    "            merged_events[-1][2] = merged_events[-1][1] - merged_events[-1][0]\n",
    "\n",
    "    return merged_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Get a random frame from the video\n",
    "video_path = \"/mnt/upramdya_data/MD/MultiMazeRecorder/Videos/231115_TNT_Fine_2_Videos_Tracked/arena9/corridor6/corridor6_preprocessed.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the frame at 2109\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 2109)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Display the frame\n",
    "\n",
    "plt.imshow(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arena_mask(binary_frame, dilation_iterations=1):\n",
    "    \"\"\"Create a mask that keeps only the area inside the detected arena.\"\"\"\n",
    "    contours, _ = cv2.findContours(\n",
    "        binary_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    mask = np.zeros_like(binary_frame)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
    "        \n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask, kernel, iterations=dilation_iterations)\n",
    "    \n",
    "    return dilated_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarise(frame, postpros = True):\n",
    "    \"\"\"Detect the corridors in a frame using a simple threshold.\"\"\"\n",
    "    if len(frame.shape) == 3:  # Convert to grayscale if needed\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(frame, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    if postpros:\n",
    "        kernel = np.ones((60, 20), np.uint8)  # Smaller kernel to avoid losing details\n",
    "        closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    else:\n",
    "        closing = binary\n",
    "\n",
    "    return closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make  a mask of the arena\n",
    "binary_frame = binarise(frame)\n",
    "\n",
    "mask = create_arena_mask(binary_frame, dilation_iterations=0)\n",
    "\n",
    "# Display the mask\n",
    "\n",
    "plt.imshow(mask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_binarise(frame, block_size=11, c=2):\n",
    "    if len(frame.shape) == 3:  # Convert to grayscale if needed\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    binary = cv2.adaptiveThreshold(frame, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c)\n",
    "    \n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def apply_asymmetric_morphological_operations(binary_frame, kernel_size=2, open_iterations=1, close_iterations=2):\n",
    "    # Create a kernel for morphological operations\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply morphological opening (erosion followed by dilation)\n",
    "    opened_frame = cv2.morphologyEx(binary_frame, cv2.MORPH_OPEN, kernel, iterations=open_iterations)\n",
    "    \n",
    "    # Apply morphological closing (dilation followed by erosion)\n",
    "    closed_frame = cv2.morphologyEx(opened_frame, cv2.MORPH_CLOSE, kernel, iterations=close_iterations)\n",
    "    \n",
    "    return closed_frame\n",
    "\n",
    "\n",
    "# Apply adaptive thresholding to the frame\n",
    "binary_frame = adaptive_binarise(frame, block_size=31, c=5)\n",
    "\n",
    "# Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10)\n",
    "\n",
    "# Apply the mask to the processed frame\n",
    "masked_frame = cv2.bitwise_and(processed_frame, processed_frame, mask=mask)\n",
    "\n",
    "# Display the masked frame\n",
    "plt.imshow(masked_frame, cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can detect inside the mask how many objects of a given size are present\n",
    "\n",
    "def detect_objects(frame, min_area=500, max_area=1000):\n",
    "    # Find contours in the frame\n",
    "    contours, _ = cv2.findContours(frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours based on area\n",
    "    objects = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if min_area <= area <= max_area:\n",
    "            objects.append(contour)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "# Detect objects in the masked frame\n",
    "\n",
    "objects = detect_objects(masked_frame, min_area=300, max_area=1000)\n",
    "\n",
    "print(f\"Number of objects detected: {len(objects)}\")\n",
    "# Draw the detected objects on the frame\n",
    "\n",
    "frame_with_objects = cv2.drawContours(frame.copy(), objects, -1, (0, 255, 0), thickness=2)\n",
    "\n",
    "# Display the frame with the detected objects\n",
    "\n",
    "plt.imshow(frame_with_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this kind of treatment to 1000 frames to see how the video looks like\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/ConnectedComponents\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get the total number of frames\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Pick start and end, and make a video with processed frames\n",
    "start_frame = 10000\n",
    "end_frame = 12000\n",
    "\n",
    "# Set the frame position to the start frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Initialize the video writer\n",
    "output_file = f\"{output_dir}/processed_video.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = 29\n",
    "\n",
    "# Get the frame size\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Process the frames\n",
    "for frame_count in range(start_frame, end_frame):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Create the arena mask\n",
    "    mask = create_arena_mask(binary, dilation_iterations=0)\n",
    "\n",
    "    # Apply adaptive thresholding to the frame\n",
    "    binary_frame = adaptive_binarise(frame, block_size=31, c=5)\n",
    "\n",
    "    # Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "    processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10)\n",
    "\n",
    "    # Apply the mask to the processed frame\n",
    "    masked_frame = cv2.bitwise_and(processed_frame, processed_frame, mask=mask)\n",
    "\n",
    "    # Detect objects in the masked frame\n",
    "    objects = detect_objects(masked_frame, min_area=300, max_area=1000)\n",
    "\n",
    "    # Draw the detected objects on the frame\n",
    "    frame_with_objects = cv2.drawContours(frame.copy(), objects, -1, (0, 255, 0), thickness=2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame_with_objects)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Video processing complete. Output saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the total number of frames\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialize the list of contact events\n",
    "contact_events = []\n",
    "\n",
    "# Initialize variables to store the current event\n",
    "current_event_start = None\n",
    "current_event_end = None\n",
    "\n",
    "event_min_length = 1\n",
    "frame_count = 0\n",
    "max_frames = 1000  # Process only the first 1000 frames\n",
    "\n",
    "while frame_count < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Threshold the image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Apply the mask to the frame\n",
    "    masked_frame = cv2.bitwise_and(binary, binary, mask=mask)\n",
    "    \n",
    "    # Apply adaptive thresholding to the frame\n",
    "    binary_frame = adaptive_binarise(frame, block_size=21, c=9)\n",
    "    \n",
    "    # Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "    processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=3)\n",
    "    \n",
    "    # Apply the mask to the processed frame\n",
    "    masked_frame = cv2.bitwise_and(processed_frame, processed_frame, mask=mask)\n",
    "    \n",
    "    # Detect objects in the masked frame\n",
    "    objects = detect_objects(masked_frame, min_area=300, max_area=1000)\n",
    "    \n",
    "    # Check if there is only one shape (indicating contact)\n",
    "    if len(objects) == 1:\n",
    "        if current_event_start is None:\n",
    "            current_event_start = frame_count\n",
    "        current_event_end = frame_count\n",
    "    else:\n",
    "        if current_event_start is not None:\n",
    "            duration = current_event_end - current_event_start\n",
    "            if duration > event_min_length:\n",
    "                contact_events.append([current_event_start, current_event_end, duration])\n",
    "            current_event_start = None\n",
    "            current_event_end = None\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "# Handle the last event if it was still ongoing\n",
    "if current_event_start is not None:\n",
    "    duration = current_event_end - current_event_start\n",
    "    if duration > event_min_length:\n",
    "        contact_events.append([current_event_start, current_event_end, duration])\n",
    "\n",
    "cap.release()\n",
    "\n",
    "print(\"Contact events:\", contact_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make clips of the contact events\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/ConnectedComponents\"\n",
    "\n",
    "# Make clips of the contact events\n",
    "\n",
    "for i, event in enumerate(contact_events):\n",
    "    # Define the start and end frames of the event\n",
    "    start_frame = event[0]\n",
    "    end_frame = event[1]\n",
    "\n",
    "    # Define the output file name\n",
    "    output_file = f\"{output_dir}/contact_event_{i}.mp4\"\n",
    "\n",
    "    # Make the clip\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    # Get the frame width and height\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) > end_frame:\n",
    "            break\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Saved contact event {i} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's first detect event where the fly is in contact with the ball and then crop the frames around the fly and the ball and apply the connected components analysis to detect contacts.\n",
    "\n",
    "# Detect proximity events\n",
    "\n",
    "# Define the threshold distance\n",
    "threshold = 50\n",
    "\n",
    "events = find_contact_events(FLytrack, Balltrack, threshold=threshold, gap_between_events=4, event_min_length=2, fps=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first event and crop the frames around the fly and the ball\n",
    "\n",
    "# Load the first frame of the video corresponding to the first event\n",
    "\n",
    "# Get the first event\n",
    "event = events[0]\n",
    "\n",
    "# Get the start and end frames of the event\n",
    "\n",
    "start_frame = event[0]\n",
    "\n",
    "end_frame = event[1]\n",
    "\n",
    "# Load the video\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the frame position to the start frame\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Read the frame\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Release the video capture object\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Display the frame\n",
    "\n",
    "plt.imshow(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the frame around the fly and the ball so 50 pixels after the fly and 20 before the ball\n",
    "\n",
    "# Get the fly and ball coordinates\n",
    "\n",
    "fly_x = FLytrack.loc[event[0], \"x_Thorax\"]\n",
    "\n",
    "fly_y = FLytrack.loc[event[0], \"y_Thorax\"]\n",
    "\n",
    "ball_x = Balltrack.loc[event[0], \"x_centre\"]\n",
    "\n",
    "ball_y = Balltrack.loc[event[0], \"y_centre\"]\n",
    "\n",
    "# Define the crop box\n",
    "\n",
    "crop_box = (int(fly_x - 15), int(ball_y - 50), int(fly_x + 15), int(fly_y + 50))\n",
    "\n",
    "# Crop the frame\n",
    "\n",
    "cropped_frame = frame[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2]]\n",
    "\n",
    "# Display the cropped frame\n",
    "\n",
    "plt.imshow(cropped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try classic binarisation\n",
    "\n",
    "# Apply binary thresholding to the cropped frame\n",
    "\n",
    "binary_frame = cv2.threshold(cropped_frame, 80, 255, cv2.THRESH_BINARY )[1]\n",
    "\n",
    "# invert the binary frame\n",
    "\n",
    "binary_frame = cv2.bitwise_not(binary_frame)\n",
    "\n",
    "# Display the binary frame\n",
    "\n",
    "plt.imshow(binary_frame, cmap=\"gray\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adaptive binarisation\n",
    "\n",
    "# Apply adaptive thresholding to the cropped frame\n",
    "\n",
    "binary_frame = adaptive_binarise(cropped_frame, block_size=51, c=3)\n",
    "\n",
    "\n",
    "\n",
    "# Display the binary frame\n",
    "\n",
    "plt.imshow(binary_frame, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply connected components analysis to the cropped frame\n",
    "\n",
    "# Apply adaptive thresholding to the cropped frame\n",
    "\n",
    "binary_frame = adaptive_binarise(cropped_frame, block_size=51, c=3)\n",
    "\n",
    "# Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "\n",
    "processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=20)\n",
    "\n",
    "# Apply connected components analysis to the processed frame\n",
    "\n",
    "objects = detect_objects(processed_frame, min_area=300, max_area=2000)\n",
    "\n",
    "# Draw the detected objects on the cropped frame\n",
    "\n",
    "frame_with_objects = cv2.drawContours(cropped_frame.copy(), objects, -1, (0, 255, 0), thickness=2)\n",
    "\n",
    "# Display the frame with the detected objects\n",
    "\n",
    "plt.imshow(frame_with_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_binarise(frame, block_size=31, c=5):\n",
    "    if len(frame.shape) == 3:  # Convert to grayscale if needed\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    binary = cv2.adaptiveThreshold(frame, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c)\n",
    "    \n",
    "    # Invert the binary image\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10):\n",
    "    # Create a kernel for morphological operations\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply morphological opening (erosion followed by dilation)\n",
    "    opened_frame = cv2.morphologyEx(binary_frame, cv2.MORPH_OPEN, kernel, iterations=open_iterations)\n",
    "    \n",
    "    # Apply morphological closing (dilation followed by erosion)\n",
    "    closed_frame = cv2.morphologyEx(opened_frame, cv2.MORPH_CLOSE, kernel, iterations=close_iterations)\n",
    "    \n",
    "    return closed_frame\n",
    "\n",
    "def detect_objects(frame, min_area=500, max_area=1000):\n",
    "    # Find contours in the frame\n",
    "    contours, _ = cv2.findContours(frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours based on area\n",
    "    objects = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if min_area <= area <= max_area:\n",
    "            objects.append(contour)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "def create_arena_mask(binary_frame, dilation_iterations=0):\n",
    "    # Create a mask for the arena\n",
    "    mask = np.zeros_like(binary_frame)\n",
    "    contours, _ = cv2.findContours(binary_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "    \n",
    "    if dilation_iterations > 0:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.dilate(mask, kernel, iterations=dilation_iterations)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def check_contact(binary_frame):\n",
    "    # Perform connected component analysis\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_frame)\n",
    "    \n",
    "    # Exclude the background label (0)\n",
    "    num_objects = num_labels - 1\n",
    "    \n",
    "    # If there is only one object, the fly and the ball are touching\n",
    "    if num_objects == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/ConnectedComponents\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get the total number of frames\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Pick start and end, and make a video with processed frames\n",
    "start_frame = 12000\n",
    "end_frame = 13000\n",
    "\n",
    "# Set the frame position to the start frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Initialize the video writer\n",
    "output_file = f\"{output_dir}/cropped_process_video.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = 29\n",
    "\n",
    "# Get the frame size\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Process the frames\n",
    "for frame_count in range(start_frame, end_frame):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding to the frame\n",
    "    _, binary_frame = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invert the binary frame\n",
    "    binary_frame = cv2.bitwise_not(binary_frame)\n",
    "\n",
    "    # Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "    processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10)\n",
    "\n",
    "    # Check if the fly and the ball are touching\n",
    "    contact = check_contact(processed_frame)\n",
    "\n",
    "    # Draw the result on the frame\n",
    "    frame_with_contact = frame.copy()\n",
    "    if contact:\n",
    "        cv2.putText(frame_with_contact, \"Contact\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame_with_contact, \"No Contact\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame_with_contact)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Video processing complete. Output saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_binarise(frame, block_size=31, c=5):\n",
    "    if len(frame.shape) == 3:  # Convert to grayscale if needed\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    binary = cv2.adaptiveThreshold(frame, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, c)\n",
    "    \n",
    "    # Invert the binary image\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10):\n",
    "    # Create a kernel for morphological operations\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply morphological opening (erosion followed by dilation)\n",
    "    opened_frame = cv2.morphologyEx(binary_frame, cv2.MORPH_OPEN, kernel, iterations=open_iterations)\n",
    "    \n",
    "    # Apply morphological closing (dilation followed by erosion)\n",
    "    closed_frame = cv2.morphologyEx(opened_frame, cv2.MORPH_CLOSE, kernel, iterations=close_iterations)\n",
    "    \n",
    "    return closed_frame\n",
    "\n",
    "def detect_objects(frame, min_area=500, max_area=1000):\n",
    "    # Find contours in the frame\n",
    "    contours, _ = cv2.findContours(frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours based on area\n",
    "    objects = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if min_area <= area <= max_area:\n",
    "            objects.append(contour)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "def create_arena_mask(binary_frame, dilation_iterations=0):\n",
    "    # Create a mask for the arena\n",
    "    mask = np.zeros_like(binary_frame)\n",
    "    contours, _ = cv2.findContours(binary_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "    \n",
    "    if dilation_iterations > 0:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.dilate(mask, kernel, iterations=dilation_iterations)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def check_contact(binary_frame):\n",
    "    # Perform connected component analysis\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_frame)\n",
    "    \n",
    "    # Exclude the background label (0)\n",
    "    num_objects = num_labels - 1\n",
    "    \n",
    "    # If there is only one object, the fly and the ball are touching\n",
    "    if num_objects == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_contact_events(FLytrack, Balltrack, threshold=11, gap_between_events=4, event_min_length=2, fps=30):\n",
    "    \"\"\"\n",
    "    This function finds contact events where the fly's legs are touching the ball for a minimum amount of time.\n",
    "\n",
    "    Parameters:\n",
    "    FLytrack (DataFrame): DataFrame containing the fly's tracking data.\n",
    "    Balltrack (DataFrame): DataFrame containing the ball's tracking data.\n",
    "    threshold (int): The distance threshold (in pixels) for the fly's legs to be considered in contact with the ball.\n",
    "    gap_between_events (int): The minimum gap required between two events, expressed in seconds.\n",
    "    event_min_length (int): The minimum length of an event, expressed in seconds.\n",
    "    fps (int): Frames per second of the video.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of contact events, where each event is represented as [start_frame, end_frame, duration].\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the gap between events and the minimum event length from seconds to frames\n",
    "    gap_between_events = gap_between_events * fps\n",
    "    event_min_length = event_min_length * fps\n",
    "\n",
    "    # Compute the Euclidean distance for Lfront and Rfront\n",
    "    distances_Lfront = np.sqrt((FLytrack[\"x_Lfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Lfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "    distances_Rfront = np.sqrt((FLytrack[\"x_Rfront\"] - Balltrack[\"x_centre\"])**2 + (FLytrack[\"y_Rfront\"] - Balltrack[\"y_centre\"])**2)\n",
    "\n",
    "    # Find frames where either Lfront or Rfront is within the threshold distance\n",
    "    contact_frames = np.where((distances_Lfront < threshold) | (distances_Rfront < threshold))[0]\n",
    "\n",
    "    # If no contact frames are found, return an empty list\n",
    "    if len(contact_frames) == 0:\n",
    "        return []\n",
    "\n",
    "    # Find the distance between consecutive contact frames\n",
    "    distance_betw_frames = np.diff(contact_frames)\n",
    "\n",
    "    # Find the points where the distance between frames is greater than the gap between events\n",
    "    split_points = np.where(distance_betw_frames > gap_between_events)[0]\n",
    "\n",
    "    # Add the first and last points to the split points\n",
    "    split_points = np.insert(split_points, 0, -1)\n",
    "    split_points = np.append(split_points, len(contact_frames) - 1)\n",
    "\n",
    "    # Initialize the list of contact events\n",
    "    contact_events = []\n",
    "\n",
    "    # Iterate over the split points to find events\n",
    "    for f in range(0, len(split_points) - 1):\n",
    "        # Define the start and end of the region of interest (ROI)\n",
    "        start_roi = contact_frames[split_points[f] + 1]\n",
    "        end_roi = contact_frames[split_points[f + 1]]\n",
    "\n",
    "        # Calculate the duration of the event\n",
    "        duration = end_roi - start_roi\n",
    "\n",
    "        # If the duration of the event is greater than the minimum length, add the event to the list\n",
    "        if duration > event_min_length:\n",
    "            contact_events.append([start_roi, end_roi, duration])\n",
    "\n",
    "    return contact_events\n",
    "\n",
    "output_dir = \"/mnt/upramdya_data/MD/Other_Videos/ContactDetection/ConnectedComponents\"\n",
    "\n",
    "# Detect proximity events\n",
    "threshold = 50\n",
    "events = find_contact_events(FLytrack, Balltrack, threshold=threshold, gap_between_events=4, event_min_length=2, fps=29)\n",
    "\n",
    "# Process each event\n",
    "for event in events:\n",
    "    start_frame = event[0]\n",
    "    end_frame = event[1]\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        exit()\n",
    "\n",
    "    # Set the frame position to the start frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    # Initialize the video writer\n",
    "    output_file = f\"{output_dir}/processed_event_{start_frame}_{end_frame}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = 29\n",
    "\n",
    "    # Get the frame size\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process the frames for the current event\n",
    "    for frame_count in range(start_frame, end_frame):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Ensure the frame is grayscale\n",
    "        if len(frame.shape) == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get the fly and ball coordinates\n",
    "        fly_x = FLytrack.loc[frame_count, \"x_Thorax\"]\n",
    "        fly_y = FLytrack.loc[frame_count, \"y_Thorax\"]\n",
    "        ball_x = Balltrack.loc[frame_count, \"x_centre\"]\n",
    "        ball_y = Balltrack.loc[frame_count, \"y_centre\"]\n",
    "        \n",
    "        # Get the center of the frame\n",
    "        center_x = frame.shape[1] // 2\n",
    "\n",
    "        # Define the crop box\n",
    "        crop_x1 = max(0, int(center_x - 15))\n",
    "        crop_y1 = max(0, int(ball_y - 50))\n",
    "        crop_x2 = min(frame.shape[1], int(center_x + 15))\n",
    "        crop_y2 = min(frame.shape[0], int(fly_y + 50))\n",
    "\n",
    "        # Crop the frame\n",
    "        cropped_frame = frame[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "        # Check if the cropped frame is empty\n",
    "        if cropped_frame.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Apply binary thresholding to the cropped frame\n",
    "        _, binary_frame = cv2.threshold(cropped_frame, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Invert the binary frame\n",
    "        binary_frame = cv2.bitwise_not(binary_frame)\n",
    "\n",
    "        # Apply asymmetric morphological operations to reduce noise and fill holes\n",
    "        processed_frame = apply_asymmetric_morphological_operations(binary_frame, kernel_size=1, open_iterations=1, close_iterations=10)\n",
    "\n",
    "        # Check if the fly and the ball are touching\n",
    "        contact = check_contact(processed_frame)\n",
    "\n",
    "        # Draw a green or red circle on the top of the frame based on the contact\n",
    "        \n",
    "        frame_with_contact = frame.copy()\n",
    "        if contact:\n",
    "            cv2.circle(frame_with_contact, (center_x, int((crop_y1 + crop_y2) / 2)), 10, (0, 255, 0), -1)\n",
    "        else:\n",
    "            cv2.circle(frame_with_contact, (center_x, int((crop_y1 + crop_y2) / 2)), 10, (0, 0, 255), -1)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame_with_contact)\n",
    "\n",
    "    # Release the video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "print(\"Video processing complete. Output saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
